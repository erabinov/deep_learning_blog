---
title: "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016"
date: "2023-03-07"
format:
  html:
    html-math-method: mathjax
---

## Introduction 

In November 2022, my friend gave me the datasets included in the "data" folder for this post as "Yr1116Birth.csv" and "Yr1116Death.csv" and told me to develop a model that predicts a county-by-county infant mortality rate based on various demographic features that were recorded in the data. The data lists births and deaths of infants in the years 2011-2016 in North Carolina by county of residence, race, and ethnicity of the mother. The Excel file `Dataset Descriptions.xls` lists all the information available in this dataset, and I presume that the information comes from the [NC State Center for Vital Statistics](https://schs.dph.ncdhhs.gov/data/vital.cfm).
I've also supplemented the data with county-by-county income data which I found at [datausa](https://datausa.io/profile/geo/north-carolina), though this data is also readily available from various public agencies like the Census Bureau.
I use the data to compute infant mortality rates by county, race, and year.
I used the 2011-2015 data to train my models and the 2016 data for validation.

This post is a write-up of my analysis of this data.
This was an interesting task because it was a crash course in a few important data science skills: data wrangling, rare event modeling, and maps & data visualization.

### Data wrangling

My data came from a number of different sources. There were separate birth and death data tables, the income data came from its own source, and the purely geographic data used to generate the map at the end of this post came from the `mapdata` package.

Combining these sources presented some difficulty. The birth and death data had different numerical codings for the race of the baby/mother, so I had to make a custom function `collapse` which collapsed the many extra race codings in the birth data into the simpler "other" category in the death data.
In a similar vein, different sources encoded county information in different ways: North Carolina orders the counties from 1 to 100, but there is also a FIPS code which is more useful for national data, and of course the name of a county is a fine label for it.
One defect in my data was that one of my sources mis-spelled "Tyrrell", and it took me a while to detect this error.

### Rare Event Modeling

When my friend presented me with this data, he and I discussed the interesting fact that some counties recorded no infant deaths for certain races in certain years.
I don't think that this was due to incomplete records or reporting anomalies: when I investigated these cases, I found that there were fewer than 100 births in the previous year in the same race and county.
The overall infant mortality rate was about .7% in these years, so the expected number of infant deaths when there are fewer than 100 births is less than 1.

My friend raised the possibility that I could model this problem as a classification problem: given a partiular infant birth, predict the probability that it would die in the first year of its life.
I considered this possibility, but decided not to do the analysis in this way, since the birth data contained more information, like the infant's birth weight, that was not reflected in the death data, so I thought it might be hard to measure the effect of these additional variables on a given infant's likelihood of death.
So, instead I modeled the problem as a regression problem: predict a county's infant mortality rate in a given county by year and race, given the county's average values for the other predictors in the birth data (e.g., birth weight in grams, median income in the county, number of cigarettes smoked by the mother).

Nevertheless, the data still presented the challenges associated with classification problems in which the classes are very unbalanced in number.
To get a feel for why there is an issue, let's consider one of those counties where there were no infant deaths in a given year.
Because infant mortality rates are on the order of 1/1000, to detect significant changes in rates between counties, it makes sense to measure them on a log scale.
The counties where there were no infant deaths, we would record an infant mortality rate of 0, which would be (infinitely) many orders of magnitude smaller than the typical rate.
To solve this, I added .001 to the infant mortality rates before taking the logarithm.
This is sort of like label smoothing: I don't want the model to make too much of those points where there happened to be no infant deaths.

One other thing I tried to do was to aggregate the counties into clusters and compute only in-cluster infant mortality rates for the training data.
This was an attempt to reduce year-to-year variance due to small sample sizes.
However, I found that my implementation of this idea didn't really improve the validation-set error.
So, in the end, I didn't put this into practice.
But, if you look at the code for this notebook, you'll find relics of that approach.


### Visualization and Maps

This project was a good way for me to practice what I had learned about making data visualizations from *Introduction to Statistical Learning with R*, including feature importance charts. But it was also an opportunity for me to learn how to make a county-by-county heat map; this map appears in the last section of this 


## Loading Packages and Cleaning Data


In this section, I clean the data and wrangle it into a form amenable to analysis.
I omit most of this process from the presentation version of the notebook, but the interested readers can examine the code, which is available on my Github page.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,message=FALSE,error=FALSE)
```


```{r, include=FALSE}
library(ggplot2)
library(tidyverse)
library(urbnmapr)
library(dplyr)
library(ggmap)
library(maps)
library(mapdata)
library(glmnet)
```


```{r, include=FALSE}
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )
states<-map_data("state")
nc_df<-subset(states,region=="north carolina")
nc_base <- ggplot(data = nc_df, mapping = aes(x = long, y = lat, group = group)) + 
  coord_fixed(1.3) + 
  geom_polygon(color = "black", fill = "gray")
counties<-map_data("county")
nc_county<-subset(counties,region=="north carolina")
```


```{r, include=FALSE}
#raw.bd1<-read_csv("data/birth_data.csv")
#raw.bd1$Year<-as.integer(raw.bd1$Year)
#raw.bd2<-read_csv("data/birth_data2.csv")
br<-read_csv("data/Yr1116Birth.csv")
dr<-read_csv("data/Yr1116Death.csv")
county.conversion<-read_csv("data/County_Codes_Table_1.csv")
county.conversion<-county.conversion[,1:3]
incomes<-read_csv("data/Income_by_Location.csv")
incomes$Geography<-str_sub(incomes$Geography,1,-12)
incomes<-rename(incomes,subregion=`Geography`)
#Resolve YOB vs. YEAR column names.
br<-rename(br, YEAR=YOB)
#Resolve MRACER vs. RACE column names.
br<-rename(br, RACE=MRACER)
```


```{r, include=FALSE}
## The birth and death tables contain different codings for race, so I have to change that. I begin by defining a function "collapse" which translates the birth data race coding to death data race  coding. 
collapse<-function(x){
  if(x==1|x==2|x==3)
    return(x)
  else
    return(4)
}
br$RACE<-sapply(br$RACE,collapse)
county.conversion$CORES<-as.double(county.conversion$CORES)
br<-left_join(br,county.conversion[,1:2])
br$COUNTY<-tolower(br$COUNTY)
br<-rename(br, subregion=COUNTY)
simple_incomes<-incomes[incomes$`ID Race`==0&incomes$`ID Year`==2016,c(5,7)]
simple_incomes$subregion<-tolower(simple_incomes$subregion)
br<-left_join(br, simple_incomes)
```


```{r, include=FALSE}
# countycounts<-count(br,CORES)
# countyracecounts<-count(br,CORES,RACE)
# race1births<-rep(0,100)
# for (i in 1:100) {
#   race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==1,]$n)
# }
# race2births<-rep(0,100)
# for (i in 1:100) {
#   race2births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==2,]$n)
# }
# race3births<-rep(0,100)
# for (i in 1:100) {
#   race3births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==3,]$n)
# }
# race4births<-rep(0,100)
# for (i in 1:100) {
#   race4births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==4,]$n)
# }
# countycounts<-bind_cols(countycounts,race1births,race2births,race3births,race4births)
# countycounts<-rename(countycounts,NUMBIRTH=n,NUM1=...3,NUM2=...4,NUM3=...5,NUM4=...6)
# 
# ##We try K-means clustering with about 20 clusters. We want population to have the largest effect, so we don't normalize the data. The result is probably that race plays a very small role in the size of the clusters.
# set.seed(1)
# num_clusters = 75
# km.out<-kmeans(countycounts[,2:6],num_clusters,nstart=50)
# km.out$cluster
# countycounts<-bind_cols(countycounts,CLUSTER=km.out$cluster)

COREStoCLUSTER<-function(x){
  if(0<=x&x<=100)
    return(x)
  else
    return(NA)
}

clustervals <- sapply(br$CORES,COREStoCLUSTER)
br$CLUSTER<-clustervals

drclustervals<- sapply(dr$CORES,COREStoCLUSTER)
dr$CLUSTER<-drclustervals
```

```{r, include=FALSE}
brdrcountscluster<-count(br,YEAR,CLUSTER,RACE)
brdrcountscluster<-rename(brdrcountscluster,NUMBIRTH=n)
#I noticed, for example, that the original brdrcounts had no entry for (YEAR,CORES,RACE)=(2011,3,3). Based on a superficial check of a few of the missing values, this seems plausible, but I still want to set NUMBIRTH=0 for the missing combinations. Mostly RACE=3 rows are missing, which seems plausible since the Native American population of NC is around 1-2%. 
brdrcountscluster<-complete(brdrcountscluster,YEAR,RACE)
brdrcountscluster<-replace_na(brdrcountscluster,list(NUMBIRTH=0))
drcountscluster<-count(dr,YEAR,CLUSTER,RACE)
#Same sort of completion for death counts. There are even more missing combos, since the death numbers overall in a low-population county can be quite small.
drcountscluster<-complete(drcountscluster,YEAR,CLUSTER,RACE)
drcountscluster<-replace_na(drcountscluster,list(n=0))

brdrcountscluster<-left_join(brdrcountscluster,drcountscluster)
brdrcountscluster<-rename(brdrcountscluster,NUMDEATH=n)
brdrcountscluster<-replace_na(brdrcountscluster,list(NUMDEATH=0))

brdrcounts<- count(br,YEAR,CORES,RACE)
brcounts<- brdrcounts
brcounts<- complete(brcounts, YEAR,CORES,RACE)
brcounts<-replace_na(brcounts,list(n=0))
drcounts<- count(dr,YEAR,CORES,RACE)
drcounts<-complete(drcounts, YEAR, CORES, RACE)
drcounts<-replace_na(drcounts,list(n=0))
brdrcounts<- left_join(brdrcounts[,-4], drcounts[,-4])
brdrcounts<- complete(brdrcounts, YEAR, CORES, RACE)
brdrcountscluster<-complete(brdrcountscluster,YEAR,CLUSTER,RACE)
brdrcountscluster<- replace_na(brdrcountscluster,list(NUMDEATH=0,NUMBIRTH=0))
```

```{r, include=FALSE}
##I add the group means for each statistic in br to the birth and death counts.
brdrcounts<-bind_cols(brdrcounts,CIGPN=rep(0,nrow(brdrcounts)),CIGFN=rep(0,nrow(brdrcounts)),CIGSN=rep(0,nrow(brdrcounts)),CIGLN=rep(0,nrow(brdrcounts)),BWTG=rep(0,nrow(brdrcounts)),GEST=rep(0,nrow(brdrcounts)),PLUR=rep(0,nrow(brdrcounts)), MAGE=rep(0,nrow(brdrcounts)),PARITY=rep(0,nrow(brdrcounts)),INCOME=rep(0,nrow(brdrcounts)),IMR=rep(0,nrow(brdrcounts)),CLUSTER=rep(0,nrow(brdrcounts)))
for (i in 2011:2016) {
  for(j in 1:100){
   for(k in 1:4){
     subset<-br[br$YEAR==i&br$CORES==j&br$RACE==k,]
     cigpn<-mean(subset$CIGPN)
     cigfn<-mean(subset$CIGFN)
     cigsn<-mean(subset$CIGSN)
     cigln<-mean(subset$CIGLN)
     bwtg<-mean(subset$BWTG)
     gest<-mean(subset$GEST)
     plur<-mean(subset$PLUR)
     mage<-mean(subset$MAGE)
     parity<-mean(subset$PARITY)
     incomes<-mean(subset$`Household Income by Race`)
     if(i!=2016){
       cluster_imr<- brdrcountscluster[brdrcountscluster$YEAR==i&brdrcountscluster$CLUSTER==COREStoCLUSTER(j)&brdrcountscluster$RACE==k,]$NUMDEATH/brdrcountscluster[brdrcountscluster$YEAR==i&brdrcountscluster$CLUSTER==COREStoCLUSTER(j)&brdrcountscluster$RACE==k,]$NUMBIRTH
     }
     else{
       cluster_imr<-drcounts[drcounts$YEAR==i&drcounts$CORES==j&drcounts$RACE==k,]$n/brcounts[brcounts$YEAR==i&brcounts$CORES==j&brcounts$RACE==k,]$n
     }
     
     n<-which(brdrcounts$YEAR==i&brdrcounts$CORES==j&brdrcounts$RACE==k)[[1]]
     brdrcounts[n,]$CIGPN<-cigpn
     brdrcounts[n,]$CIGFN<-cigfn
     brdrcounts[n,]$CIGSN<-cigsn
     brdrcounts[n,]$CIGLN<-cigln
     brdrcounts[n,]$BWTG<-bwtg
     brdrcounts[n,]$GEST<-gest
     brdrcounts[n,]$PLUR<-plur
     brdrcounts[n,]$MAGE<-mage
     brdrcounts[n,]$PARITY<-parity
     brdrcounts[n,]$INCOME<-incomes
     #Impute the mean IMR for those observations with zero deaths.
     if(!is.na(cluster_imr)&cluster_imr==0){
       cluster_imr<-.007
     }
     brdrcounts[n,]$IMR<-cluster_imr
     brdrcounts[n,]$CLUSTER<-COREStoCLUSTER(j)
     } 
  }
}
brdrcounts<-replace_na(brdrcounts,list(CIGPN=0,CIGFN=0,CIGSN=0,CIGLN=0,BWTG=0,GEST=0,PLUR=0,MAGE=0,PARITY=0,INCOME=0))
## We chose not to replace NaN IMR with 0.
```


```{r, include=FALSE}
brdrcounts$IMR<-log(brdrcounts$IMR)
```

The result of all these manipulations is a data frame `brdrcounts` which looks like this:

```{r}
head(brdrcounts)
```
Most of the columns are explained in the file `Dataset Descriptions.xls`; "IMR" is the log of infant mortality rate, and "Cluster" is just a duplicate of "CORES" (it's an artifact from when I tried to apply clustering to the data).

## Trying Different Models

Now, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.

The first method we try is just a linear model; we perform subset selection by validation-set MSE.

```{r, include=FALSE}
#brdrcounts$YEAR<-as.factor(brdrcounts$YEAR)
brdrcounts$RACE<-as.factor(brdrcounts$RACE)
brdrcounts$CORES<-as.factor(brdrcounts$CORES)
train.data<-brdrcounts[brdrcounts$YEAR!=2016,]
test.data<-brdrcounts[brdrcounts$YEAR==2016,]
foldsnums<-sample(1:10,nrow(train.data),replace=TRUE)
fits<-list(length=10)
for(i in 1:10){
  foldslogical<-foldsnums==i
  fits[[i]]<- glm(IMR~.-CORES,data=train.data[,1:14],subset=foldslogical,na.action = na.exclude)
}
library(leaps)
regfit<-regsubsets(IMR~.-CORES,data=train.data[,1:14],nvmax=35)
val.errors<-rep(NA,30)
test.mat<-model.matrix(IMR~.-CORES,data=test.data[1:14])
for (i in 1:10) {
  coefi<-coef(regfit,id=i)
  pred<-test.mat[,names(coefi)]%*% coefi
  val.errors[i]<-mean((na.omit(test.data$IMR)-pred)^2)
}
```

```{r}
which.min(val.errors)
coef(regfit,which.min(val.errors))
subset.error<-val.errors[which.min(val.errors)]
```

The best model seems to associate a decline in infant mortality rate if the mother is American Indian or "Other" (not White, Black, or American Indian).
It's hard to understand the sign of the coefficients for "CIGPN" and "CIGFN".
My guess is that this has to do with the fact that I imputed a slightly lower-than-average infant mortality rate when the death count for a given county, race, and year is zero.


Let's now try lasso regression.

```{r}
x<-model.matrix(IMR~.-CORES,train.data[,1:14])[,-1]
y<-na.omit(train.data$IMR)
set.seed(1)
cv.lasso<-cv.glmnet(x,y,alpha=1,family="gaussian")
plot(cv.lasso)
coef(cv.lasso,cv.lasso$lambda.min)
coef(cv.lasso,cv.lasso$lambda.1se)
```
The 1se lambda value gives a model in which the only predictor is RACE2 (African American).

```{r, include=FALSE}
simple.lasso.model <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = cv.lasso$lambda.1se)
best.lasso.model <- glmnet(x,y,alpha=1,family="gaussian",lambda=cv.lasso$lambda.min)
test.x<-model.matrix(IMR~.-CORES,test.data[,1:14])[,-1]
simple.preds<-predict(simple.lasso.model,test.x)
best.preds<-predict(best.lasso.model,test.x)
simple.lasso.error<-mean((simple.preds-na.omit(test.data$IMR))^2)
best.lasso.error<-mean((best.preds-na.omit(test.data$IMR))^2)
```

Now we try ridge regression:
```{r}
cv.ridge<-cv.glmnet(x,y,alpha=0,family="gaussian")
plot(cv.ridge)
coef(cv.ridge,cv.ridge$lambda.min)
coef(cv.ridge,cv.ridge$lambda.1se)
```

```{r, include=FALSE}
simple.ridge.model <- glmnet(x, y, alpha = 0, family = "gaussian", lambda = cv.ridge$lambda.1se)
best.ridge.model <- glmnet(x,y,alpha=0,family="gaussian",lambda=cv.ridge$lambda.min)
test.x<-model.matrix(IMR~.-CORES,test.data[,1:14])[,-1]
simple.preds<-predict(simple.ridge.model,test.x)
best.preds<-predict(best.ridge.model,test.x)
simple.ridge.error<-mean((simple.preds-na.omit(test.data$IMR))^2)
simple.ridge.error
best.ridge.error<-mean((best.preds-na.omit(test.data$IMR))^2)
```

Let's try to train a single tree.

```{r}
library(tree)
tree.model<-tree(IMR~.-CORES,train.data)
summary(tree.model)
plot(tree.model)
text(tree.model,pretty=0)
tree.preds<-predict(tree.model,na.omit(test.data))
tree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)
```
The most notable differences in IMR come from GEST and RACE2.


Let's do some tree pruning. The following graph shows that the minimum deviance is obtained via a tree with 6 nodes; however, there doesn't seem to be much difference between a tree with 3 nodes and a tree with 6 nodes.

```{r}
set.seed(2)
tree.cv<-cv.tree(tree.model)
plot(tree.cv$size,tree.cv$dev,type="b")
prune.tree.model<-prune.tree(tree.model,best=3)
plot(prune.tree.model)
text(prune.tree.model,pretty=0)
prune.tree.preds<-predict(prune.tree.model,na.omit(test.data))
prune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)
```

As we noted above, GEST and RACE2 are the major factors in this tree.


Let's try random forests.

```{r}
library(randomForest)
set.seed(12)
rf.model<-randomForest(IMR~.-CORES,na.omit(train.data[,1:14]), importance=TRUE)
rf.preds<-predict(rf.model,newdata=na.omit(test.data))
rf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)
varImpPlot(rf.model)
```
Again, "RACE", "BWTG", and the various "CIG" predictors appear near the top of the %IncMSE chart.


```{r, include=FALSE}
totalbirths<-count(br,YEAR)
totaldeaths<-count(dr,YEAR)
totalIMR<-totaldeaths$n/totalbirths$n
avgIMR<-mean(totalIMR[1:5])
```

Last model is boosting:

```{r}
library(gbm)
set.seed(15)
boost.model<-gbm(IMR~.-CORES,data=na.omit(train.data),distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.model)
boost.preds<-predict(boost.model,newdata=na.omit(test.data))
boost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)
```

##Summary of Results

Now, we summarize our results in a table:

```{r}
mean.error<-mean((na.omit(test.data$IMR)-log(avgIMR+.007))^2)
test.error.data<-data.frame(Method=c("No Dependence on Predictors","Best Subset Linear Model","Simple Lasso","Lowest MSE Lasso", "Simple Ridge", "Lowest MSE Ridge", "Tree","Pruned Tree","Random Forests","Boosting"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,simple.ridge.error, best.ridge.error, tree.error,prune.error,rf.error,boost.error))
test.error.data
```
Random forests seems to have done the best. This is consistent with its reputation as the best out-of-the-box method.
Boosting is finnicky, and probably required some more hyperparameter tuning.

##Graphs and Visualizations
This is a graph of infant mortality rates by race and year.
```{r, echo=FALSE}
br.dr.race.year<-count(br,RACE,YEAR)
br.dr.race.year<-rename(br.dr.race.year,numbirth=n)
br.dr.race.year<-left_join(br.dr.race.year, count(dr,RACE,YEAR))
br.dr.race.year<-rename(br.dr.race.year,numdeath=n)
data.plot<-ggplot() + 
  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = "White"), data=br.dr.race.year[br.dr.race.year$RACE==1,], )+
 geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = "Black"), data=br.dr.race.year[br.dr.race.year$RACE==2,], )+
  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = "Indian"), data=br.dr.race.year[br.dr.race.year$RACE==3,], )+
  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = "Other"), data=br.dr.race.year[br.dr.race.year$RACE==4,], )+
scale_color_manual(name = "Race", values = c("White" = "darkred", "Black" = "darkblue","Indian"="darkgreen","Other"="black"))+
labs(title = "NC Infant Mortality Rate by Race and Year", y="Infant Mortality Rate", x="Year")+
  theme(plot.title = element_text(hjust = 0.5))
data.plot
```

This is a heat map of North Carolina by infant mortality rate:

```{r, echo=FALSE}
br.dr.county<-count(br,subregion)
br.dr.county<-rename(br.dr.county,numbirth=n)
dr.county<-count(dr,CORES)
dr.county<-left_join(dr.county,county.conversion[,1:2])
dr.county<-rename(dr.county,subregion=COUNTY)
dr.county<-rename(dr.county,numdeath=n)
dr.county$subregion<-tolower(dr.county$subregion)
br.dr.county<-left_join(br.dr.county,dr.county)
imr<-br.dr.county$numdeath/br.dr.county$numbirth
br.dr.county<-bind_cols(br.dr.county,IMR=imr)
nccodr<-inner_join(nc_county,br.dr.county,by="subregion")
drmap <- nc_base + 
      geom_polygon(data = nccodr, aes(fill = IMR), color = "white") +
      geom_polygon(color = "black", fill = NA) +
      theme_bw() +
      ditch_the_axes+
      scale_fill_gradient(
        low = "#00FFFF",
        high = "#000000",
        space = "Lab",
        na.value = "grey50",
        guide = "colourbar",
        aesthetics = "fill"
        )+
  labs(title = "NC Infant Mortality Rates by County")+
  theme(plot.title = element_text(hjust = 0.5))
drmap
```

## Conlcusion

It was interesting to go back to this project a few months after I first did it, because I noticed a lot of places where what I have learned in the interim could have come in handy.
For example, I now have more robust EDA and feature engineering frameworks.
Keep an eye out for a future blog post in which I discuss these issues in more depth in the context of my participation in recent Kaggle competitions.

