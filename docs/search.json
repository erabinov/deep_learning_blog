[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eugene’s Deep Learning Blog",
    "section": "",
    "text": "Improvements to a Classical Simulation of a Quantum Algorithm\n\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on the Value-Loading Problem\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat I’ve Learned from My First Kaggle Competitions\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRubik’s Cube Solver\n\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing a Neural-Net Digit Classifier (Almost) from Scratch\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Universal Approximation Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog!\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi everybody! My name is Eugene! Until June 2023, I was a postdoc in pure mathematics at the University of Notre Dame. I received my PhD in mathematics from University of California, Berkeley in 2021. For the past year and a half, I’ve been learning a lot about computer science, and in particular machine learning, data science, quantum computing, and AI safety. This blog collects some thoughts and projects I’ve worked on as part of this journey."
  },
  {
    "objectID": "posts/first-ipynb-post/index.html",
    "href": "posts/first-ipynb-post/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces, so I may have to periodically rebuild. If it doesn’t work for you, raise an issue on the GitHub page, and I will rebuild.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay."
  },
  {
    "objectID": "posts/10-01-23-post-1/index.html",
    "href": "posts/10-01-23-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/12-01-23-post-1/index.html",
    "href": "posts/12-01-23-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left(b_i+ \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html",
    "href": "posts/18-01-23-post-1/index.html",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "",
    "text": "In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.\nIt actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.\n\n\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\((N-1)\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}(w,x_i).\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) is something that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand it, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\mathbb{R}^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "href": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?",
    "text": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\(N-1\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}.\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand this, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\RR^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the maximum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02<00:00]\n    \n    \n\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('testing'),Path('training')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/1'),Path('training/2'),Path('training/5'),Path('training/9'),Path('training/7'),Path('training/6'),Path('training/0'),Path('training/8'),Path('training/4'),Path('training/3')]\n\n\n\nnums = [(path/'training'/str(i)).ls() for i in range(10)]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "href": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\nStep one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\nLet’s create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\nWe will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:\n\nnum_tensors = [[tensor(Image.open(o)) for o in nums[i]] for i in range(10)]\n\n\nlen(num_tensors), len(num_tensors[2])\n\n(10, 5958)\n\n\n\nstacks = [torch.stack(num_tensors[i]).float()/255 for i in range(10)]\nstacks[2].shape\n\ntorch.Size([5958, 28, 28])\n\n\n\nmeans = [stacks[i].float().mean(0) for i in range(10)]\n[show_image(o) for o in means]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "href": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\n#An ordinary Python list of pyTorch tensors, with the validation images for digit i in the ith index.\nvalids = [torch.stack([tensor(Image.open(o)) \n                            for o in (path/'testing'/str(i)).ls()]) for i in range(10)]\nvalids = [o.float()/255 for o in valids]\n\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\n\ndef mnist_distance_list(x):\n  dists = [mnist_distance(x,means[i]) for i in range(10)]\n  return torch.stack(dists)\n\ndef which_is_it(x):\n  return torch.argmin(mnist_distance_list(x),dim=0)\n\nLet’s test it on our example case:\n\na_3 = stacks[3][2007]\nmnist_distance_list(a_3), which_is_it(a_3), show_image(a_3)\n\n(tensor([0.1583, 0.1372, 0.1469, 0.1612, 0.1534, 0.1595, 0.1483, 0.1514, 0.1612, 0.1485]),\n tensor(1),\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7dc64648b0>)\n\n\n\n\n\n\nsmall_test = stacks[3][0:2]\nsmall_test.shape\nmnist_distance_list(small_test[0]), mnist_distance_list(small_test[1]),mnist_distance_list(small_test)\n\n(tensor([0.1826, 0.1183, 0.1458, 0.1037, 0.1429, 0.1319, 0.1613, 0.1408, 0.1254, 0.1331]),\n tensor([0.1965, 0.1569, 0.1953, 0.1421, 0.1825, 0.1581, 0.1766, 0.1784, 0.1759, 0.1732]),\n tensor([[0.1826, 0.1965],\n         [0.1183, 0.1569],\n         [0.1458, 0.1953],\n         [0.1037, 0.1421],\n         [0.1429, 0.1825],\n         [0.1319, 0.1581],\n         [0.1613, 0.1766],\n         [0.1408, 0.1784],\n         [0.1254, 0.1759],\n         [0.1331, 0.1732]]))\n\n\n\nwhich_is_it(small_test[0]),which_is_it(small_test[1]), which_is_it(small_test), which_is_it(small_test)==3\n\n(tensor(3), tensor(3), tensor([3, 3]), tensor([True, True]))\n\n\n\naccuracies = [(which_is_it(valids[i])==i).float().mean() for i in range(10)]\noverall_accuracy = tensor([accuracies[i]*len(valids[i]) for i in range(10)])\noverall_accuracy = overall_accuracy.sum()/10000.\naccuracies, overall_accuracy\n\n([tensor(0.8153),\n  tensor(0.9982),\n  tensor(0.4234),\n  tensor(0.6089),\n  tensor(0.6680),\n  tensor(0.3262),\n  tensor(0.7871),\n  tensor(0.7646),\n  tensor(0.4425),\n  tensor(0.7760)],\n tensor(0.6685))"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "href": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\ntrain_x = torch.cat([stacks[i] for i in range(10)]).view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\ntrain_y = tensor([])\nfor i in range (10):\n  add = torch.stack([tensor([i]) for j in range(len(nums[i]))])\n  train_y=torch.cat((train_y,add),0)\ntrain_y= train_y.squeeze()\ntrain_x.shape, train_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000]))\n\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor(0.))\n\n\n\nvalid_x = torch.cat([valids[i] for i in range(10)]).view(-1, 28*28)\nvalid_y = tensor([])\nfor i in range(10):\n  add=torch.stack([tensor([i]) for j in range(len(valids[i]))])\n  valid_y =torch.cat((valid_y,add),0)\nvalid_y = valid_y.squeeze()\nvalid_dset = list(zip(valid_x,valid_y))\n\n\nvalid_x.shape, valid_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000]))\n\n\nNow we need an (initially random) weight for every pixel (this is the initialize step in our seven-step process):\n\ndef init_params(size, std=1.0): \n  ungrad = (torch.randn(size)*std).squeeze()\n  return ungrad.requires_grad_()\n\n: \n\n\n: \n\n\n\nweights = init_params((28*28,10))\nweights.shape\n\ntorch.Size([784, 10])\n\n\n\nbias = init_params((1,10))\nbias.shape\n\ntorch.Size([10])\n\n\n\n((train_x.float()@(weights)) + bias).shape\n\ntorch.Size([60000, 10])\n\n\n\ndef linear1(xb): return xb.float()@weights + bias\npreds_log_prob = linear1(train_x)\n\n\npreds_log_prob[:4]\n\ntensor([[-18.1373,  -6.1067,   2.9795,  -1.2941,  -2.0108,  -1.8045,  -5.9375, -10.6166,   6.8176,   9.3555],\n        [ -8.9188,   4.6967,  -3.0078,   2.0427,  -9.5952,   6.2474,  -1.6729,  -6.3322,  13.9906,  27.3094],\n        [-12.4062,  -0.5152,   6.4807,   4.4728, -13.1531,   2.5008,  -1.0917, -12.9701,   7.1787,  10.7713],\n        [ -9.2247,  11.5279,  -0.7430,   1.1205,  -5.2792, -11.2010, -11.0678, -11.6129,   5.9237,  15.7904]], grad_fn=<SliceBackward0>)\n\n\n\npreds= torch.argmax(preds_log_prob,1)\npreds = preds.reshape(60000)\npreds.shape, preds\n\n(torch.Size([60000]), tensor([9, 9, 9,  ..., 8, 9, 8]))\n\n\n\ncorrects = preds.float()==train_y.squeeze()\ncorrects\n\ntensor([False, False, False,  ..., False,  True, False])\n\n\n\ncorrects.float().mean().item()\n\n0.12043333053588867\n\n\n\ndef mnist_loss(predictions, targets):\n    targets = targets.long()\n    losses = F.cross_entropy(predictions,targets.squeeze())\n    return losses.mean()\n\n\nSGD and Mini-Batches\n\nweights = init_params((28*28,10))\nbias = init_params(10)\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nbatch = train_x[:4]\nbatch.shape, train_y[:4].shape\n\n(torch.Size([4, 784]), torch.Size([4, 1]))\n\n\n\npreds = linear1(batch)\npreds, train_y[:4]\n\n(tensor([[ 19.2725,  22.0151, -27.2578,  10.6764,  -4.2019,  22.7855,   6.2560,   4.3134,  -0.2266,  -5.1313],\n         [ 18.2444,   4.6494, -17.6840,  10.6016, -12.3112,  23.4429,   3.3657,   5.7126,   3.6596,   5.7853],\n         [ 25.9703,   9.6952, -19.0290,  10.1290,   4.2884,  21.3991,   5.5114,  -1.0090,   9.3477, -20.2521],\n         [  8.6966,   1.3858,  -6.3632,   1.8474,   2.0362,  23.2705, -12.0754,   3.2197,   4.8403,   2.4272]], grad_fn=<AddBackward0>),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(5.9254, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\nbias.grad\n\ntensor([-7.4619e-01,  7.7517e-02,  3.3744e-14,  1.6141e-06,  2.4515e-10,  6.6868e-01,  1.1896e-08,  7.0398e-09,  1.8070e-08,  5.5542e-09])\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-5.5499e-10),\n tensor([-6.0718e-02,  2.6098e-06,  8.3043e-06,  3.9334e-06,  4.4649e-02,  1.0095e-02,  1.9338e-03,  2.0388e-14,  1.0862e-08,  4.0262e-03]))\n\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\ndef batch_accuracy(xb, yb):\n    preds = torch.argmax(xb,dim=1).squeeze()\n    correct = preds.float() == yb.float().squeeze()\n    return correct.float().mean()\n\nWe can check it works:\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.2500)\n\n\nand then put the batches together:\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.1018\n\n\nThat’s our starting point. Let’s train for one epoch, and see if the accuracy improves:\n\nlr = .1\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.7798\n\n\nThen do a few more:\n\nfor i in range(30):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end='\\n')\n\n0.8844 0.8854 0.8856 0.8866 0.8869 0.8869 0.8861 0.8859 0.8862 0.8864 0.8864 0.8866 0.887 0.8873 0.8873 0.8872 0.8872 0.8874 0.8875 0.8876 0.8876 0.8878 0.8877 0.8877 0.8881 0.8881 0.888 0.888 0.8881 0.8881 \n\n\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,10))\nb2 = init_params(10)\n\n\ntrain_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\nprint(validate_epoch(simple_net), end=' ')\n\n\nfor i in range(150):\n  train_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\n  if i%15==0:\n    print(validate_epoch(simple_net), end='\\n')"
  },
  {
    "objectID": "posts/02-02-23-post-1/index.html",
    "href": "posts/02-02-23-post-1/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "For this blog post, I want to talk about principal components analysis (PCA). This is something I learned about from the great book Introduction to Statistical Learning with R by James, Witten, Hastie, and Tibshirani. This is another one of those “delve deeper into the math theory” posts.\nSo the book presents two different equivalent formulations of the principal components problem. To study this, let \\(X\\) be our data set, represented as a \\(n\\times p\\) matrix. We assume that the columns of \\(X\\) have zero mean. We will use symbols like \\(\\phi_1\\) to represent a \\(p\\)-vector, and symbols like \\(z_1\\) to represent an \\(n\\)-vector. We will think of vectors always as column vectors and use the superscript \\(T\\) to denote the corresponding row vectors, e.g. \\(\\phi_1^T\\). The first formulation of the PCA problem is in the following optimization problem: we want to maximize \\(\\phi^T X^T X \\phi= ||X\\phi||^2\\) by varying \\(\\phi\\) among all unit \\(p\\)-vectors. The vector \\(z_1 = X\\phi_1\\) is the linear combination of our features with the highest variance (for this interpretation, it was important to set the columns of \\(X\\) to have zero mean). The matrix \\(X^T X\\) is symmetric and positive-semi-definite, so it can be diagonalized and all its eigenvalues are non-negative. Let us assume that \\(X^T X\\) has only positive eigenvalues, since a zero eigenvalue would mean that some linear combination of the columns of \\(X\\) is zero, and therefore that one of the features encoded in \\(X\\) is redundant. Let \\(y_1,\\ldots, y_p\\) be the eigenvectors of \\(X^TX\\), and \\(\\lambda_1,\\ldots, \\lambda_p\\) be the corresponding eigenvalues, arranged in increasing order. Then, if we write \\[\\phi = c_1y_1+\\cdots+c_py_p,\\] we have \\[||\\phi||^2 = \\sum_i c_i^2\\] and \\[||X\\phi||^2 = \\sum_i \\lambda_i c_i^2.\\] This is a constrained optimization problem (maximize \\(\\sum_i c_i^2\\lambda_i\\) subject to \\(\\sum_i c_i^2=1\\)) and it can be solved by Lagrange multipliers. The maximum value for the objective function is \\(\\lambda_p\\) and is obtained when \\(c_p=1\\) and the other \\(c\\)s are zero. Then, the first principal component loading vector \\(\\phi_1 =y_p\\) is the eigenvector of \\(X^TX\\) corresponding to the highest eigenvalue, and the corresponding principal component is \\(z_1 = X\\phi_1\\). Next, we seek \\(z_2 = X\\phi_2\\) such that \\(z_2^T z_2\\) is maximal among \\(z_2\\) such that \\(z_2^T z_1=0\\). Since \\(z_1 = Xy_p\\) and \\(z_2 = X\\phi_2\\), we have \\[z_2^T z_1 = \\phi_2^T X^TX y_p = \\lambda_p \\phi_2^T y_p=\\lambda_p \\phi_2^T\\phi_1.\\] So, because \\(\\lambda_p\\neq 0\\), \\(z_1\\) and \\(z_2\\) are orthogonal only if \\(\\phi_1\\) and \\(\\phi_2\\) are orthogonal. So, finding \\(\\phi_2\\) amounts to finding the next highest eigenvalue \\(\\lambda_{p-1}\\) and the corresponding eigenvector \\(y_{p-1}\\) (the orthogonal complement to \\(\\phi_1\\) is spanned by \\(y_1,\\ldots, y_{p-1}\\), so we simply repeat the argument that got \\(y_1\\) on this smaller space). And so on, till we’ve found the \\(m\\) largest eigenvalues of \\(X^TX\\) and have decided to stop. To recap: in PCA, we\n\nFind the linear combination \\(z_1=X\\phi_1\\) of the columns of \\(X\\) which has maximal variance. This corresponds to picking the eigenvector of \\(X^TX\\) with maximal eigenvalue.\nAmong the linear combinations \\(z_2 = X\\phi_2\\) of the columns of \\(X\\) wich \\(z_2\\) orthogonal to \\(z_1\\), we choose the one which has maximal variance. This amounts to picking out the second-largest eigenvalue of \\(X^TX\\).\nAnd so on… We pick out the \\(m\\) largest eigenvalues of \\(X^TX\\), their corresponding eigenvectors, and their corresponding principal components.\n\nThe book claims but doesn’t show that it this procedure is equivalent to considering the following optimization problem instead: minimize \\(\\mathrm{tr}((X-AB)^T(X-AB))\\) as a\\(A\\) ranges over the space of \\(n\\times m\\) matrices and \\(B\\) ranges over the space of \\(m \\times p\\) matrices. Let’s show this. First, let’s note that we can write \\(AB = \\sum_{j=1}^m z_i \\phi_i^T\\), where the \\(z_i\\) are \\(n\\) vectors and the \\(\\phi_i\\) are \\(p\\)-vectors (unrelated so far to the ones found from the first formulation of PCA). We may assume that the \\(\\phi_i\\) are linearly independent (for if we can write one \\(\\phi\\) as a linear combination of the others, we can manifest this as a redefinition of the \\(z\\)’s). For a similar reason, we may assume that the \\(\\phi_i\\) are orthonormal. Under these assumptions, we may write \\[\n\\begin{aligned}\n\\mathrm{tr}((X-AB)^T(X-AB))&= \\mathrm{tr}(X^TX)-2\\sum_{i} \\mathrm{tr}(X^T z_i \\phi_i^T)+\\sum_{i,j}\\mathrm{tr}(\\phi_i z_i^T z_j \\phi_j^T)\\\\\n&= \\mathrm{tr}(X^TX)- 2\\sum_{i} \\phi_i^T X^T z_i + \\sum_{i} z_i^T z_j,\n\\end{aligned}\n\\] where we have used that \\[\n\\mathrm{tr}(\\phi' \\phi^T) = ||\\phi||^2 \\phi \\cdot \\phi'\n\\] for any two \\(p\\)-vectors \\(\\phi\\), \\(\\phi'\\). So, we are trying to find \\(z_i\\) and \\(\\phi_i\\) to minimize the above trace, subject to the condition that \\(\\phi_i \\cdot \\phi_j = \\delta_{i,j}\\) (i.e. the \\(\\phi\\)’s are orthonormal). There is no constraint on the \\(z\\)’s, and the simple optimization problem for the \\(z\\)’s gives \\[z_i = X\\phi_i\\]. The optimization problem with respect to the \\(y_i\\) is a bit more subtle because the \\(\\phi\\)’s are constrained. But the Lagrange optimization problem tells us that \\[\nX^Tz_i =X^TX\\phi_i = \\sum_j \\beta_j \\phi_j,\n\\] where the \\(\\beta_i\\) are undetermined Lagrange multipliers. But this tells us that \\(X^TX\\) preserves the space spanned by the \\(\\phi_i\\). Let’s call this space \\(W\\). Taking all this into account, the objective function becomes \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX).\n\\] To compute the second trace in the above equation, we just need to know the eigenvalues of \\(X^TX\\) when restricted to \\(W\\). Let us suppose that the eigenvalues of \\(X^TX\\) on \\(W\\) are \\(\\lambda_{i_1},\\ldots, \\lambda_{i_m}\\). Let \\(\\lambda_{i_{m+1}},\\ldots, \\lambda_{i_{p}}\\) be the remaining eigenvalues. Then, \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX)=\\sum_{j=m+1}^p\\lambda_{i_j}.\n\\] So the objective will be minimized precisely by choosing the \\(m\\) largest eigenvalues of \\(X^TX\\), just as in the first formulation of PCA! It follows that to minimze \\[\\mathrm{tr}((X-AB)^T(X-AB)),\\] we choose the \\(m\\) largest eigenvalues of \\(X^TX\\) (which above we called \\(\\lambda_p, \\lambda_{p-1},\\ldots, \\lambda_{p-m+1}\\), with respective eigenvectors \\(y_p,\\ldots, y_{p-m+1}\\)), set \\(\\phi_i = y_{p-i+1}\\), \\(z_i = Xy_{p-i+1}\\), and \\(AB = \\sum_{i=1}^m z_i \\phi_i^T\\), as desired."
  },
  {
    "objectID": "posts/07-03-23-post-1/NC_Birth_and_Death.html",
    "href": "posts/07-03-23-post-1/NC_Birth_and_Death.html",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "",
    "text": "Loading the Data and Packages\nI’m using Google Maps data here.\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(urbnmapr)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(mapdata)\nlibrary(glmnet)\n\nSet up the base NC map:\n\nditch_the_axes <- theme(\n  axis.text = element_blank(),\n  axis.line = element_blank(),\n  axis.ticks = element_blank(),\n  panel.border = element_blank(),\n  panel.grid = element_blank(),\n  axis.title = element_blank()\n  )\nstates<-map_data(\"state\")\nnc_df<-subset(states,region==\"north carolina\")\nnc_base <- ggplot(data = nc_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.3) + \n  geom_polygon(color = \"black\", fill = \"gray\")\ncounties<-map_data(\"county\")\nnc_county<-subset(counties,region==\"north carolina\")\n\nWe read the data in.\n\nraw.bd1<-read_csv(\"data/birth_data.csv\")\nraw.bd1$Year<-as.integer(raw.bd1$Year)\nraw.bd2<-read_csv(\"data/birth_data2.csv\")\nbr<-read_csv(\"data/Yr1116Birth.csv\")\ndr<-read_csv(\"data/Yr1116Death.csv\")\ncounty.conversion<-read_csv(\"data/County_Codes_Table_1.csv\")\ncounty.conversion<-county.conversion[,1:3]\nincomes<-read_csv(\"data/Income_by_Location.csv\")\nincomes$Geography<-str_sub(incomes$Geography,1,-12)\nincomes<-rename(incomes,subregion=`Geography`)\n#Resolve YOB vs. YEAR column names.\nbr<-rename(br, YEAR=YOB)\n#Resolve MRACER vs. RACE column names.\nbr<-rename(br, RACE=MRACER)\n\nWe massage br so that we can compare the br and dr data, and so that we will be able to make a map later.\n\n## The birth and death tables contain different codings for race, so I have to change that smh. I begin by defining a function \"collapse\" which translates the birth data race coding to dezath data race  coding. \ncollapse<-function(x){\n  if(x==1|x==2|x==3)\n    return(x)\n  else\n    return(4)\n}\nbr$RACE<-sapply(br$RACE,collapse)\ncounty.conversion$CORES<-as.double(county.conversion$CORES)\nbr<-left_join(br,county.conversion[,1:2])\nbr$COUNTY<-tolower(br$COUNTY)\nbr<-rename(br, subregion=COUNTY)\nsimple_incomes<-incomes[incomes$`ID Race`==0&incomes$`ID Year`==2016,c(5,7)]\nsimple_incomes$subregion<-tolower(simple_incomes$subregion)\nbr<-left_join(br, simple_incomes)\n\nIn order to minimze some of the variance associated with the small county data, we form 17 clusters of counties, and we will take in-cluster means of all the data in br.\n\ncountycounts<-count(br,CORES)\ncountyracecounts<-count(br,CORES,RACE)\nrace1births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==1,]$n)\n}\nrace2births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==2,]$n)\n}\nrace3births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==3,]$n)\n}\nrace4births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==4,]$n)\n}\ncountycounts<-bind_cols(countycounts,race1births,race2births,race3births,race4births)\ncountycounts<-rename(countycounts,NUMBIRTH=n,NUM1=...3,NUM2=...4,NUM3=...5,NUM4=...6)\n\n##We try K-means clustering with about 20 clusters. We try to identify clusters with  We want population to have the largest effect, so we don't normalize the data. The result is probably that race plays a very small role in the size of the clusters.\nset.seed(1)\nkm.out<-kmeans(countycounts[,2:6],17,nstart=50)\nkm.out$cluster\n\n  [1]  4  9 10 12 12 11  8 11  9 14 16  2 16 15 10 17 12  4 17 12 11 10 14 17  3\n [26]  6 12  9  4  9  2  1 17  1 15 16 10 10 17 12  6 17  4 17 14 12 14  7  4  9\n [51] 16 10  2 15 15  8  9 12 12 13 11  9 14 14 16 11  1  5 10  8 17 11  9 16 11\n [76]  3 17  4 14  3 15  2  8 15  9 15 12 12  7 16 17 13 11 11  9  4 15 14  9 11\n\ncountycounts<-bind_cols(countycounts,CLUSTER=km.out$cluster)\n\nCOREStoCLUSTER<-function(x){\n  if(0<=x&x<=100)\n    return(km.out$cluster[x])\n  else\n    return(NA)\n}\nclustervals<-sapply(br$CORES,COREStoCLUSTER)\nbr<-bind_cols(br,CLUSTER=clustervals)\ndrclustervals<-sapply(dr$CORES,COREStoCLUSTER)\ndr<-bind_cols(dr,drclustervals)\ndr<-rename(dr,CLUSTER=...6)\n\n\nbrdrcounts<-count(br,YEAR,CLUSTER,RACE)\nbrdrcounts<-rename(brdrcounts,NUMBIRTH=n)\n#I noticed, for example, that the original brdrcounts had no entry for (YEAR,CORES,RACE)=(2011,3,3). Based on a superficial check of a few of the missing values, this seems plausible, but I still want to set NUMBIRTH=0 for the missing combinations. Mostly RACE=3 rows are missing, which seems plausible since the Native American population of NC is around 1-2%. \nbrdrcounts<-complete(brdrcounts,YEAR,CLUSTER,RACE)\nbrdrcounts<-replace_na(brdrcounts,list(NUMBIRTH=0))\ndrcounts<-count(dr,YEAR,CLUSTER,RACE)\n#Same sort of completion for death counts. There are even more missing combos, since the death numbers overall in a low-population county can be quite small.\ndrcounts<-complete(drcounts,YEAR,CLUSTER,RACE)\ndrcounts<-replace_na(drcounts,list(n=0))\n\nbrdrcounts<-left_join(brdrcounts,drcounts)\nbrdrcounts<-rename(brdrcounts,NUMDEATH=n)\n\n##I add the group means for each statistic in br to the birth and death counts.\nbrdrcounts<-bind_cols(brdrcounts,CIGPN=rep(0,nrow(brdrcounts)),CIGFN=rep(0,nrow(brdrcounts)),CIGSN=rep(0,nrow(brdrcounts)),CIGLN=rep(0,nrow(brdrcounts)),BWTG=rep(0,nrow(brdrcounts)),GEST=rep(0,nrow(brdrcounts)),PLUR=rep(0,nrow(brdrcounts)), MAGE=rep(0,nrow(brdrcounts)),PARITY=rep(0,nrow(brdrcounts)),INCOME=rep(0,nrow(brdrcounts)))\n\nfor (i in 2011:2016) {\n  for(j in 1:17){\n   for(k in 1:4){\n     subset<-br[br$YEAR==i&br$CLUSTER==j&br$RACE==k,]\n     cigpn<-mean(subset$CIGPN)\n     cigfn<-mean(subset$CIGFN)\n     cigsn<-mean(subset$CIGSN)\n     cigln<-mean(subset$CIGLN)\n     bwtg<-mean(subset$BWTG)\n     gest<-mean(subset$GEST)\n     plur<-mean(subset$PLUR)\n     mage<-mean(subset$MAGE)\n     parity<-mean(subset$PARITY)\n     incomes<-mean(subset$`Household Income by Race`)\n     n<-which(brdrcounts$YEAR==i&brdrcounts$CLUSTER==j&brdrcounts$RACE==k)[[1]]\n     brdrcounts[n,6]<-cigpn\n     brdrcounts[n,7]<-cigfn\n     brdrcounts[n,8]<-cigsn\n     brdrcounts[n,9]<-cigln\n     brdrcounts[n,10]<-bwtg\n     brdrcounts[n,11]<-gest\n     brdrcounts[n,12]<-plur\n     brdrcounts[n,13]<-mage\n     brdrcounts[n,14]<-parity\n     brdrcounts[n,15]<-incomes\n     } \n  }\n}\nbrdrcounts<-replace_na(brdrcounts,list(CIGPN=0,CIGFN=0,CIGSN=0,CIGLN=0,BWTG=0,GEST=0,PLUR=0,MAGE=0,PARITY=0,INCOME=0))\nimrates<-brdrcounts$NUMDEATH/brdrcounts$NUMBIRTH\nbrdrcounts<-bind_cols(brdrcounts,IMR=imrates)\n## We chose not to replace NaN IMR with 0.\n\nThese are the indices in brdrcounts of the CLUSTER/RACE/YEAR combos which have no births:\n\nwhich(brdrcounts$NUMBIRTH==0)\n\n[1]  27  95 163 231 299 367\n\n\nI’ll take that, as better than having many.\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.\n\n#brdrcounts$YEAR<-as.factor(brdrcounts$YEAR)\nbrdrcounts$RACE<-as.factor(brdrcounts$RACE)\nbrdrcounts$CLUSTER<-as.factor(brdrcounts$CLUSTER)\ntrain.data<-brdrcounts[brdrcounts$YEAR!=2016,]\ntest.data<-brdrcounts[brdrcounts$YEAR==2016,]\nfoldsnums<-sample(1:10,nrow(train.data),replace=TRUE)\nfits<-list(length=10)\nfor(i in 1:10){\n  foldslogical<-foldsnums==i\n  fits[[i]]<- glm(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,subset=foldslogical,na.action = na.exclude)\n}\nlibrary(leaps)\nregfit<-regsubsets(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,nvmax=35)\nval.errors<-rep(NA,30)\ntest.mat<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,data=test.data)\nfor (i in 1:30) {\n  coefi<-coef(regfit,id=i)\n  pred<-test.mat[,names(coefi)]%*% coefi\n  val.errors[i]<-mean((na.omit(test.data$IMR)-pred)^2)\n}\n##logreg.bycty<-glm(SURVIVED~CORES+YEAR+RACE,data=br,subset=train,family=\"binomial\")\n\nThe above is a validation-set approach to variable selection. I find that the best model predicts infant-mortality rates taking into account only whether or not the group is African American:\n\nwhich.min(val.errors)\n\n[1] 1\n\ncoef(regfit,which.min(val.errors))\n\n(Intercept)       RACE2 \n0.005308092 0.009012201 \n\nsubset.error<-val.errors[which.min(val.errors)]\nsubset.error\n\n[1] 0.0002065257\n\n\nLet’s try lasso regression.\n\nx<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,train.data)[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n\n\n\ncoef(cv.lasso,cv.lasso$lambda.min)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  1.732326e-01\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     8.439871e-04\nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11   -7.764165e-04\nCLUSTER12    1.146498e-03\nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        4.158737e-03\nRACE3        .           \nRACE4       -2.297542e-03\nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -4.663241e-06\nGEST        -3.926446e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n\ncoef(cv.lasso,cv.lasso$lambda.1se)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  6.808897e-02\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     .           \nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11    .           \nCLUSTER12    .           \nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        3.111985e-03\nRACE3        .           \nRACE4        .           \nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -7.565233e-07\nGEST        -1.529002e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n\n\n\nsimple.lasso.model <- glmnet(x, y, alpha = 1, family = \"gaussian\", lambda = cv.lasso$lambda.1se)\nbest.lasso.model <- glmnet(x,y,alpha=1,family=\"gaussian\",lambda=cv.lasso$lambda.min)\ntest.x<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,test.data)[,-1]\nsimple.preds<-predict(simple.lasso.model,test.x)\nbest.preds<-predict(best.lasso.model,test.x)\nsimple.lasso.error<-mean((simple.preds-na.omit(test.data$IMR))^2)\nsimple.lasso.error\n\n[1] 0.0002038072\n\nbest.lasso.error<-mean((best.preds-na.omit(test.data$IMR))^2)\nbest.lasso.error\n\n[1] 0.0002161291\n\n\nThe simple model, which uses only RACE2 (African-American), birth weight in grams, and gestation period, is better than the more complex one on the test data. Also, it is better than the best subset selected model.\nAn unpruned tree model does worse than all three:\n\nlibrary(tree)\ntree.model<-tree(IMR~.-NUMBIRTH-NUMDEATH,train.data)\nsummary(tree.model)\n\n\nRegression tree:\ntree(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = train.data)\nVariables actually used in tree construction:\n[1] \"BWTG\"    \"GEST\"    \"CLUSTER\" \"PLUR\"    \"PARITY\"  \"CIGFN\"   \"RACE\"   \n[8] \"MAGE\"   \nNumber of terminal nodes:  14 \nResidual mean deviance:  3.229e-05 = 0.01036 / 321 \nDistribution of residuals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.0242200 -0.0018800 -0.0005768  0.0000000  0.0016300  0.0265900 \n\nplot(tree.model)\ntext(tree.model,pretty=0)\n\n\n\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\ntree.error\n\n[1] 0.0002433887\n\n\nLet’s do some tree pruning. The following shows that the minimum deviance is obtained via a tree with 5 nodes. The pruning improves the test MSE a bit, but it still doesn’t beat the lasso.\n\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n\n\n\nprune.tree.model<-prune.tree(tree.model,best=5)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n\n\n\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\nprune.error\n\n[1] 0.0002200646\n\n\nLet’s try random forests. These don’t provide a dramatic improvement over the single tree. Continue to confirm that RACE, GEST, and BWTG are biggest predictors.\n\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-NUMBIRTH-NUMDEATH,na.omit(train.data), importance=TRUE)\nrf.model\n\n\nCall:\n randomForest(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = na.omit(train.data),      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 5.982042e-05\n                    % Var explained: 18.83\n\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nrf.error\n\n[1] 0.0002202987\n\nvarImpPlot(rf.model)\n\n\n\n\nHave to try boosting:\n\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-NUMBIRTH-NUMDEATH,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n\n\n\n\n            var   rel.inf\nCLUSTER CLUSTER 31.569196\nGEST       GEST 12.781752\nBWTG       BWTG 10.765729\nPLUR       PLUR  9.510822\nRACE       RACE  7.189561\nMAGE       MAGE  6.451966\nPARITY   PARITY  6.013129\nCIGPN     CIGPN  4.016546\nCIGFN     CIGFN  3.494723\nINCOME   INCOME  3.101411\nCIGSN     CIGSN  1.897428\nCIGLN     CIGLN  1.866473\nYEAR       YEAR  1.341264\n\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)\nboost.error\n\n[1] 0.0002398635\n\n\nNow, we summarize our results. As a whole, the regularized linear methods worked the best. Of the trees, the best method was the pruned tree.\n\nmean.error<-mean(na.omit(test.data$IMR)-mean(na.omit(train.data$IMR)))^2\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\",\"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n\n                       Method   Test.Error\n1 No Dependence on Predictors 1.541823e-06\n2    Best Subset Linear Model 2.065257e-04\n3                Simple Lasso 2.038072e-04\n4            Lowest MSE Lasso 2.161291e-04\n5                        Tree 2.433887e-04\n6                 Pruned Tree 2.200646e-04\n7              Random Forests 2.202987e-04\n8                    Boosting 2.398635e-04\n\n\n\nbr.dr.race.year<-count(br,RACE,YEAR)\nbr.dr.race.year<-rename(br.dr.race.year,numbirth=n)\nbr.dr.race.year<-left_join(br.dr.race.year, count(dr,RACE,YEAR))\nbr.dr.race.year<-rename(br.dr.race.year,numdeath=n)\ndata.plot<-ggplot() + \n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"White\"), data=br.dr.race.year[br.dr.race.year$RACE==1,], )+\n geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Black\"), data=br.dr.race.year[br.dr.race.year$RACE==2,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Indian\"), data=br.dr.race.year[br.dr.race.year$RACE==3,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Other\"), data=br.dr.race.year[br.dr.race.year$RACE==4,], )+\nscale_color_manual(name = \"Race\", values = c(\"White\" = \"darkred\", \"Black\" = \"darkblue\",\"Indian\"=\"darkgreen\",\"Other\"=\"black\"))\ndata.plot\n\n\n\n\nGoing to try to make a map of North Carolina, and to see the county FIPs.\n\nbr.dr.county<-count(br,subregion)\nbr.dr.county<-rename(br.dr.county,numbirth=n)\ndr.county<-count(dr,CORES)\ndr.county<-left_join(dr.county,county.conversion[,1:2])\ndr.county<-rename(dr.county,subregion=COUNTY)\ndr.county<-rename(dr.county,numdeath=n)\ndr.county$subregion<-tolower(dr.county$subregion)\nbr.dr.county<-left_join(br.dr.county,dr.county)\nimr<-br.dr.county$numdeath/br.dr.county$numbirth\nbr.dr.county<-bind_cols(br.dr.county,IMR=imr)\nnccodr<-inner_join(nc_county,br.dr.county,by=\"subregion\")\ndrmap <- nc_base + \n      geom_polygon(data = nccodr, aes(fill = IMR), color = \"white\") +\n      geom_polygon(color = \"black\", fill = NA) +\n      theme_bw() +\n      ditch_the_axes+\n      scale_fill_gradient(\n        low = \"#00FFFF\",\n        high = \"#000000\",\n        space = \"Lab\",\n        na.value = \"grey50\",\n        guide = \"colourbar\",\n        aesthetics = \"fill\"\n        )\ndrmap"
  },
  {
    "objectID": "posts/23-03-14-post-1/index.html",
    "href": "posts/23-03-14-post-1/index.html",
    "title": "Rubik’s Cube Solver",
    "section": "",
    "text": "I have uploaded my old Rubik’s Cube project to GitHub here. The project doesn’t use deep learning or ML, but it may be interesting to readers nevertheless."
  },
  {
    "objectID": "posts/23-02-02-post-1/index.html",
    "href": "posts/23-02-02-post-1/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "For this blog post, I want to talk about principal components analysis (PCA). This is something I learned about from the great book Introduction to Statistical Learning with R by James, Witten, Hastie, and Tibshirani. This is another one of those “delve deeper into the math theory” posts.\nSo the book presents two different equivalent formulations of the principal components problem. To study this, let \\(X\\) be our data set, represented as a \\(n\\times p\\) matrix. We assume that the columns of \\(X\\) have zero mean. We will use symbols like \\(\\phi_1\\) to represent a \\(p\\)-vector, and symbols like \\(z_1\\) to represent an \\(n\\)-vector. We will think of vectors always as column vectors and use the superscript \\(T\\) to denote the corresponding row vectors, e.g. \\(\\phi_1^T\\). The first formulation of the PCA problem is in the following optimization problem: we want to maximize \\(\\phi^T X^T X \\phi= ||X\\phi||^2\\) by varying \\(\\phi\\) among all unit \\(p\\)-vectors. The vector \\(z_1 = X\\phi_1\\) is the linear combination of our features with the highest variance (for this interpretation, it was important to set the columns of \\(X\\) to have zero mean). The matrix \\(X^T X\\) is symmetric and positive-semi-definite, so it can be diagonalized and all its eigenvalues are non-negative. Let us assume that \\(X^T X\\) has only positive eigenvalues, since a zero eigenvalue would mean that some linear combination of the columns of \\(X\\) is zero, and therefore that one of the features encoded in \\(X\\) is redundant. Let \\(y_1,\\ldots, y_p\\) be the eigenvectors of \\(X^TX\\), and \\(\\lambda_1,\\ldots, \\lambda_p\\) be the corresponding eigenvalues, arranged in increasing order. Then, if we write \\[\\phi = c_1y_1+\\cdots+c_py_p,\\] we have \\[||\\phi||^2 = \\sum_i c_i^2\\] and \\[||X\\phi||^2 = \\sum_i \\lambda_i c_i^2.\\] This is a constrained optimization problem (maximize \\(\\sum_i c_i^2\\lambda_i\\) subject to \\(\\sum_i c_i^2=1\\)) and it can be solved by Lagrange multipliers. The maximum value for the objective function is \\(\\lambda_p\\) and is obtained when \\(c_p=1\\) and the other \\(c\\)s are zero. Then, the first principal component loading vector \\(\\phi_1 =y_p\\) is the eigenvector of \\(X^TX\\) corresponding to the highest eigenvalue, and the corresponding principal component is \\(z_1 = X\\phi_1\\). Next, we seek \\(z_2 = X\\phi_2\\) such that \\(z_2^T z_2\\) is maximal among \\(z_2\\) such that \\(z_2^T z_1=0\\). Since \\(z_1 = Xy_p\\) and \\(z_2 = X\\phi_2\\), we have \\[z_2^T z_1 = \\phi_2^T X^TX y_p = \\lambda_p \\phi_2^T y_p=\\lambda_p \\phi_2^T\\phi_1.\\] So, because \\(\\lambda_p\\neq 0\\), \\(z_1\\) and \\(z_2\\) are orthogonal only if \\(\\phi_1\\) and \\(\\phi_2\\) are orthogonal. So, finding \\(\\phi_2\\) amounts to finding the next highest eigenvalue \\(\\lambda_{p-1}\\) and the corresponding eigenvector \\(y_{p-1}\\) (the orthogonal complement to \\(\\phi_1\\) is spanned by \\(y_1,\\ldots, y_{p-1}\\), so we simply repeat the argument that got \\(y_1\\) on this smaller space). And so on, till we’ve found the \\(m\\) largest eigenvalues of \\(X^TX\\) and have decided to stop. To recap: in PCA, we\n\nFind the linear combination \\(z_1=X\\phi_1\\) of the columns of \\(X\\) which has maximal variance. This corresponds to picking the eigenvector of \\(X^TX\\) with maximal eigenvalue.\nAmong the linear combinations \\(z_2 = X\\phi_2\\) of the columns of \\(X\\) wich \\(z_2\\) orthogonal to \\(z_1\\), we choose the one which has maximal variance. This amounts to picking out the second-largest eigenvalue of \\(X^TX\\).\nAnd so on… We pick out the \\(m\\) largest eigenvalues of \\(X^TX\\), their corresponding eigenvectors, and their corresponding principal components.\n\nThe book claims but doesn’t show that it this procedure is equivalent to considering the following optimization problem instead: minimize \\(\\mathrm{tr}((X-AB)^T(X-AB))\\) as a\\(A\\) ranges over the space of \\(n\\times m\\) matrices and \\(B\\) ranges over the space of \\(m \\times p\\) matrices. Let’s show this. First, let’s note that we can write \\(AB = \\sum_{j=1}^m z_i \\phi_i^T\\), where the \\(z_i\\) are \\(n\\) vectors and the \\(\\phi_i\\) are \\(p\\)-vectors (unrelated so far to the ones found from the first formulation of PCA). We may assume that the \\(\\phi_i\\) are linearly independent (for if we can write one \\(\\phi\\) as a linear combination of the others, we can manifest this as a redefinition of the \\(z\\)’s). For a similar reason, we may assume that the \\(\\phi_i\\) are orthonormal. Under these assumptions, we may write \\[\n\\begin{aligned}\n\\mathrm{tr}((X-AB)^T(X-AB))&= \\mathrm{tr}(X^TX)-2\\sum_{i} \\mathrm{tr}(X^T z_i \\phi_i^T)+\\sum_{i,j}\\mathrm{tr}(\\phi_i z_i^T z_j \\phi_j^T)\\\\\n&= \\mathrm{tr}(X^TX)- 2\\sum_{i} \\phi_i^T X^T z_i + \\sum_{i} z_i^T z_j,\n\\end{aligned}\n\\] where we have used that \\[\n\\mathrm{tr}(\\phi' \\phi^T) = ||\\phi||^2 \\phi \\cdot \\phi'\n\\] for any two \\(p\\)-vectors \\(\\phi\\), \\(\\phi'\\). So, we are trying to find \\(z_i\\) and \\(\\phi_i\\) to minimize the above trace, subject to the condition that \\(\\phi_i \\cdot \\phi_j = \\delta_{i,j}\\) (i.e. the \\(\\phi\\)’s are orthonormal). There is no constraint on the \\(z\\)’s, and the simple optimization problem for the \\(z\\)’s gives \\[z_i = X\\phi_i\\]. The optimization problem with respect to the \\(y_i\\) is a bit more subtle because the \\(\\phi\\)’s are constrained. But the Lagrange optimization problem tells us that \\[\nX^Tz_i =X^TX\\phi_i = \\sum_j \\beta_j \\phi_j,\n\\] where the \\(\\beta_i\\) are undetermined Lagrange multipliers. But this tells us that \\(X^TX\\) preserves the space spanned by the \\(\\phi_i\\). Let’s call this space \\(W\\). Taking all this into account, the objective function becomes \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX).\n\\] To compute the second trace in the above equation, we just need to know the eigenvalues of \\(X^TX\\) when restricted to \\(W\\). Let us suppose that the eigenvalues of \\(X^TX\\) on \\(W\\) are \\(\\lambda_{i_1},\\ldots, \\lambda_{i_m}\\). Let \\(\\lambda_{i_{m+1}},\\ldots, \\lambda_{i_{p}}\\) be the remaining eigenvalues. Then, \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX)=\\sum_{j=m+1}^p\\lambda_{i_j}.\n\\] So the objective will be minimized precisely by choosing the \\(m\\) largest eigenvalues of \\(X^TX\\), just as in the first formulation of PCA! It follows that to minimze \\[\\mathrm{tr}((X-AB)^T(X-AB)),\\] we choose the \\(m\\) largest eigenvalues of \\(X^TX\\) (which above we called \\(\\lambda_p, \\lambda_{p-1},\\ldots, \\lambda_{p-m+1}\\), with respective eigenvectors \\(y_p,\\ldots, y_{p-m+1}\\)), set \\(\\phi_i = y_{p-i+1}\\), \\(z_i = Xy_{p-i+1}\\), and \\(AB = \\sum_{i=1}^m z_i \\phi_i^T\\), as desired."
  },
  {
    "objectID": "posts/23-01-12-post-1/index.html",
    "href": "posts/23-01-12-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left(b_i+ \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  },
  {
    "objectID": "posts/23-01-10-post-1/index.html",
    "href": "posts/23-01-10-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "",
    "text": "In November 2022, my friend gave me the datasets included in the “data” folder for this post as “Yr1116Birth.csv” and “Yr1116Death.csv” and told me to develop a model that predicts a county-by-county infant mortality rate based on various demographic features that were recorded in the data. The data lists births and deaths of infants in the years 2011-2016 in North Carolina by county of residence, race, and ethnicity of the mother. The Excel file Dataset Descriptions.xls lists all the information available in this dataset, and I presume that the information comes from the NC State Center for Vital Statistics. I’ve also supplemented the data with county-by-county income data which I found at datausa, though this data is also readily available from various public agencies like the Census Bureau. I use the data to compute infant mortality rates by county, race, and year. I used the 2011-2015 data to train my models and the 2016 data for validation.\nThis post is a write-up of my analysis of this data. This was an interesting task because it was a crash course in a few important data science skills: data wrangling, rare event modeling, and maps & data visualization.\n\n\nMy data came from a number of different sources. There were separate birth and death data tables, the income data came from its own source, and the purely geographic data used to generate the map at the end of this post came from the mapdata package.\nCombining these sources presented some difficulty. The birth and death data had different numerical codings for the race of the baby/mother, so I had to make a custom function collapse which collapsed the many extra race codings in the birth data into the simpler “other” category in the death data. In a similar vein, different sources encoded county information in different ways: North Carolina orders the counties from 1 to 100, but there is also a FIPS code which is more useful for national data, and of course the name of a county is a fine label for it. One defect in my data was that one of my sources mis-spelled “Tyrrell”, and it took me a while to detect this error.\n\n\n\nWhen my friend presented me with this data, he and I discussed the interesting fact that some counties recorded no infant deaths for certain races in certain years. I don’t think that this was due to incomplete records or reporting anomalies: when I investigated these cases, I found that there were fewer than 100 births in the previous year in the same race and county. The overall infant mortality rate was about .7% in these years, so the expected number of infant deaths when there are fewer than 100 births is less than 1.\nMy friend raised the possibility that I could model this problem as a classification problem: given a partiular infant birth, predict the probability that it would die in the first year of its life. I considered this possibility, but decided not to do the analysis in this way, since the birth data contained more information, like the infant’s birth weight, that was not reflected in the death data, so I thought it might be hard to measure the effect of these additional variables on a given infant’s likelihood of death. So, instead I modeled the problem as a regression problem: predict a county’s infant mortality rate in a given county by year and race, given the county’s average values for the other predictors in the birth data (e.g., birth weight in grams, median income in the county, number of cigarettes smoked by the mother).\nNevertheless, the data still presented the challenges associated with classification problems in which the classes are very unbalanced in number. To get a feel for why there is an issue, let’s consider one of those counties where there were no infant deaths in a given year. Because infant mortality rates are on the order of 1/1000, to detect significant changes in rates between counties, it makes sense to measure them on a log scale. The counties where there were no infant deaths, we would record an infant mortality rate of 0, which would be (infinitely) many orders of magnitude smaller than the typical rate. To solve this, I added .001 to the infant mortality rates before taking the logarithm. This is sort of like label smoothing: I don’t want the model to make too much of those points where there happened to be no infant deaths.\nOne other thing I tried to do was to aggregate the counties into clusters and compute only in-cluster infant mortality rates for the training data. This was an attempt to reduce year-to-year variance due to small sample sizes. However, I found that my implementation of this idea didn’t really improve the validation-set error. So, in the end, I didn’t put this into practice. But, if you look at the code for this notebook, you’ll find relics of that approach.\n\n\n\nThis project was a good way for me to practice what I had learned about making data visualizations from Introduction to Statistical Learning with R, including feature importance charts. But it was also an opportunity for me to learn how to make a county-by-county heat map; this map appears in the last section of this"
  },
  {
    "objectID": "posts/23-01-18-post-1/index.html",
    "href": "posts/23-01-18-post-1/index.html",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "",
    "text": "In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.\nIt actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.\n\n\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\((N-1)\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}(w,x_i).\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) is something that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand it, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\mathbb{R}^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#loading-packages-and-cleaning-data",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#loading-packages-and-cleaning-data",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Loading Packages and Cleaning Data",
    "text": "Loading Packages and Cleaning Data\nIn this section, I clean the data and wrangle it into a form amenable to analysis. I omit most of this process from the presentation version of the notebook, but the interested readers can examine the code, which is available on my Github page.\nThe result of all these manipulations is a data frame brdrcounts which looks like this:\n\nhead(brdrcounts)\n\n# A tibble: 6 × 15\n   YEAR CORES  RACE CIGPN CIGFN CIGSN CIGLN  BWTG  GEST  PLUR  MAGE PARITY\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1  2011     1     1 3.25  1.91  1.43  1.33  3318.  38.7  1.03  27.1   2.33\n2  2011     1     2 2.58  1.58  1.06  0.942 3089.  38.2  1.05  25.5   2.77\n3  2011     1     3 6.5   5     5     5     2758.  38.5  1     25     2.75\n4  2011     1     4 0.347 0.180 0.178 0.159 3287.  38.7  1.03  27.2   3.05\n5  2011     2     1 2.93  1.87  1.63  1.49  3252.  38.2  1.05  26.6   2.71\n6  2011     2     2 0     0     0     0     3433.  39.5  1     23.4   2.15\n# … with 3 more variables: INCOME <dbl>, IMR <dbl>, CLUSTER <dbl>\n\n\nMost of the columns are explained in the file Dataset Descriptions.xls; “IMR” is the log of infant mortality rate, and “Cluster” is just a duplicate of “CORES” (it’s an artifact from when I tried to apply clustering to the data)."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#trying-different-models",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#trying-different-models",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Trying Different Models",
    "text": "Trying Different Models\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.\nThe first method we try is just a linear model; we perform subset selection by validation-set MSE.\n\nwhich.min(val.errors)\n\n[1] 10\n\ncoef(regfit,which.min(val.errors))\n\n  (Intercept)         RACE2         RACE3         CIGPN         CIGFN \n-1.983988e+00  4.961281e-01  1.442747e-01 -7.878452e-03 -5.804568e-03 \n        CIGSN          BWTG          GEST          PLUR        PARITY \n 1.456896e-02 -1.282548e-04 -4.569572e-02 -4.074168e-01  7.307301e-03 \n       INCOME \n-9.240510e-06 \n\nsubset.error<-val.errors[which.min(val.errors)]\n\nThe best model seems to associate a decline in infant mortality rate if the mother is American Indian or “Other” (not White, Black, or American Indian). It’s hard to understand the sign of the coefficients for “CIGPN” and “CIGFN”. My guess is that this has to do with the fact that I imputed a slightly lower-than-average infant mortality rate when the death count for a given county, race, and year is zero.\nLet’s now try lasso regression.\n\nx<-model.matrix(IMR~.-CORES,train.data[,1:14])[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n\n\n\ncoef(cv.lasso,cv.lasso$lambda.min)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -7.914631e+00\nYEAR         2.907554e-03\nRACE2        4.879737e-01\nRACE3        1.309074e-01\nRACE4       -7.877888e-03\nCIGPN       -6.122286e-03\nCIGFN        .           \nCIGSN        6.751858e-03\nCIGLN        .           \nBWTG        -1.183393e-04\nGEST        -4.529146e-02\nPLUR        -3.807254e-01\nMAGE        -1.634793e-04\nPARITY       6.375718e-03\nINCOME      -8.955468e-06\n\ncoef(cv.lasso,cv.lasso$lambda.1se)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) -4.8453328\nYEAR         .        \nRACE2        0.1535657\nRACE3        .        \nRACE4        .        \nCIGPN        .        \nCIGFN        .        \nCIGSN        .        \nCIGLN        .        \nBWTG         .        \nGEST         .        \nPLUR         .        \nMAGE         .        \nPARITY       .        \nINCOME       .        \n\n\nThe 1se lambda value gives a model in which the only predictor is RACE2 (African American).\nNow we try ridge regression:\n\ncv.ridge<-cv.glmnet(x,y,alpha=0,family=\"gaussian\")\nplot(cv.ridge)\n\n\n\ncoef(cv.ridge,cv.ridge$lambda.min)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -1.002797e+01\nYEAR         3.996398e-03\nRACE2        4.597315e-01\nRACE3        1.149176e-01\nRACE4       -2.115184e-02\nCIGPN       -5.598731e-03\nCIGFN       -1.539379e-03\nCIGSN        5.012017e-03\nCIGLN        2.696535e-03\nBWTG        -1.336627e-04\nGEST        -4.438239e-02\nPLUR        -3.947143e-01\nMAGE        -2.115157e-03\nPARITY       7.580164e-03\nINCOME      -8.632730e-06\n\ncoef(cv.ridge,cv.ridge$lambda.1se)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -5.599637e+00\nYEAR         8.684702e-04\nRACE2        8.397161e-02\nRACE3       -3.927352e-03\nRACE4       -3.221903e-02\nCIGPN       -6.046998e-05\nCIGFN       -5.124061e-05\nCIGSN       -9.542885e-06\nCIGLN       -3.908353e-05\nBWTG        -5.967245e-05\nGEST        -1.357928e-02\nPLUR        -1.939902e-02\nMAGE        -6.071366e-03\nPARITY       7.083761e-04\nINCOME      -1.630609e-06\n\n\nLet’s try to train a single tree.\n\nlibrary(tree)\ntree.model<-tree(IMR~.-CORES,train.data)\nsummary(tree.model)\n\n\nRegression tree:\ntree(formula = IMR ~ . - CORES, data = train.data)\nVariables actually used in tree construction:\n[1] \"RACE\"   \"INCOME\" \"PLUR\"   \"GEST\"   \"CIGLN\" \nNumber of terminal nodes:  6 \nResidual mean deviance:  0.3125 = 582.5 / 1864 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.33800 -0.15540 -0.08285  0.00000  0.14070  4.18600 \n\nplot(tree.model)\ntext(tree.model,pretty=0)\n\n\n\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\n\nThe most notable differences in IMR come from GEST and RACE2.\nLet’s do some tree pruning. The following graph shows that the minimum deviance is obtained via a tree with 6 nodes; however, there doesn’t seem to be much difference between a tree with 3 nodes and a tree with 6 nodes.\n\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n\n\n\nprune.tree.model<-prune.tree(tree.model,best=3)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n\n\n\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\n\nAs we noted above, GEST and RACE2 are the major factors in this tree.\nLet’s try random forests.\n\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-CORES,na.omit(train.data[,1:14]), importance=TRUE)\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nvarImpPlot(rf.model)\n\n\n\n\nAgain, “RACE”, “BWTG”, and the various “CIG” predictors appear near the top of the %IncMSE chart.\nLast model is boosting:\n\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-CORES,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n\n\n\n\n            var   rel.inf\nGEST       GEST 14.410847\nBWTG       BWTG 12.606412\nMAGE       MAGE 11.118088\nPARITY   PARITY 10.495755\nCLUSTER CLUSTER  8.163164\nINCOME   INCOME  7.517338\nCIGPN     CIGPN  6.803060\nPLUR       PLUR  6.779247\nCIGFN     CIGFN  5.553177\nCIGLN     CIGLN  4.856280\nCIGSN     CIGSN  4.735801\nRACE       RACE  4.725559\nYEAR       YEAR  2.235272\n\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)"
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#conlcusion",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#conlcusion",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Conlcusion",
    "text": "Conlcusion\nIt was interesting to go back to this project a few months after I first did it, because I noticed a lot of places where what I have learned in the interim could have come in handy. For example, I now have more robust EDA and feature engineering frameworks. Keep an eye out for a future blog post in which I discuss these issues in more depth in the context of my participation in recent Kaggle competitions."
  },
  {
    "objectID": "posts/23-03-15-post-1/index.html",
    "href": "posts/23-03-15-post-1/index.html",
    "title": "What I’ve Learned from My First Kaggle Competitions",
    "section": "",
    "text": "Introduction\nIn the past few months, I’ve sharpened my data science axe on three different Kaggle competitions: two tabular binary classification problems, and one tabular ordinal regression problem.\nI’ve learned a lot about how to set up a full data pipeline, and much of what I learned comes from this notebook.\nYou could classify what I’ve learned into three categories: tools, techniques for EDA and feature engineering, and rare event modeling.\n\n\nNew Tools\nI’ve learned about XGBoost, which seems to be a favored boosting method on Kaggle. In the past, I’ve had trouble getting boosting to match random forests in performance, but I also learned about optuna, which is a framework for hyperparameter tuning. Together with optuna, I can now use XGBoost to get pretty good baseline performance.\n\n\nNew Techniques for EDA and Feature Engineering\nOne good way to start exploratory data analysis (EDA) is to produce pair plots for all the variables. This was already something I was doing. Sometimes, though, there are very many predictors, so it’s difficult to pick out information by eye from the pair plots. In that case, it helps to restrict to plots of the dependent variable against the predictors. I’ve also taken to computing Pearson corelation coefficients between the dependent variable and the predictors. For classification problems, it’s also helpful to model predictor distributions by class, as in the following image, which comes from a hotel bookings dataset: \nI look for graphs where the two distributions are noticeably different. In the above image, for example, the three most important features amenable to feature engineering seemed to be “avg_price_per_room”, “lead_time”, and “no_of_special_requests”. For feature engineering, it’s important to try some interactions and higher-order polynomial terms involving the most significant predictors. It’s worth investigating certain ratios between these predictors as well. To get great results, though, it’s also important to think about domain-specific combinations of predictors; for example, I tried to make 1) a “fussiness” variable which combined “no_of_special_requests” with “required_car_parking_space” and 2) a “cancellation rate” predictor. In the hotels dataset, though, this didn’t help too much.\nThe image below shows feature distributions for the most promising engineered features:\n\n\n\nEngineered Feature Distributions\n\n\nI think the best features are ones for which the distributions are different not only for the two target classes, but also different from other features already appearing in the dataset.\n\n\nRare Event Modeling\nOne of the competitions involved modeling the risk of credit card default for bank accounts. Another involved predicting a wine taster rating for different wines. In the former case, defaults were rare; in the latter case, there were very few wines that received a score of either 8 or 3 (maximum and minimum scores in the dataset, respectively). So I had to learn about methods to detect rare events. The problem with rare events is that a baseline model which predicts that the event doesn’t occur has a high accuracy. The rarity of the positive observations increases the variance of the predictors for the positive observations, so it becomes difficult to construct a model with a better accuracy than the baseline one.\nThe method I settled on was to ensemble: I divided the training data into about 100 data sets. Each data set had all of the positive observations, and a different subset of the negative observations. The result was that the classes were distributed more equally in the smaller datasets than in the original dataset. Then, I trained a classification (random forest) model on each of the 100 data sets separately, and took the mean prediction probability over the 100 models. This technique is known as “ensembling”: the idea is that each of the constituent models has a chance to learn something meaningful about what distinguishes some negative observations from the positive ones. If certain features keep appearing in the various models, then they will aggregate to an important feature in the ensemble. Another way of putting this is that we want to be close to the inflection point of the logistic function:\n\n\n\n\n\nIf we start at the distant asymptotes of the logistic function, then we will need very large coefficients in front of our predictors to produce a meaningful difference in probabilities. Then, a small amount of noise in that predictor can produce a very large change in probabilities.\nIn the end, the ensemble method will dramatically overrate the probabilities that it assigns to positive events, but usually we are more interested in accuracy or area under the ROC curve, so that the probability over-estimate can be compensated by a change in the threshold.\n\n\nConclusion\nWhen I was revisiting my infant mortality project, I noticed that most of what I discussed above would have been very applicable to that project as well. For example, distribution plots would have helped me to engineer features that may have improved my models. In any case, looking back on my Kaggle competitions has helped me to appreciate how far I’ve come in my journey since November."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#summary-of-results",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#summary-of-results",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Summary of Results",
    "text": "Summary of Results\nNow, we summarize our results in a table:\n\nmean.error<-mean((na.omit(test.data$IMR)-log(avgIMR+.007))^2)\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\", \"Simple Ridge\", \"Lowest MSE Ridge\", \"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,simple.ridge.error, best.ridge.error, tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n\n                        Method Test.Error\n1  No Dependence on Predictors  0.6817740\n2     Best Subset Linear Model  0.3456520\n3                 Simple Lasso  0.3640914\n4             Lowest MSE Lasso  0.3462914\n5                 Simple Ridge  0.3660809\n6             Lowest MSE Ridge  0.3466169\n7                         Tree  0.3436803\n8                  Pruned Tree  0.3643193\n9               Random Forests  0.3240173\n10                    Boosting  0.4121946\n\n\nRandom forests seems to have done the best. This is consistent with its reputation as the best out-of-the-box method. Boosting is finnicky, and probably required some more hyperparameter tuning."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#graphs-and-visualizations",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#graphs-and-visualizations",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Graphs and Visualizations",
    "text": "Graphs and Visualizations\nThis is a graph of infant mortality rates by race and year.\n\n\n\n\n\nThis is a heat map of North Carolina by infant mortality rate:"
  },
  {
    "objectID": "posts/23-09-19-post-1/index.html",
    "href": "posts/23-09-19-post-1/index.html",
    "title": "Thoughts on the Value-Loading Problem",
    "section": "",
    "text": "This is a blog post I started writing up in September. I wrote the first draft in about a week, then set it aside with the intent of polishing it up and doing more reading to improve it. But I got waylaid in these plans, and now coming back to it, it seems in good enough shape to post. Even though the post is still somewhat rough-and-ready, I hope it can be of some use.\nNot long ago, I read Superintelligence by Nick Bostrom, an overall careful accounting of the possible futures that might occur if/once some form of superintelligence arises. It is an oracular work, written nearly ten years ago, and it discusses many issues that are starting to become unavoidable. Nevertheless, I think the book is written in a certain spirit—what is often called “Scientism”—which pinpoints the problems of superintelligence accurately but prescribes the poison as the remedy.\nScientism, in brief, refers to the idea that the methods of the natural sciences are the only way to knowledge of the truth in all matters (including those—like culture, spirituality, and morality—that have traditionally been taken to be outside the purview of natural science).\nIn Technopoly, Neil Postman describes three types of society: the tool-using culture, the technocracy, and the technopoly. It is worth understanding the distinction between the first and last of these, the middle one being a sort of superposition or dialectical intermediate of the two. Perhaps the tidiest way to put the distinction is that a tool-using culture uses tools, and a technopoly is used by its tools, though this is probably an oversimplification. Or to put it another way, a tool-using culture uses its values to circumscribe its tools, while a technopoly allows its tools to circumscribe its values. In a technopoly, Scientism is a dominant strain of thought, and so is the associated idea that the scientific method “generates specific principles which can be used to organize society on a rational and humane basis,” as Postman puts it.\nIt is in the scientistic spirit that many of the solutions to the problems of superintellgience are discussed in Bostrom’s book. One of the central issues that arises with AI is the difficulty of instilling human values into a potentially ultra-powerful AI system. This is known as the value-loading problem, and it’s a very thorny question. There are many facets to the problem, but the main one is that there is no widespread agreement on the meaning of “human values”. The first hurdle in creating AI was to figure out how to get a computer to do something that typical humans can do but can’t describe at a computer level of precision, like identifying pictures of dog breeds. The value-loading problem is the next level of this issue: how to get a computer to do something humans don’t even agree they know how to do, like act ethically.\nThis is the context in which Bostrom proposes two “half-baked”, in his words, solutions to the problem. I will mostly discuss not his solution, but Paul Christiano’s, though they both take as a starting point the idea that even if we can’t teach the AI our values, we can teach it good principles of ethical methodology. To some extent, this is more or less the only way to address the issue, since in any case we don’t want the AI to learn static human values, but instead to make its beliefs subject to revision in light of new evidence or new ideas (as human values, to the extent that there are such values, have been over time). To be honest, though, I don’t believe this makes the original problem any easier to solve: philosophers have not even been able to agree on the methodology of ethics, since, for example, there is still no agreement on whether virtue ethics, deontological ethics, or consequentialist ethics should be the dominant principle. In other words, this is just a kicking of the can down the road.\nNow, to Christiano’s proposal in particular. It starts off by assuming the consequentialist perspective. Consequentialism is my own horse in the race, but I will note that the choice seems to be more of a contortion of ethics to technology than the other way around, since it seems like consequentialism is the easiest ethical framework to teach to a computer, especially a computer/program designed to optimize for its objectives. Incidentally, Postman talks a lot about how each technology comes laden with its own ideology; optimization/consequentialism seems to be that ideology for current deep learning models.\nThe essence of the proposal is this: we are looking for a utility function \\(U\\) to teach to our AIs as the basis of their ethics. To do this, we 1) make a mathematical model of a single human brain and 2) specify an environment in which that human brain will spend centuries reflecting on the nature of the good. Then, \\(U\\) is whatever utility function that brain comes up with. The appeal of the proposal is that it gives a more or less precise definition of \\(U\\) without necessarily specifying how to compute \\(U\\) in practice. It has the flavor of a mathematical statement like “let \\(n\\) be the number of primes less than \\(2^{2^{100}}\\)”. Here, \\(n\\) is easy to define, but complicated to compute exactly. The hope is that while we mere mortals can’t evaluate \\(U\\) directly, perhaps the AI will at least approximately be able to. A philosopher friend of mine once told me the saying: “Mathematics is a game with rules but no goal. Philosophy is a game with a goal but no rules.”\nMy objections to Christiano’s proposal happen both at the level of its feasibility and at the level of its background assumptions. The first is that we have to heed Box’s warning when making a mathematical model of a human brain: “all models are wrong, but some are useful”. What make a useful model? We might take “useful” to mean “parsimoniously helps to make accurate predictions on the subject of interest”. So, how might we understand the usefulness of a mathematical model of the human brain? There are two possibilities: we are capable of evaluating the predictions of the model for ourselves at least in simple cases; in this case, to make our model, we almost certainly had to make simplifying assumptions. Of course, simplifying assumptions are part and parcel of the scientific method, but in this proposal any simplifying assumption becomes a de facto meta-ethical one. For example, we have to decide what aspects, if any, of the human’s corporeal existence are relevant to their ethical decision making. This seems to me to be quite a subtle question. For example, when I’m hungry, I’m less charitable to others and my analytical faculties are diminshed. So, perhaps a good model will simply not model hunger. But, on the other hand, our capacity to suffer (and in particular to be hungry) seems to be an important component of the empathy in which our ethical beliefs are grounded. So, perhaps a good model will model hunger, but not make any actual decisions while hungry. Another example: I do my best thinking either on a walk or splayed out face down on a couch. Whichever human gets chosen to model moral thinking for all of humanity, that person is likely to have their own favored kinesthetic/tactile environments for thinking. So, it seems plausible to include these senses in the model. And so on… Every decision about what to include in the model is a decision about what is and isn’t relevant to ethical reasoning. It’s easy to imagine that the ultimate conclusions of the model will depend strongly on each such decision, especially given that we’re going to run the model for hundreds of subjective years. Even assuming that consequentialism is the correct meta-ethical framework, when it comes to deciding which consequentialism is the relevant one, Christiano’s proposal is far from value-neutral: the value decisions are simply hidden in implementation decisions. Technology can never bridge the gap between “is” and “ought”. That is for humans alone.\nNow, let’s suppose that we manage to overcome these objections, say by making the model include everything that could possibly be relevant to ethical decision making. There are still many implementation decisions with similar meta-ethical consequences:\n\nWhich human’s brain are we modeling?\nAt what period of that person’s life do we start the simulation?\nWhat is the environment we simulate for the brain model?\nHow long do we run the simluation for?\n\nIt’s easy to imagine that every single one of these decisions will have a powerful impact on the final result of the computation. I suspect that part of the appeal of Christiano’s proposal is that it pretends to skirt thorny ethical questions with a “scientific/mathematical” framework. But all it does is replace the question “what is right and wrong?” with the question “how long do we run the simulation?”. At least the former question is something humans have been thinking about for thousands of years. What human has powerful enough intuitions to answer the latter question? (For this particular question, it’s plausible that the model will stabilize its moral philosophy in the long-term, but that may depend on the implementation of the other aspects of the proposal, e.g., if the environment has periodic “stress-test” events for the brain.)\nThese are questions we have to answer assuming that we accept the premises of the proposal. But two weak parts of the proposal stand out: first, the idea that we should accept a “philosopher-sovereign”, and second, Goodhart’s Law. Even given that the philosopher-sovereign has spent a lot of time thinking through the issues, at the end of the day, the best that a mathematical model of that person’s brain could do is to accurately predict the ethical stances that person would have under the circumstances of the simulation. That’s the only way that the model can be “wrong, but useful”. But, as always, there’s the gap between “is” and “ought”. And of course, the choice of human brain is going to be extremely controversial. One way out of this last objection is to somehow simulate many (possibly all) human brains and try to obtain some sort of consensus in this way. This approach starts to bleed a bit into Yudkowsky’s coherent extrapolated volition proposal, which I like a bit more because it has a more democratic flavor, and it tries to capture the idea that we might improve morally over time. So, I would be tempted to take the CEV proposal over this one.\nNow, to the Goodhart’s Law objection, let us suppose that there is a true utility function whose maximization is the sole purpose of ethical action. Even if you are inclined towards consequentialism, as I am, there are a number of serious hurdles to overcome in order to justify this belief. But let’s sweep those under the rug. Let’s also sweep under the rug the Saint Petersburg paradox, and instead return to the arguments that \\(U\\) is extremely sensitive to implementation decisions. It’s worth scrutinizing this argument a bit because it may not be the case. As a very rough model of what might be happening, we might imagine the problem of finding \\(U\\) as an optimization problem over the space of all utility functions. We are assuming that \\(U\\) is the unique global maximum of the objective function; but there could be many local maxima, in which case the optimization algorithm may only find a local one. A counter-argument one may make is to suppose \\(U\\) is such a salient point of the space of all utility functions that all or almost all agents will find it, given enough time. One could imagine the landscape of all utility functions to have a bright, high beacon shining out such that any weary traveler, having stopped at the top of a smaller hill of the terrain, will notice it and be drawn to it. The idea is reminiscent of the Habermasian “unforced force of the better argument”. This is certainly a possibility, but it seems to me not to be certain enough to support a wager over the future of sentient life. So we should consider the possibility that the brain model finds a suboptimal utility function \\(U'\\). In that case, Goodhart’s Law tells us that optimization for \\(U'\\) will look extremely different from optimization for \\(U\\), even if \\(U'\\) is a reasonable approximation of \\(U\\). I think it’s best to look for solutions to the alignment problem which don’t involve mathematical optimization at all.\nMarshall McLuhan, in Understanding Media: The Extensions of Man, argues that many of our communications technologies—starting with the alphabet and proceeding through the printing press, telegraph, and computer—are manifestations of a certain approach to knowledge which seeks to divide the world into identical, repeatable units and analyze phenomena from the ground up using these units. Obviously, this is a useful approach, but it has its limitations. The problem of AI safety is essentially that AI has the potential to be the reductio ad absurdum of this worldview: we want to avoid creating a world measured only in paper clips.\nTechnology can’t tell us what we value: it can only mirror and amplify what we value back towards us."
  },
  {
    "objectID": "posts/27-01-12-post-1/index.html",
    "href": "posts/27-01-12-post-1/index.html",
    "title": "Improvements to a Classical Simulation of a Quantum Algorithm",
    "section": "",
    "text": "This post is about a small project that I’ve been working on for the last month or so. I’ve been learning a lot about quantum computing lately, and as a theoretical undertaking that ties my research interests in math and physics with my more recent, practical computer-scientific adventures, the topic really excites me! The project concerns the quantum Schur transform (a quantum algorithm whose purpose I describe in a bit more detail in a minute) and consists in improvements to this Python implementation of a classical simulation of the version of the algorithm presented in this paper by Kirby and Strauch (the code is due to Kirby alone).\nI came upon this algorithm because I was trying to spin up a version of another algorithm, the port-based teleportation protocols described in this paper, and the teleportation protocol requires the Schur transform as part of its construction. As I was toying with teleportation, I noticed somewhat slow runtimes of the Schur methods I was using, so I decided to peer under the hood and see what might be causing this. The main source of slow runtimes was the fact that Kirby used Python-native lists to represent the relevant unitary matrices, as opposed to numPy or pyTorch. These modules are specifically optimized for matrix computations, so replacing Python lists and list comprehensions or for loops with equivalent numPy array or pyTorch methods leads to much faster run times. (It should be noted in order to exonerate Kirby of his sins that he wrote the code for an undergraduate research project in 2017.) Further speedups were made possible by using pyTorch’s and sciPy’s sparse matrix implementations (many of the matrices corresponded to the identity on most of the relevant qubits)."
  },
  {
    "objectID": "posts/24-01-16-post-1/index.html",
    "href": "posts/24-01-16-post-1/index.html",
    "title": "Improvements to a Classical Simulation of a Quantum Algorithm",
    "section": "",
    "text": "This post is about a small project that I’ve been working on for the last month or so. I’ve been learning a lot about quantum computing lately, and as a theoretical undertaking that ties my research interests in math and physics with my more recent, practical computer-scientific adventures, the topic really excites me! The project concerns the quantum Schur transform (a quantum algorithm whose purpose I describe in a bit more detail in the Appendix) and consists in improvements to this Python implementation of a classical simulation of the version of the algorithm presented in this paper by Kirby and Strauch (the code is due to Kirby alone).\nI came upon this algorithm because I was trying to spin up a version of another algorithm, the port-based teleportation protocols described in this paper, and the teleportation protocol requires the Schur transform as part of its construction. As I was toying with teleportation, I noticed somewhat slow runtimes of the Schur methods I was using, so I decided to peer under the hood and see what might be causing this. The main source of slow runtimes was the fact that Kirby used Python-native lists to represent the relevant unitary matrices, as opposed to NumPy or PyTorch. These modules are specifically optimized for matrix computations, so replacing Python lists and list comprehensions or for loops with equivalent numPy array or PyTorch methods leads to much faster run times. Further speedups were made possible by using PyTorch’s and sciPy’s sparse matrix implementations (many of the matrices acted as the identity on most of the qubits). The speedups obtained by my modifications are summarized by the following figure (note that the y-axis is on a logarithmic scale):\nEach graph plots the average runtime for one iteration of an algorithm as a function of the natural parameter for that algorithm. The orange dots represent the original version of the algorithm, while the blue dots represent the new version. In the top-left graph, the size (side-length) of the relevant square matrices grows linearly with the parameter \\(d\\), whereas in the other graphs it grows exponentially; hence, for that method, I was able to get a better sense of the asymptotic behavior of both algorithms.\nGeneral Comments on the Graphs\nIn all four graphs, it’s clear that the asymptotic behavior of the NumPy-based algorithms is better than that of the original ones; in some concrete instances, the NumPy version outperformed the original by a factor of 5000! Another interesting general comment: there are noticeable jumps in runtime for the list-based algorithms at powers of two. I presume this has something to do with the dynamic sizing of Python-native lists.\nSpecific Comments on the Graphs\nI turn now to each graph in turn. The top-left graph maps out how long it takes to construct the matrix of the lowering operator on a \\(d\\)-dimensional system. This is a square \\(2d+1\\)-matrix whose only non-zero entries are along the diagonal just above the main one. Hence, a sparse implementation of the function lop would be linear in \\(d\\); a dense implementation has complexity \\(O(d^2)\\). Indeed, this is what we see for the NumPy implementation. The original implementation builds up the matrix row-by-row, which requires periodically allocating more memory and copying the existing matrix. This leads to the worse asymptotic performance seen in the graph.\nThe bottom-left graph shows the implementations of a helper function; here were the greatest performance improvements, by a factor of 5000 or more.\nThe top-right graph shows one of the main functions, implementing a version of the Schur transform. Here and in the last graph, the greatest performance difference is the difference between minutes and seconds or fractions of a second.\nFinally, the bottom-right graph shows the time it takes to compute the full unitary matrix for the Schur transform on \\(d\\) qubits. (This is the function which uses sparse PyTorch matrices.) The performance differences here are not as visually stark, but it should be noted that I was prevented from running the test for \\(d=9\\) and \\(d=10\\) by the slowness of the original algorithm, whereas the NumPy implementation could run in a feasible time on my machine. One-off tests for \\(d=9\\) yielded runtimes of 19 minutes and 13 seconds for the old and new implementations, respectively. Moreover, it is possible to run the schurmat function on a GPU (because it uses PyTorch matrix multiplication) and obtain even further speedups.\nConclusion This was a fun and practical exercise for me to get into the NumPy mindset. As a mathematician, I am used to thinking about matrix operations at a level of abstraction which is indifferent to the details of implementation. This project has helped me to get more in the computer scientist’s mindset: implementation can be the difference between a practical runtime and an impractical one!"
  },
  {
    "objectID": "posts/24-01-16-post-1/index.html#appendix-what-is-the-schur-transform",
    "href": "posts/24-01-16-post-1/index.html#appendix-what-is-the-schur-transform",
    "title": "Improvements to a Classical Simulation of a Quantum Algorithm",
    "section": "Appendix: What is the Schur Transform?",
    "text": "Appendix: What is the Schur Transform?\nFor the interested reader, I want to briefly explain the purpose of the Schur transform. Given \\(d\\) qubits, let \\(\\sigma_{x,i}, \\sigma_{y,i}, \\sigma_{z,i}\\) denote the matrices which act by\n\\[\n\\sigma_x =\n\\begin{bmatrix}\n0& 1\\\\ 1&0\n\\end{bmatrix}, \\quad\n\\sigma_y =\n\\begin{bmatrix}\n0 & -i\\\\ i & 0\n\\end{bmatrix}, \\quad\n\\sigma_z =\n\\begin{bmatrix}\n1 & 0 \\\\ 0 & -1\n\\end{bmatrix}\n\\] respectively, on the \\(i\\)th qubit and otherwise by the identity. The computational basis is characterized by the fact that it is a set of joint eigenvectors for the \\(d\\) commuting operators \\(\\sigma_{z,1},\\ldots \\sigma_{z,d}\\). A vector in the computational basis is uniquely and entirely determined by its eigenvalues for these \\(d\\) operators. The Schur transform takes the computational basis to the eigenbasis for a different commuting set of eigenvetors. Namely, let \\[\nS_{1,\\ldots, j}= \\left(\\sum_{i=1}^j \\sigma_{x,i}\\right)^2 + \\left(\\sum_{i=1}^j \\sigma_{y,i}\\right)^2 + \\left(\\sum_{i=1}^j \\sigma_{z,i}\\right)^2\n\\] and let \\[\nZ = \\sum_{i=1}^d \\sigma_{z,i}.\n\\] Then, the Schur basis is a basis of eigenvectors for the \\(d\\) commuting operators \\((S_{1,2},S_{1,2,3},\\ldots, S_{1,\\ldots, d}, Z)\\). Just as we had for the computational basis, a Schur basis vector is entirely and uniquely determined by its eigenvalues for these \\(d\\) operators. As mentioned above, the Schur transformation is simply the unitary transformation that takes the basis vectors of the computational basis to the basis vectors of the Schur basis (there is some confusion in the literature as to whether it is this matrix or its inverse/adjoint which is the true Schur transform).\nThe Schur basis is related to the representation theory of \\(SU(2)\\), which is deeply important in the mathematics of spin. But for the reader unfamiliar with representation theory, we can summarize the utility of the Schur basis by making the following observation: given an element \\(v\\) of the Schur basis, then for all \\(i\\), \\(\\sigma_{x,i}\\cdot v\\) (as well as \\(\\sigma_{y,i}\\cdot v\\) and \\(\\sigma_{z,i}\\cdot v\\)) will have the same eigenvalues for the \\(S\\) operators as \\(v\\) itself. (The same is not true for the computational basis, which gets shuffled around by the X and Y Pauli operators.) This can sometimes dramatically simplify the theoretical considerations involved in verifying the validity of a quantum protocol."
  }
]