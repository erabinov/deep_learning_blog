[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eugene’s Deep Learning Blog",
    "section": "",
    "text": "The Universal Approximation Theorem\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWelcome to my blog!\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi everybody! My name is Eugene! I’m a postdoc in pure mathematics at the University of Notre Dame. I received my PhD in mathematics from University of California, Berkeley in 2021. My research interests center around mathematical formulations of quantum field theory. As a side/pet project, I am learning about data science and machine learning. This blog is to document some of the things I’m learning. I am using the book Deep Learning for Coders and following the corresponding course from fast.ai."
  },
  {
    "objectID": "posts/first-ipynb-post/index.html",
    "href": "posts/first-ipynb-post/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces, so I may have to periodically rebuild. If it doesn’t work for you, raise an issue on the GitHub page, and I will rebuild.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay."
  },
  {
    "objectID": "posts/10-01-23-post-1/index.html",
    "href": "posts/10-01-23-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/12-01-23-post-1/index.html",
    "href": "posts/12-01-23-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left( \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  }
]