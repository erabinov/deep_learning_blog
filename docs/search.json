[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eugene’s Deep Learning Blog",
    "section": "",
    "text": "What I’ve Learned from My First Kaggle Competitions\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRubik’s Cube Solver\n\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing a Neural-Net Digit Classifier (Almost) from Scratch\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Universal Approximation Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog!\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi everybody! My name is Eugene! I’m a postdoc in pure mathematics at the University of Notre Dame. I received my PhD in mathematics from University of California, Berkeley in 2021. My research interests center around mathematical formulations of quantum field theory. As a side/pet project, I am learning about data science and machine learning. This blog is to document some of the things I’m learning. I am using the book Deep Learning for Coders and following the corresponding course from fast.ai."
  },
  {
    "objectID": "posts/first-ipynb-post/index.html",
    "href": "posts/first-ipynb-post/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces, so I may have to periodically rebuild. If it doesn’t work for you, raise an issue on the GitHub page, and I will rebuild.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay."
  },
  {
    "objectID": "posts/10-01-23-post-1/index.html",
    "href": "posts/10-01-23-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/12-01-23-post-1/index.html",
    "href": "posts/12-01-23-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left(b_i+ \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html",
    "href": "posts/18-01-23-post-1/index.html",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "",
    "text": "In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.\nIt actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.\n\n\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\((N-1)\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}(w,x_i).\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) is something that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand it, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\mathbb{R}^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "href": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?",
    "text": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\(N-1\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}.\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand this, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\RR^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the maximum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02<00:00]\n    \n    \n\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('testing'),Path('training')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/1'),Path('training/2'),Path('training/5'),Path('training/9'),Path('training/7'),Path('training/6'),Path('training/0'),Path('training/8'),Path('training/4'),Path('training/3')]\n\n\n\nnums = [(path/'training'/str(i)).ls() for i in range(10)]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "href": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\nStep one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\nLet’s create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\nWe will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:\n\nnum_tensors = [[tensor(Image.open(o)) for o in nums[i]] for i in range(10)]\n\n\nlen(num_tensors), len(num_tensors[2])\n\n(10, 5958)\n\n\n\nstacks = [torch.stack(num_tensors[i]).float()/255 for i in range(10)]\nstacks[2].shape\n\ntorch.Size([5958, 28, 28])\n\n\n\nmeans = [stacks[i].float().mean(0) for i in range(10)]\n[show_image(o) for o in means]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "href": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\n#An ordinary Python list of pyTorch tensors, with the validation images for digit i in the ith index.\nvalids = [torch.stack([tensor(Image.open(o)) \n                            for o in (path/'testing'/str(i)).ls()]) for i in range(10)]\nvalids = [o.float()/255 for o in valids]\n\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\n\ndef mnist_distance_list(x):\n  dists = [mnist_distance(x,means[i]) for i in range(10)]\n  return torch.stack(dists)\n\ndef which_is_it(x):\n  return torch.argmin(mnist_distance_list(x),dim=0)\n\nLet’s test it on our example case:\n\na_3 = stacks[3][2007]\nmnist_distance_list(a_3), which_is_it(a_3), show_image(a_3)\n\n(tensor([0.1583, 0.1372, 0.1469, 0.1612, 0.1534, 0.1595, 0.1483, 0.1514, 0.1612, 0.1485]),\n tensor(1),\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7dc64648b0>)\n\n\n\n\n\n\nsmall_test = stacks[3][0:2]\nsmall_test.shape\nmnist_distance_list(small_test[0]), mnist_distance_list(small_test[1]),mnist_distance_list(small_test)\n\n(tensor([0.1826, 0.1183, 0.1458, 0.1037, 0.1429, 0.1319, 0.1613, 0.1408, 0.1254, 0.1331]),\n tensor([0.1965, 0.1569, 0.1953, 0.1421, 0.1825, 0.1581, 0.1766, 0.1784, 0.1759, 0.1732]),\n tensor([[0.1826, 0.1965],\n         [0.1183, 0.1569],\n         [0.1458, 0.1953],\n         [0.1037, 0.1421],\n         [0.1429, 0.1825],\n         [0.1319, 0.1581],\n         [0.1613, 0.1766],\n         [0.1408, 0.1784],\n         [0.1254, 0.1759],\n         [0.1331, 0.1732]]))\n\n\n\nwhich_is_it(small_test[0]),which_is_it(small_test[1]), which_is_it(small_test), which_is_it(small_test)==3\n\n(tensor(3), tensor(3), tensor([3, 3]), tensor([True, True]))\n\n\n\naccuracies = [(which_is_it(valids[i])==i).float().mean() for i in range(10)]\noverall_accuracy = tensor([accuracies[i]*len(valids[i]) for i in range(10)])\noverall_accuracy = overall_accuracy.sum()/10000.\naccuracies, overall_accuracy\n\n([tensor(0.8153),\n  tensor(0.9982),\n  tensor(0.4234),\n  tensor(0.6089),\n  tensor(0.6680),\n  tensor(0.3262),\n  tensor(0.7871),\n  tensor(0.7646),\n  tensor(0.4425),\n  tensor(0.7760)],\n tensor(0.6685))"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "href": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\ntrain_x = torch.cat([stacks[i] for i in range(10)]).view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\ntrain_y = tensor([])\nfor i in range (10):\n  add = torch.stack([tensor([i]) for j in range(len(nums[i]))])\n  train_y=torch.cat((train_y,add),0)\ntrain_y= train_y.squeeze()\ntrain_x.shape, train_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000]))\n\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor(0.))\n\n\n\nvalid_x = torch.cat([valids[i] for i in range(10)]).view(-1, 28*28)\nvalid_y = tensor([])\nfor i in range(10):\n  add=torch.stack([tensor([i]) for j in range(len(valids[i]))])\n  valid_y =torch.cat((valid_y,add),0)\nvalid_y = valid_y.squeeze()\nvalid_dset = list(zip(valid_x,valid_y))\n\n\nvalid_x.shape, valid_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000]))\n\n\nNow we need an (initially random) weight for every pixel (this is the initialize step in our seven-step process):\n\ndef init_params(size, std=1.0): \n  ungrad = (torch.randn(size)*std).squeeze()\n  return ungrad.requires_grad_()\n\n: \n\n\n: \n\n\n\nweights = init_params((28*28,10))\nweights.shape\n\ntorch.Size([784, 10])\n\n\n\nbias = init_params((1,10))\nbias.shape\n\ntorch.Size([10])\n\n\n\n((train_x.float()@(weights)) + bias).shape\n\ntorch.Size([60000, 10])\n\n\n\ndef linear1(xb): return xb.float()@weights + bias\npreds_log_prob = linear1(train_x)\n\n\npreds_log_prob[:4]\n\ntensor([[-18.1373,  -6.1067,   2.9795,  -1.2941,  -2.0108,  -1.8045,  -5.9375, -10.6166,   6.8176,   9.3555],\n        [ -8.9188,   4.6967,  -3.0078,   2.0427,  -9.5952,   6.2474,  -1.6729,  -6.3322,  13.9906,  27.3094],\n        [-12.4062,  -0.5152,   6.4807,   4.4728, -13.1531,   2.5008,  -1.0917, -12.9701,   7.1787,  10.7713],\n        [ -9.2247,  11.5279,  -0.7430,   1.1205,  -5.2792, -11.2010, -11.0678, -11.6129,   5.9237,  15.7904]], grad_fn=<SliceBackward0>)\n\n\n\npreds= torch.argmax(preds_log_prob,1)\npreds = preds.reshape(60000)\npreds.shape, preds\n\n(torch.Size([60000]), tensor([9, 9, 9,  ..., 8, 9, 8]))\n\n\n\ncorrects = preds.float()==train_y.squeeze()\ncorrects\n\ntensor([False, False, False,  ..., False,  True, False])\n\n\n\ncorrects.float().mean().item()\n\n0.12043333053588867\n\n\n\ndef mnist_loss(predictions, targets):\n    targets = targets.long()\n    losses = F.cross_entropy(predictions,targets.squeeze())\n    return losses.mean()\n\n\nSGD and Mini-Batches\n\nweights = init_params((28*28,10))\nbias = init_params(10)\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nbatch = train_x[:4]\nbatch.shape, train_y[:4].shape\n\n(torch.Size([4, 784]), torch.Size([4, 1]))\n\n\n\npreds = linear1(batch)\npreds, train_y[:4]\n\n(tensor([[ 19.2725,  22.0151, -27.2578,  10.6764,  -4.2019,  22.7855,   6.2560,   4.3134,  -0.2266,  -5.1313],\n         [ 18.2444,   4.6494, -17.6840,  10.6016, -12.3112,  23.4429,   3.3657,   5.7126,   3.6596,   5.7853],\n         [ 25.9703,   9.6952, -19.0290,  10.1290,   4.2884,  21.3991,   5.5114,  -1.0090,   9.3477, -20.2521],\n         [  8.6966,   1.3858,  -6.3632,   1.8474,   2.0362,  23.2705, -12.0754,   3.2197,   4.8403,   2.4272]], grad_fn=<AddBackward0>),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(5.9254, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\nbias.grad\n\ntensor([-7.4619e-01,  7.7517e-02,  3.3744e-14,  1.6141e-06,  2.4515e-10,  6.6868e-01,  1.1896e-08,  7.0398e-09,  1.8070e-08,  5.5542e-09])\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-5.5499e-10),\n tensor([-6.0718e-02,  2.6098e-06,  8.3043e-06,  3.9334e-06,  4.4649e-02,  1.0095e-02,  1.9338e-03,  2.0388e-14,  1.0862e-08,  4.0262e-03]))\n\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\ndef batch_accuracy(xb, yb):\n    preds = torch.argmax(xb,dim=1).squeeze()\n    correct = preds.float() == yb.float().squeeze()\n    return correct.float().mean()\n\nWe can check it works:\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.2500)\n\n\nand then put the batches together:\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.1018\n\n\nThat’s our starting point. Let’s train for one epoch, and see if the accuracy improves:\n\nlr = .1\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.7798\n\n\nThen do a few more:\n\nfor i in range(30):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end='\\n')\n\n0.8844 0.8854 0.8856 0.8866 0.8869 0.8869 0.8861 0.8859 0.8862 0.8864 0.8864 0.8866 0.887 0.8873 0.8873 0.8872 0.8872 0.8874 0.8875 0.8876 0.8876 0.8878 0.8877 0.8877 0.8881 0.8881 0.888 0.888 0.8881 0.8881 \n\n\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,10))\nb2 = init_params(10)\n\n\ntrain_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\nprint(validate_epoch(simple_net), end=' ')\n\n\nfor i in range(150):\n  train_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\n  if i%15==0:\n    print(validate_epoch(simple_net), end='\\n')"
  },
  {
    "objectID": "posts/02-02-23-post-1/index.html",
    "href": "posts/02-02-23-post-1/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "For this blog post, I want to talk about principal components analysis (PCA). This is something I learned about from the great book Introduction to Statistical Learning with R by James, Witten, Hastie, and Tibshirani. This is another one of those “delve deeper into the math theory” posts.\nSo the book presents two different equivalent formulations of the principal components problem. To study this, let \\(X\\) be our data set, represented as a \\(n\\times p\\) matrix. We assume that the columns of \\(X\\) have zero mean. We will use symbols like \\(\\phi_1\\) to represent a \\(p\\)-vector, and symbols like \\(z_1\\) to represent an \\(n\\)-vector. We will think of vectors always as column vectors and use the superscript \\(T\\) to denote the corresponding row vectors, e.g. \\(\\phi_1^T\\). The first formulation of the PCA problem is in the following optimization problem: we want to maximize \\(\\phi^T X^T X \\phi= ||X\\phi||^2\\) by varying \\(\\phi\\) among all unit \\(p\\)-vectors. The vector \\(z_1 = X\\phi_1\\) is the linear combination of our features with the highest variance (for this interpretation, it was important to set the columns of \\(X\\) to have zero mean). The matrix \\(X^T X\\) is symmetric and positive-semi-definite, so it can be diagonalized and all its eigenvalues are non-negative. Let us assume that \\(X^T X\\) has only positive eigenvalues, since a zero eigenvalue would mean that some linear combination of the columns of \\(X\\) is zero, and therefore that one of the features encoded in \\(X\\) is redundant. Let \\(y_1,\\ldots, y_p\\) be the eigenvectors of \\(X^TX\\), and \\(\\lambda_1,\\ldots, \\lambda_p\\) be the corresponding eigenvalues, arranged in increasing order. Then, if we write \\[\\phi = c_1y_1+\\cdots+c_py_p,\\] we have \\[||\\phi||^2 = \\sum_i c_i^2\\] and \\[||X\\phi||^2 = \\sum_i \\lambda_i c_i^2.\\] This is a constrained optimization problem (maximize \\(\\sum_i c_i^2\\lambda_i\\) subject to \\(\\sum_i c_i^2=1\\)) and it can be solved by Lagrange multipliers. The maximum value for the objective function is \\(\\lambda_p\\) and is obtained when \\(c_p=1\\) and the other \\(c\\)s are zero. Then, the first principal component loading vector \\(\\phi_1 =y_p\\) is the eigenvector of \\(X^TX\\) corresponding to the highest eigenvalue, and the corresponding principal component is \\(z_1 = X\\phi_1\\). Next, we seek \\(z_2 = X\\phi_2\\) such that \\(z_2^T z_2\\) is maximal among \\(z_2\\) such that \\(z_2^T z_1=0\\). Since \\(z_1 = Xy_p\\) and \\(z_2 = X\\phi_2\\), we have \\[z_2^T z_1 = \\phi_2^T X^TX y_p = \\lambda_p \\phi_2^T y_p=\\lambda_p \\phi_2^T\\phi_1.\\] So, because \\(\\lambda_p\\neq 0\\), \\(z_1\\) and \\(z_2\\) are orthogonal only if \\(\\phi_1\\) and \\(\\phi_2\\) are orthogonal. So, finding \\(\\phi_2\\) amounts to finding the next highest eigenvalue \\(\\lambda_{p-1}\\) and the corresponding eigenvector \\(y_{p-1}\\) (the orthogonal complement to \\(\\phi_1\\) is spanned by \\(y_1,\\ldots, y_{p-1}\\), so we simply repeat the argument that got \\(y_1\\) on this smaller space). And so on, till we’ve found the \\(m\\) largest eigenvalues of \\(X^TX\\) and have decided to stop. To recap: in PCA, we\n\nFind the linear combination \\(z_1=X\\phi_1\\) of the columns of \\(X\\) which has maximal variance. This corresponds to picking the eigenvector of \\(X^TX\\) with maximal eigenvalue.\nAmong the linear combinations \\(z_2 = X\\phi_2\\) of the columns of \\(X\\) wich \\(z_2\\) orthogonal to \\(z_1\\), we choose the one which has maximal variance. This amounts to picking out the second-largest eigenvalue of \\(X^TX\\).\nAnd so on… We pick out the \\(m\\) largest eigenvalues of \\(X^TX\\), their corresponding eigenvectors, and their corresponding principal components.\n\nThe book claims but doesn’t show that it this procedure is equivalent to considering the following optimization problem instead: minimize \\(\\mathrm{tr}((X-AB)^T(X-AB))\\) as a\\(A\\) ranges over the space of \\(n\\times m\\) matrices and \\(B\\) ranges over the space of \\(m \\times p\\) matrices. Let’s show this. First, let’s note that we can write \\(AB = \\sum_{j=1}^m z_i \\phi_i^T\\), where the \\(z_i\\) are \\(n\\) vectors and the \\(\\phi_i\\) are \\(p\\)-vectors (unrelated so far to the ones found from the first formulation of PCA). We may assume that the \\(\\phi_i\\) are linearly independent (for if we can write one \\(\\phi\\) as a linear combination of the others, we can manifest this as a redefinition of the \\(z\\)’s). For a similar reason, we may assume that the \\(\\phi_i\\) are orthonormal. Under these assumptions, we may write \\[\n\\begin{aligned}\n\\mathrm{tr}((X-AB)^T(X-AB))&= \\mathrm{tr}(X^TX)-2\\sum_{i} \\mathrm{tr}(X^T z_i \\phi_i^T)+\\sum_{i,j}\\mathrm{tr}(\\phi_i z_i^T z_j \\phi_j^T)\\\\\n&= \\mathrm{tr}(X^TX)- 2\\sum_{i} \\phi_i^T X^T z_i + \\sum_{i} z_i^T z_j,\n\\end{aligned}\n\\] where we have used that \\[\n\\mathrm{tr}(\\phi' \\phi^T) = ||\\phi||^2 \\phi \\cdot \\phi'\n\\] for any two \\(p\\)-vectors \\(\\phi\\), \\(\\phi'\\). So, we are trying to find \\(z_i\\) and \\(\\phi_i\\) to minimize the above trace, subject to the condition that \\(\\phi_i \\cdot \\phi_j = \\delta_{i,j}\\) (i.e. the \\(\\phi\\)’s are orthonormal). There is no constraint on the \\(z\\)’s, and the simple optimization problem for the \\(z\\)’s gives \\[z_i = X\\phi_i\\]. The optimization problem with respect to the \\(y_i\\) is a bit more subtle because the \\(\\phi\\)’s are constrained. But the Lagrange optimization problem tells us that \\[\nX^Tz_i =X^TX\\phi_i = \\sum_j \\beta_j \\phi_j,\n\\] where the \\(\\beta_i\\) are undetermined Lagrange multipliers. But this tells us that \\(X^TX\\) preserves the space spanned by the \\(\\phi_i\\). Let’s call this space \\(W\\). Taking all this into account, the objective function becomes \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX).\n\\] To compute the second trace in the above equation, we just need to know the eigenvalues of \\(X^TX\\) when restricted to \\(W\\). Let us suppose that the eigenvalues of \\(X^TX\\) on \\(W\\) are \\(\\lambda_{i_1},\\ldots, \\lambda_{i_m}\\). Let \\(\\lambda_{i_{m+1}},\\ldots, \\lambda_{i_{p}}\\) be the remaining eigenvalues. Then, \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX)=\\sum_{j=m+1}^p\\lambda_{i_j}.\n\\] So the objective will be minimized precisely by choosing the \\(m\\) largest eigenvalues of \\(X^TX\\), just as in the first formulation of PCA! It follows that to minimze \\[\\mathrm{tr}((X-AB)^T(X-AB)),\\] we choose the \\(m\\) largest eigenvalues of \\(X^TX\\) (which above we called \\(\\lambda_p, \\lambda_{p-1},\\ldots, \\lambda_{p-m+1}\\), with respective eigenvectors \\(y_p,\\ldots, y_{p-m+1}\\)), set \\(\\phi_i = y_{p-i+1}\\), \\(z_i = Xy_{p-i+1}\\), and \\(AB = \\sum_{i=1}^m z_i \\phi_i^T\\), as desired."
  },
  {
    "objectID": "posts/07-03-23-post-1/NC_Birth_and_Death.html",
    "href": "posts/07-03-23-post-1/NC_Birth_and_Death.html",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "",
    "text": "Loading the Data and Packages\nI’m using Google Maps data here.\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(urbnmapr)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(mapdata)\nlibrary(glmnet)\n\nSet up the base NC map:\n\nditch_the_axes <- theme(\n  axis.text = element_blank(),\n  axis.line = element_blank(),\n  axis.ticks = element_blank(),\n  panel.border = element_blank(),\n  panel.grid = element_blank(),\n  axis.title = element_blank()\n  )\nstates<-map_data(\"state\")\nnc_df<-subset(states,region==\"north carolina\")\nnc_base <- ggplot(data = nc_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.3) + \n  geom_polygon(color = \"black\", fill = \"gray\")\ncounties<-map_data(\"county\")\nnc_county<-subset(counties,region==\"north carolina\")\n\nWe read the data in.\n\nraw.bd1<-read_csv(\"data/birth_data.csv\")\nraw.bd1$Year<-as.integer(raw.bd1$Year)\nraw.bd2<-read_csv(\"data/birth_data2.csv\")\nbr<-read_csv(\"data/Yr1116Birth.csv\")\ndr<-read_csv(\"data/Yr1116Death.csv\")\ncounty.conversion<-read_csv(\"data/County_Codes_Table_1.csv\")\ncounty.conversion<-county.conversion[,1:3]\nincomes<-read_csv(\"data/Income_by_Location.csv\")\nincomes$Geography<-str_sub(incomes$Geography,1,-12)\nincomes<-rename(incomes,subregion=`Geography`)\n#Resolve YOB vs. YEAR column names.\nbr<-rename(br, YEAR=YOB)\n#Resolve MRACER vs. RACE column names.\nbr<-rename(br, RACE=MRACER)\n\nWe massage br so that we can compare the br and dr data, and so that we will be able to make a map later.\n\n## The birth and death tables contain different codings for race, so I have to change that smh. I begin by defining a function \"collapse\" which translates the birth data race coding to dezath data race  coding. \ncollapse<-function(x){\n  if(x==1|x==2|x==3)\n    return(x)\n  else\n    return(4)\n}\nbr$RACE<-sapply(br$RACE,collapse)\ncounty.conversion$CORES<-as.double(county.conversion$CORES)\nbr<-left_join(br,county.conversion[,1:2])\nbr$COUNTY<-tolower(br$COUNTY)\nbr<-rename(br, subregion=COUNTY)\nsimple_incomes<-incomes[incomes$`ID Race`==0&incomes$`ID Year`==2016,c(5,7)]\nsimple_incomes$subregion<-tolower(simple_incomes$subregion)\nbr<-left_join(br, simple_incomes)\n\nIn order to minimze some of the variance associated with the small county data, we form 17 clusters of counties, and we will take in-cluster means of all the data in br.\n\ncountycounts<-count(br,CORES)\ncountyracecounts<-count(br,CORES,RACE)\nrace1births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==1,]$n)\n}\nrace2births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==2,]$n)\n}\nrace3births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==3,]$n)\n}\nrace4births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==4,]$n)\n}\ncountycounts<-bind_cols(countycounts,race1births,race2births,race3births,race4births)\ncountycounts<-rename(countycounts,NUMBIRTH=n,NUM1=...3,NUM2=...4,NUM3=...5,NUM4=...6)\n\n##We try K-means clustering with about 20 clusters. We try to identify clusters with  We want population to have the largest effect, so we don't normalize the data. The result is probably that race plays a very small role in the size of the clusters.\nset.seed(1)\nkm.out<-kmeans(countycounts[,2:6],17,nstart=50)\nkm.out$cluster\n\n  [1]  4  9 10 12 12 11  8 11  9 14 16  2 16 15 10 17 12  4 17 12 11 10 14 17  3\n [26]  6 12  9  4  9  2  1 17  1 15 16 10 10 17 12  6 17  4 17 14 12 14  7  4  9\n [51] 16 10  2 15 15  8  9 12 12 13 11  9 14 14 16 11  1  5 10  8 17 11  9 16 11\n [76]  3 17  4 14  3 15  2  8 15  9 15 12 12  7 16 17 13 11 11  9  4 15 14  9 11\n\ncountycounts<-bind_cols(countycounts,CLUSTER=km.out$cluster)\n\nCOREStoCLUSTER<-function(x){\n  if(0<=x&x<=100)\n    return(km.out$cluster[x])\n  else\n    return(NA)\n}\nclustervals<-sapply(br$CORES,COREStoCLUSTER)\nbr<-bind_cols(br,CLUSTER=clustervals)\ndrclustervals<-sapply(dr$CORES,COREStoCLUSTER)\ndr<-bind_cols(dr,drclustervals)\ndr<-rename(dr,CLUSTER=...6)\n\n\nbrdrcounts<-count(br,YEAR,CLUSTER,RACE)\nbrdrcounts<-rename(brdrcounts,NUMBIRTH=n)\n#I noticed, for example, that the original brdrcounts had no entry for (YEAR,CORES,RACE)=(2011,3,3). Based on a superficial check of a few of the missing values, this seems plausible, but I still want to set NUMBIRTH=0 for the missing combinations. Mostly RACE=3 rows are missing, which seems plausible since the Native American population of NC is around 1-2%. \nbrdrcounts<-complete(brdrcounts,YEAR,CLUSTER,RACE)\nbrdrcounts<-replace_na(brdrcounts,list(NUMBIRTH=0))\ndrcounts<-count(dr,YEAR,CLUSTER,RACE)\n#Same sort of completion for death counts. There are even more missing combos, since the death numbers overall in a low-population county can be quite small.\ndrcounts<-complete(drcounts,YEAR,CLUSTER,RACE)\ndrcounts<-replace_na(drcounts,list(n=0))\n\nbrdrcounts<-left_join(brdrcounts,drcounts)\nbrdrcounts<-rename(brdrcounts,NUMDEATH=n)\n\n##I add the group means for each statistic in br to the birth and death counts.\nbrdrcounts<-bind_cols(brdrcounts,CIGPN=rep(0,nrow(brdrcounts)),CIGFN=rep(0,nrow(brdrcounts)),CIGSN=rep(0,nrow(brdrcounts)),CIGLN=rep(0,nrow(brdrcounts)),BWTG=rep(0,nrow(brdrcounts)),GEST=rep(0,nrow(brdrcounts)),PLUR=rep(0,nrow(brdrcounts)), MAGE=rep(0,nrow(brdrcounts)),PARITY=rep(0,nrow(brdrcounts)),INCOME=rep(0,nrow(brdrcounts)))\n\nfor (i in 2011:2016) {\n  for(j in 1:17){\n   for(k in 1:4){\n     subset<-br[br$YEAR==i&br$CLUSTER==j&br$RACE==k,]\n     cigpn<-mean(subset$CIGPN)\n     cigfn<-mean(subset$CIGFN)\n     cigsn<-mean(subset$CIGSN)\n     cigln<-mean(subset$CIGLN)\n     bwtg<-mean(subset$BWTG)\n     gest<-mean(subset$GEST)\n     plur<-mean(subset$PLUR)\n     mage<-mean(subset$MAGE)\n     parity<-mean(subset$PARITY)\n     incomes<-mean(subset$`Household Income by Race`)\n     n<-which(brdrcounts$YEAR==i&brdrcounts$CLUSTER==j&brdrcounts$RACE==k)[[1]]\n     brdrcounts[n,6]<-cigpn\n     brdrcounts[n,7]<-cigfn\n     brdrcounts[n,8]<-cigsn\n     brdrcounts[n,9]<-cigln\n     brdrcounts[n,10]<-bwtg\n     brdrcounts[n,11]<-gest\n     brdrcounts[n,12]<-plur\n     brdrcounts[n,13]<-mage\n     brdrcounts[n,14]<-parity\n     brdrcounts[n,15]<-incomes\n     } \n  }\n}\nbrdrcounts<-replace_na(brdrcounts,list(CIGPN=0,CIGFN=0,CIGSN=0,CIGLN=0,BWTG=0,GEST=0,PLUR=0,MAGE=0,PARITY=0,INCOME=0))\nimrates<-brdrcounts$NUMDEATH/brdrcounts$NUMBIRTH\nbrdrcounts<-bind_cols(brdrcounts,IMR=imrates)\n## We chose not to replace NaN IMR with 0.\n\nThese are the indices in brdrcounts of the CLUSTER/RACE/YEAR combos which have no births:\n\nwhich(brdrcounts$NUMBIRTH==0)\n\n[1]  27  95 163 231 299 367\n\n\nI’ll take that, as better than having many.\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.\n\n#brdrcounts$YEAR<-as.factor(brdrcounts$YEAR)\nbrdrcounts$RACE<-as.factor(brdrcounts$RACE)\nbrdrcounts$CLUSTER<-as.factor(brdrcounts$CLUSTER)\ntrain.data<-brdrcounts[brdrcounts$YEAR!=2016,]\ntest.data<-brdrcounts[brdrcounts$YEAR==2016,]\nfoldsnums<-sample(1:10,nrow(train.data),replace=TRUE)\nfits<-list(length=10)\nfor(i in 1:10){\n  foldslogical<-foldsnums==i\n  fits[[i]]<- glm(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,subset=foldslogical,na.action = na.exclude)\n}\nlibrary(leaps)\nregfit<-regsubsets(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,nvmax=35)\nval.errors<-rep(NA,30)\ntest.mat<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,data=test.data)\nfor (i in 1:30) {\n  coefi<-coef(regfit,id=i)\n  pred<-test.mat[,names(coefi)]%*% coefi\n  val.errors[i]<-mean((na.omit(test.data$IMR)-pred)^2)\n}\n##logreg.bycty<-glm(SURVIVED~CORES+YEAR+RACE,data=br,subset=train,family=\"binomial\")\n\nThe above is a validation-set approach to variable selection. I find that the best model predicts infant-mortality rates taking into account only whether or not the group is African American:\n\nwhich.min(val.errors)\n\n[1] 1\n\ncoef(regfit,which.min(val.errors))\n\n(Intercept)       RACE2 \n0.005308092 0.009012201 \n\nsubset.error<-val.errors[which.min(val.errors)]\nsubset.error\n\n[1] 0.0002065257\n\n\nLet’s try lasso regression.\n\nx<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,train.data)[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n\n\n\ncoef(cv.lasso,cv.lasso$lambda.min)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  1.732326e-01\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     8.439871e-04\nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11   -7.764165e-04\nCLUSTER12    1.146498e-03\nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        4.158737e-03\nRACE3        .           \nRACE4       -2.297542e-03\nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -4.663241e-06\nGEST        -3.926446e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n\ncoef(cv.lasso,cv.lasso$lambda.1se)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  6.808897e-02\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     .           \nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11    .           \nCLUSTER12    .           \nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        3.111985e-03\nRACE3        .           \nRACE4        .           \nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -7.565233e-07\nGEST        -1.529002e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n\n\n\nsimple.lasso.model <- glmnet(x, y, alpha = 1, family = \"gaussian\", lambda = cv.lasso$lambda.1se)\nbest.lasso.model <- glmnet(x,y,alpha=1,family=\"gaussian\",lambda=cv.lasso$lambda.min)\ntest.x<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,test.data)[,-1]\nsimple.preds<-predict(simple.lasso.model,test.x)\nbest.preds<-predict(best.lasso.model,test.x)\nsimple.lasso.error<-mean((simple.preds-na.omit(test.data$IMR))^2)\nsimple.lasso.error\n\n[1] 0.0002038072\n\nbest.lasso.error<-mean((best.preds-na.omit(test.data$IMR))^2)\nbest.lasso.error\n\n[1] 0.0002161291\n\n\nThe simple model, which uses only RACE2 (African-American), birth weight in grams, and gestation period, is better than the more complex one on the test data. Also, it is better than the best subset selected model.\nAn unpruned tree model does worse than all three:\n\nlibrary(tree)\ntree.model<-tree(IMR~.-NUMBIRTH-NUMDEATH,train.data)\nsummary(tree.model)\n\n\nRegression tree:\ntree(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = train.data)\nVariables actually used in tree construction:\n[1] \"BWTG\"    \"GEST\"    \"CLUSTER\" \"PLUR\"    \"PARITY\"  \"CIGFN\"   \"RACE\"   \n[8] \"MAGE\"   \nNumber of terminal nodes:  14 \nResidual mean deviance:  3.229e-05 = 0.01036 / 321 \nDistribution of residuals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.0242200 -0.0018800 -0.0005768  0.0000000  0.0016300  0.0265900 \n\nplot(tree.model)\ntext(tree.model,pretty=0)\n\n\n\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\ntree.error\n\n[1] 0.0002433887\n\n\nLet’s do some tree pruning. The following shows that the minimum deviance is obtained via a tree with 5 nodes. The pruning improves the test MSE a bit, but it still doesn’t beat the lasso.\n\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n\n\n\nprune.tree.model<-prune.tree(tree.model,best=5)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n\n\n\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\nprune.error\n\n[1] 0.0002200646\n\n\nLet’s try random forests. These don’t provide a dramatic improvement over the single tree. Continue to confirm that RACE, GEST, and BWTG are biggest predictors.\n\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-NUMBIRTH-NUMDEATH,na.omit(train.data), importance=TRUE)\nrf.model\n\n\nCall:\n randomForest(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = na.omit(train.data),      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 5.982042e-05\n                    % Var explained: 18.83\n\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nrf.error\n\n[1] 0.0002202987\n\nvarImpPlot(rf.model)\n\n\n\n\nHave to try boosting:\n\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-NUMBIRTH-NUMDEATH,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n\n\n\n\n            var   rel.inf\nCLUSTER CLUSTER 31.569196\nGEST       GEST 12.781752\nBWTG       BWTG 10.765729\nPLUR       PLUR  9.510822\nRACE       RACE  7.189561\nMAGE       MAGE  6.451966\nPARITY   PARITY  6.013129\nCIGPN     CIGPN  4.016546\nCIGFN     CIGFN  3.494723\nINCOME   INCOME  3.101411\nCIGSN     CIGSN  1.897428\nCIGLN     CIGLN  1.866473\nYEAR       YEAR  1.341264\n\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)\nboost.error\n\n[1] 0.0002398635\n\n\nNow, we summarize our results. As a whole, the regularized linear methods worked the best. Of the trees, the best method was the pruned tree.\n\nmean.error<-mean(na.omit(test.data$IMR)-mean(na.omit(train.data$IMR)))^2\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\",\"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n\n                       Method   Test.Error\n1 No Dependence on Predictors 1.541823e-06\n2    Best Subset Linear Model 2.065257e-04\n3                Simple Lasso 2.038072e-04\n4            Lowest MSE Lasso 2.161291e-04\n5                        Tree 2.433887e-04\n6                 Pruned Tree 2.200646e-04\n7              Random Forests 2.202987e-04\n8                    Boosting 2.398635e-04\n\n\n\nbr.dr.race.year<-count(br,RACE,YEAR)\nbr.dr.race.year<-rename(br.dr.race.year,numbirth=n)\nbr.dr.race.year<-left_join(br.dr.race.year, count(dr,RACE,YEAR))\nbr.dr.race.year<-rename(br.dr.race.year,numdeath=n)\ndata.plot<-ggplot() + \n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"White\"), data=br.dr.race.year[br.dr.race.year$RACE==1,], )+\n geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Black\"), data=br.dr.race.year[br.dr.race.year$RACE==2,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Indian\"), data=br.dr.race.year[br.dr.race.year$RACE==3,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Other\"), data=br.dr.race.year[br.dr.race.year$RACE==4,], )+\nscale_color_manual(name = \"Race\", values = c(\"White\" = \"darkred\", \"Black\" = \"darkblue\",\"Indian\"=\"darkgreen\",\"Other\"=\"black\"))\ndata.plot\n\n\n\n\nGoing to try to make a map of North Carolina, and to see the county FIPs.\n\nbr.dr.county<-count(br,subregion)\nbr.dr.county<-rename(br.dr.county,numbirth=n)\ndr.county<-count(dr,CORES)\ndr.county<-left_join(dr.county,county.conversion[,1:2])\ndr.county<-rename(dr.county,subregion=COUNTY)\ndr.county<-rename(dr.county,numdeath=n)\ndr.county$subregion<-tolower(dr.county$subregion)\nbr.dr.county<-left_join(br.dr.county,dr.county)\nimr<-br.dr.county$numdeath/br.dr.county$numbirth\nbr.dr.county<-bind_cols(br.dr.county,IMR=imr)\nnccodr<-inner_join(nc_county,br.dr.county,by=\"subregion\")\ndrmap <- nc_base + \n      geom_polygon(data = nccodr, aes(fill = IMR), color = \"white\") +\n      geom_polygon(color = \"black\", fill = NA) +\n      theme_bw() +\n      ditch_the_axes+\n      scale_fill_gradient(\n        low = \"#00FFFF\",\n        high = \"#000000\",\n        space = \"Lab\",\n        na.value = \"grey50\",\n        guide = \"colourbar\",\n        aesthetics = \"fill\"\n        )\ndrmap"
  },
  {
    "objectID": "posts/23-03-14-post-1/index.html",
    "href": "posts/23-03-14-post-1/index.html",
    "title": "Rubik’s Cube Solver",
    "section": "",
    "text": "I have uploaded my old Rubik’s Cube project to GitHub here. The project doesn’t use deep learning or ML, but it may be interesting to readers nevertheless."
  },
  {
    "objectID": "posts/23-02-02-post-1/index.html",
    "href": "posts/23-02-02-post-1/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "For this blog post, I want to talk about principal components analysis (PCA). This is something I learned about from the great book Introduction to Statistical Learning with R by James, Witten, Hastie, and Tibshirani. This is another one of those “delve deeper into the math theory” posts.\nSo the book presents two different equivalent formulations of the principal components problem. To study this, let \\(X\\) be our data set, represented as a \\(n\\times p\\) matrix. We assume that the columns of \\(X\\) have zero mean. We will use symbols like \\(\\phi_1\\) to represent a \\(p\\)-vector, and symbols like \\(z_1\\) to represent an \\(n\\)-vector. We will think of vectors always as column vectors and use the superscript \\(T\\) to denote the corresponding row vectors, e.g. \\(\\phi_1^T\\). The first formulation of the PCA problem is in the following optimization problem: we want to maximize \\(\\phi^T X^T X \\phi= ||X\\phi||^2\\) by varying \\(\\phi\\) among all unit \\(p\\)-vectors. The vector \\(z_1 = X\\phi_1\\) is the linear combination of our features with the highest variance (for this interpretation, it was important to set the columns of \\(X\\) to have zero mean). The matrix \\(X^T X\\) is symmetric and positive-semi-definite, so it can be diagonalized and all its eigenvalues are non-negative. Let us assume that \\(X^T X\\) has only positive eigenvalues, since a zero eigenvalue would mean that some linear combination of the columns of \\(X\\) is zero, and therefore that one of the features encoded in \\(X\\) is redundant. Let \\(y_1,\\ldots, y_p\\) be the eigenvectors of \\(X^TX\\), and \\(\\lambda_1,\\ldots, \\lambda_p\\) be the corresponding eigenvalues, arranged in increasing order. Then, if we write \\[\\phi = c_1y_1+\\cdots+c_py_p,\\] we have \\[||\\phi||^2 = \\sum_i c_i^2\\] and \\[||X\\phi||^2 = \\sum_i \\lambda_i c_i^2.\\] This is a constrained optimization problem (maximize \\(\\sum_i c_i^2\\lambda_i\\) subject to \\(\\sum_i c_i^2=1\\)) and it can be solved by Lagrange multipliers. The maximum value for the objective function is \\(\\lambda_p\\) and is obtained when \\(c_p=1\\) and the other \\(c\\)s are zero. Then, the first principal component loading vector \\(\\phi_1 =y_p\\) is the eigenvector of \\(X^TX\\) corresponding to the highest eigenvalue, and the corresponding principal component is \\(z_1 = X\\phi_1\\). Next, we seek \\(z_2 = X\\phi_2\\) such that \\(z_2^T z_2\\) is maximal among \\(z_2\\) such that \\(z_2^T z_1=0\\). Since \\(z_1 = Xy_p\\) and \\(z_2 = X\\phi_2\\), we have \\[z_2^T z_1 = \\phi_2^T X^TX y_p = \\lambda_p \\phi_2^T y_p=\\lambda_p \\phi_2^T\\phi_1.\\] So, because \\(\\lambda_p\\neq 0\\), \\(z_1\\) and \\(z_2\\) are orthogonal only if \\(\\phi_1\\) and \\(\\phi_2\\) are orthogonal. So, finding \\(\\phi_2\\) amounts to finding the next highest eigenvalue \\(\\lambda_{p-1}\\) and the corresponding eigenvector \\(y_{p-1}\\) (the orthogonal complement to \\(\\phi_1\\) is spanned by \\(y_1,\\ldots, y_{p-1}\\), so we simply repeat the argument that got \\(y_1\\) on this smaller space). And so on, till we’ve found the \\(m\\) largest eigenvalues of \\(X^TX\\) and have decided to stop. To recap: in PCA, we\n\nFind the linear combination \\(z_1=X\\phi_1\\) of the columns of \\(X\\) which has maximal variance. This corresponds to picking the eigenvector of \\(X^TX\\) with maximal eigenvalue.\nAmong the linear combinations \\(z_2 = X\\phi_2\\) of the columns of \\(X\\) wich \\(z_2\\) orthogonal to \\(z_1\\), we choose the one which has maximal variance. This amounts to picking out the second-largest eigenvalue of \\(X^TX\\).\nAnd so on… We pick out the \\(m\\) largest eigenvalues of \\(X^TX\\), their corresponding eigenvectors, and their corresponding principal components.\n\nThe book claims but doesn’t show that it this procedure is equivalent to considering the following optimization problem instead: minimize \\(\\mathrm{tr}((X-AB)^T(X-AB))\\) as a\\(A\\) ranges over the space of \\(n\\times m\\) matrices and \\(B\\) ranges over the space of \\(m \\times p\\) matrices. Let’s show this. First, let’s note that we can write \\(AB = \\sum_{j=1}^m z_i \\phi_i^T\\), where the \\(z_i\\) are \\(n\\) vectors and the \\(\\phi_i\\) are \\(p\\)-vectors (unrelated so far to the ones found from the first formulation of PCA). We may assume that the \\(\\phi_i\\) are linearly independent (for if we can write one \\(\\phi\\) as a linear combination of the others, we can manifest this as a redefinition of the \\(z\\)’s). For a similar reason, we may assume that the \\(\\phi_i\\) are orthonormal. Under these assumptions, we may write \\[\n\\begin{aligned}\n\\mathrm{tr}((X-AB)^T(X-AB))&= \\mathrm{tr}(X^TX)-2\\sum_{i} \\mathrm{tr}(X^T z_i \\phi_i^T)+\\sum_{i,j}\\mathrm{tr}(\\phi_i z_i^T z_j \\phi_j^T)\\\\\n&= \\mathrm{tr}(X^TX)- 2\\sum_{i} \\phi_i^T X^T z_i + \\sum_{i} z_i^T z_j,\n\\end{aligned}\n\\] where we have used that \\[\n\\mathrm{tr}(\\phi' \\phi^T) = ||\\phi||^2 \\phi \\cdot \\phi'\n\\] for any two \\(p\\)-vectors \\(\\phi\\), \\(\\phi'\\). So, we are trying to find \\(z_i\\) and \\(\\phi_i\\) to minimize the above trace, subject to the condition that \\(\\phi_i \\cdot \\phi_j = \\delta_{i,j}\\) (i.e. the \\(\\phi\\)’s are orthonormal). There is no constraint on the \\(z\\)’s, and the simple optimization problem for the \\(z\\)’s gives \\[z_i = X\\phi_i\\]. The optimization problem with respect to the \\(y_i\\) is a bit more subtle because the \\(\\phi\\)’s are constrained. But the Lagrange optimization problem tells us that \\[\nX^Tz_i =X^TX\\phi_i = \\sum_j \\beta_j \\phi_j,\n\\] where the \\(\\beta_i\\) are undetermined Lagrange multipliers. But this tells us that \\(X^TX\\) preserves the space spanned by the \\(\\phi_i\\). Let’s call this space \\(W\\). Taking all this into account, the objective function becomes \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX).\n\\] To compute the second trace in the above equation, we just need to know the eigenvalues of \\(X^TX\\) when restricted to \\(W\\). Let us suppose that the eigenvalues of \\(X^TX\\) on \\(W\\) are \\(\\lambda_{i_1},\\ldots, \\lambda_{i_m}\\). Let \\(\\lambda_{i_{m+1}},\\ldots, \\lambda_{i_{p}}\\) be the remaining eigenvalues. Then, \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX)=\\sum_{j=m+1}^p\\lambda_{i_j}.\n\\] So the objective will be minimized precisely by choosing the \\(m\\) largest eigenvalues of \\(X^TX\\), just as in the first formulation of PCA! It follows that to minimze \\[\\mathrm{tr}((X-AB)^T(X-AB)),\\] we choose the \\(m\\) largest eigenvalues of \\(X^TX\\) (which above we called \\(\\lambda_p, \\lambda_{p-1},\\ldots, \\lambda_{p-m+1}\\), with respective eigenvectors \\(y_p,\\ldots, y_{p-m+1}\\)), set \\(\\phi_i = y_{p-i+1}\\), \\(z_i = Xy_{p-i+1}\\), and \\(AB = \\sum_{i=1}^m z_i \\phi_i^T\\), as desired."
  },
  {
    "objectID": "posts/23-01-12-post-1/index.html",
    "href": "posts/23-01-12-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left(b_i+ \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  },
  {
    "objectID": "posts/23-01-10-post-1/index.html",
    "href": "posts/23-01-10-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "",
    "text": "In November 2022, my friend gave me the datasets included in the “data” folder for this post as “Yr1116Birth.csv” and “Yr1116Death.csv” and told me to develop a model that predicts a county-by-county infant mortality rate based on various demographic features that were recorded in the data. The data lists births and deaths of infants in the years 2011-2016 in North Carolina by county of residence, race, and ethnicity of the mother. The Excel file Dataset Descriptions.xls lists all the information available in this dataset, and I presume that the information comes from the NC State Center for Vital Statistics. I’ve also supplemented the data with county-by-county income data which I found at datausa, though this data is also readily available from various public agencies like the Census Bureau. I use the data to compute infant mortality rates by county, race, and year. I used the 2011-2015 data to train my models and the 2016 data for validation.\nThis post is a write-up of my analysis of this data. This was an interesting task because it was a crash course in a few important data science skills: data wrangling, rare event modeling, and maps & data visualization.\n\n\nMy data came from a number of different sources. There were separate birth and death data tables, the income data came from its own source, and the purely geographic data used to generate the map at the end of this post came from the mapdata package.\nCombining these sources presented some difficulty. The birth and death data had different numerical codings for the race of the baby/mother, so I had to make a custom function collapse which collapsed the many extra race codings in the birth data into the simpler “other” category in the death data. In a similar vein, different sources encoded county information in different ways: North Carolina orders the counties from 1 to 100, but there is also a FIPS code which is more useful for national data, and of course the name of a county is a fine label for it. One defect in my data was that one of my sources mis-spelled “Tyrrell”, and it took me a while to detect this error.\n\n\n\nWhen my friend presented me with this data, he and I discussed the interesting fact that some counties recorded no infant deaths for certain races in certain years. I don’t think that this was due to incomplete records or reporting anomalies: when I investigated these cases, I found that there were fewer than 100 births in the previous year in the same race and county. The overall infant mortality rate was about .7% in these years, so the expected number of infant deaths when there are fewer than 100 births is less than 1.\nMy friend raised the possibility that I could model this problem as a classification problem: given a partiular infant birth, predict the probability that it would die in the first year of its life. I considered this possibility, but decided not to do the analysis in this way, since the birth data contained more information, like the infant’s birth weight, that was not reflected in the death data, so I thought it might be hard to measure the effect of these additional variables on a given infant’s likelihood of death. So, instead I modeled the problem as a regression problem: predict a county’s infant mortality rate in a given county by year and race, given the county’s average values for the other predictors in the birth data (e.g., birth weight in grams, median income in the county, number of cigarettes smoked by the mother).\nNevertheless, the data still presented the challenges associated with classification problems in which the classes are very unbalanced in number. To get a feel for why there is an issue, let’s consider one of those counties where there were no infant deaths in a given year. Because infant mortality rates are on the order of 1/1000, to detect significant changes in rates between counties, it makes sense to measure them on a log scale. The counties where there were no infant deaths, we would record an infant mortality rate of 0, which would be (infinitely) many orders of magnitude smaller than the typical rate. To solve this, I added .001 to the infant mortality rates before taking the logarithm. This is sort of like label smoothing: I don’t want the model to make too much of those points where there happened to be no infant deaths.\nOne other thing I tried to do was to aggregate the counties into clusters and compute only in-cluster infant mortality rates for the training data. This was an attempt to reduce year-to-year variance due to small sample sizes. However, I found that my implementation of this idea didn’t really improve the validation-set error. So, in the end, I didn’t put this into practice. But, if you look at the code for this notebook, you’ll find relics of that approach.\n\n\n\nThis project was a good way for me to practice what I had learned about making data visualizations from Introduction to Statistical Learning with R, including feature importance charts. But it was also an opportunity for me to learn how to make a county-by-county heat map; this map appears in the last section of this"
  },
  {
    "objectID": "posts/23-01-18-post-1/index.html",
    "href": "posts/23-01-18-post-1/index.html",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "",
    "text": "In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.\nIt actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.\n\n\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\((N-1)\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}(w,x_i).\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) is something that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand it, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\mathbb{R}^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#loading-packages-and-cleaning-data",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#loading-packages-and-cleaning-data",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Loading Packages and Cleaning Data",
    "text": "Loading Packages and Cleaning Data\nIn this section, I clean the data and wrangle it into a form amenable to analysis. I omit most of this process from the presentation version of the notebook, but the interested readers can examine the code, which is available on my Github page.\nThe result of all these manipulations is a data frame brdrcounts which looks like this:\n\nhead(brdrcounts)\n\n# A tibble: 6 × 15\n   YEAR CORES  RACE CIGPN CIGFN CIGSN CIGLN  BWTG  GEST  PLUR  MAGE PARITY\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1  2011     1     1 3.25  1.91  1.43  1.33  3318.  38.7  1.03  27.1   2.33\n2  2011     1     2 2.58  1.58  1.06  0.942 3089.  38.2  1.05  25.5   2.77\n3  2011     1     3 6.5   5     5     5     2758.  38.5  1     25     2.75\n4  2011     1     4 0.347 0.180 0.178 0.159 3287.  38.7  1.03  27.2   3.05\n5  2011     2     1 2.93  1.87  1.63  1.49  3252.  38.2  1.05  26.6   2.71\n6  2011     2     2 0     0     0     0     3433.  39.5  1     23.4   2.15\n# … with 3 more variables: INCOME <dbl>, IMR <dbl>, CLUSTER <dbl>\n\n\nMost of the columns are explained in the file Dataset Descriptions.xls; “IMR” is the log of infant mortality rate, and “Cluster” is just a duplicate of “CORES” (it’s an artifact from when I tried to apply clustering to the data)."
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#trying-different-models",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#trying-different-models",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Trying Different Models",
    "text": "Trying Different Models\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.\nThe first method we try is just a linear model; we perform subset selection by validation-set MSE.\n\nwhich.min(val.errors)\n\n[1] 10\n\ncoef(regfit,which.min(val.errors))\n\n  (Intercept)         RACE2         RACE3         CIGPN         CIGFN \n-1.983988e+00  4.961281e-01  1.442747e-01 -7.878452e-03 -5.804568e-03 \n        CIGSN          BWTG          GEST          PLUR        PARITY \n 1.456896e-02 -1.282548e-04 -4.569572e-02 -4.074168e-01  7.307301e-03 \n       INCOME \n-9.240510e-06 \n\nsubset.error<-val.errors[which.min(val.errors)]\n\nThe best model seems to associate a decline in infant mortality rate if the mother is American Indian or “Other” (not White, Black, or American Indian). It’s hard to understand the sign of the coefficients for “CIGPN” and “CIGFN”. My guess is that this has to do with the fact that I imputed a slightly lower-than-average infant mortality rate when the death count for a given county, race, and year is zero.\nLet’s now try lasso regression.\n\nx<-model.matrix(IMR~.-CORES,train.data[,1:14])[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n\n\n\ncoef(cv.lasso,cv.lasso$lambda.min)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -7.914631e+00\nYEAR         2.907554e-03\nRACE2        4.879737e-01\nRACE3        1.309074e-01\nRACE4       -7.877888e-03\nCIGPN       -6.122286e-03\nCIGFN        .           \nCIGSN        6.751858e-03\nCIGLN        .           \nBWTG        -1.183393e-04\nGEST        -4.529146e-02\nPLUR        -3.807254e-01\nMAGE        -1.634793e-04\nPARITY       6.375718e-03\nINCOME      -8.955468e-06\n\ncoef(cv.lasso,cv.lasso$lambda.1se)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) -4.8453328\nYEAR         .        \nRACE2        0.1535657\nRACE3        .        \nRACE4        .        \nCIGPN        .        \nCIGFN        .        \nCIGSN        .        \nCIGLN        .        \nBWTG         .        \nGEST         .        \nPLUR         .        \nMAGE         .        \nPARITY       .        \nINCOME       .        \n\n\nThe 1se lambda value gives a model in which the only predictor is RACE2 (African American).\nNow we try ridge regression:\n\ncv.ridge<-cv.glmnet(x,y,alpha=0,family=\"gaussian\")\nplot(cv.ridge)\n\n\n\ncoef(cv.ridge,cv.ridge$lambda.min)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -1.002797e+01\nYEAR         3.996398e-03\nRACE2        4.597315e-01\nRACE3        1.149176e-01\nRACE4       -2.115184e-02\nCIGPN       -5.598731e-03\nCIGFN       -1.539379e-03\nCIGSN        5.012017e-03\nCIGLN        2.696535e-03\nBWTG        -1.336627e-04\nGEST        -4.438239e-02\nPLUR        -3.947143e-01\nMAGE        -2.115157e-03\nPARITY       7.580164e-03\nINCOME      -8.632730e-06\n\ncoef(cv.ridge,cv.ridge$lambda.1se)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -5.599637e+00\nYEAR         8.684702e-04\nRACE2        8.397161e-02\nRACE3       -3.927352e-03\nRACE4       -3.221903e-02\nCIGPN       -6.046998e-05\nCIGFN       -5.124061e-05\nCIGSN       -9.542885e-06\nCIGLN       -3.908353e-05\nBWTG        -5.967245e-05\nGEST        -1.357928e-02\nPLUR        -1.939902e-02\nMAGE        -6.071366e-03\nPARITY       7.083761e-04\nINCOME      -1.630609e-06\n\n\nLet’s try to train a single tree.\n\nlibrary(tree)\ntree.model<-tree(IMR~.-CORES,train.data)\nsummary(tree.model)\n\n\nRegression tree:\ntree(formula = IMR ~ . - CORES, data = train.data)\nVariables actually used in tree construction:\n[1] \"RACE\"   \"INCOME\" \"PLUR\"   \"GEST\"   \"CIGLN\" \nNumber of terminal nodes:  6 \nResidual mean deviance:  0.3125 = 582.5 / 1864 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.33800 -0.15540 -0.08285  0.00000  0.14070  4.18600 \n\nplot(tree.model)\ntext(tree.model,pretty=0)\n\n\n\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\n\nThe most notable differences in IMR come from GEST and RACE2.\nLet’s do some tree pruning. The following graph shows that the minimum deviance is obtained via a tree with 6 nodes; however, there doesn’t seem to be much difference between a tree with 3 nodes and a tree with 6 nodes.\n\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n\n\n\nprune.tree.model<-prune.tree(tree.model,best=3)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n\n\n\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\n\nAs we noted above, GEST and RACE2 are the major factors in this tree.\nLet’s try random forests.\n\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-CORES,na.omit(train.data[,1:14]), importance=TRUE)\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nvarImpPlot(rf.model)\n\n\n\n\nAgain, “RACE”, “BWTG”, and the various “CIG” predictors appear near the top of the %IncMSE chart.\nLast model is boosting:\n\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-CORES,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n\n\n\n\n            var   rel.inf\nGEST       GEST 14.410847\nBWTG       BWTG 12.606412\nMAGE       MAGE 11.118088\nPARITY   PARITY 10.495755\nCLUSTER CLUSTER  8.163164\nINCOME   INCOME  7.517338\nCIGPN     CIGPN  6.803060\nPLUR       PLUR  6.779247\nCIGFN     CIGFN  5.553177\nCIGLN     CIGLN  4.856280\nCIGSN     CIGSN  4.735801\nRACE       RACE  4.725559\nYEAR       YEAR  2.235272\n\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)\n\n##Summary of Results\nNow, we summarize our results in a table:\n\nmean.error<-mean((na.omit(test.data$IMR)-log(avgIMR+.007))^2)\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\", \"Simple Ridge\", \"Lowest MSE Ridge\", \"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,simple.ridge.error, best.ridge.error, tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n\n                        Method Test.Error\n1  No Dependence on Predictors  0.6817740\n2     Best Subset Linear Model  0.3456520\n3                 Simple Lasso  0.3640914\n4             Lowest MSE Lasso  0.3462914\n5                 Simple Ridge  0.3660809\n6             Lowest MSE Ridge  0.3466169\n7                         Tree  0.3436803\n8                  Pruned Tree  0.3643193\n9               Random Forests  0.3240173\n10                    Boosting  0.4121946\n\n\nRandom forests seems to have done the best. This is consistent with its reputation as the best out-of-the-box method. Boosting is finnicky, and probably required some more hyperparameter tuning.\n##Graphs and Visualizations This is a graph of infant mortality rates by race and year.\n\n\n\n\n\nThis is a heat map of North Carolina by infant mortality rate:"
  },
  {
    "objectID": "posts/23-03-07-post-1/NC_Birth_and_Death.html#conlcusion",
    "href": "posts/23-03-07-post-1/NC_Birth_and_Death.html#conlcusion",
    "title": "Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016",
    "section": "Conlcusion",
    "text": "Conlcusion\nIt was interesting to go back to this project a few months after I first did it, because I noticed a lot of places where what I have learned in the interim could have come in handy. For example, I now have more robust EDA and feature engineering frameworks. Keep an eye out for a future blog post in which I discuss these issues in more depth in the context of my participation in recent Kaggle competitions."
  },
  {
    "objectID": "posts/23-03-15-post-1/index.html",
    "href": "posts/23-03-15-post-1/index.html",
    "title": "What I’ve Learned from My First Kaggle Competitions",
    "section": "",
    "text": "Introduction\nIn the past few months, I’ve sharpened my data science axe on three different Kaggle competitions: two tabular binary classification problems, and one tabular ordinal regression problem.\nI’ve learned a lot about how to set up a full data pipeline, and much of what I learned comes from this notebook.\nYou could classify what I’ve learned into three categories: tools, techniques for EDA and feature engineering, and rare event modeling.\n\n\nNew Tools\nI’ve learned about XGBoost, which seems to be a favored boosting method on Kaggle. In the past, I’ve had trouble getting boosting to match random forests in performance, but I also learned about optuna, which is a framework for hyperparameter tuning. Together with optuna, I can now use XGBoost to get pretty good baseline performance.\n\n\nNew Techniques for EDA and Feature Engineering\nOne good way to start exploratory data analysis (EDA) is to produce pair plots for all the variables. This was already something I was doing. Sometimes, though, there are very many predictors, so it’s difficult to pick out information by eye from the pair plots. In that case, it helps to restrict to plots of the dependent variable against the predictors. I’ve also taken to computing Pearson corelation coefficients between the dependent variable and the predictors. For classification problems, it’s also helpful to model predictor distributions by class, as in the following image, which comes from a hotel bookings dataset: \nI look for graphs where the two distributions are noticeably different. In the above image, for example, the three most important features amenable to feature engineering seemed to be “avg_price_per_room”, “lead_time”, and “no_of_special_requests”. For feature engineering, it’s important to try some interactions and higher-order polynomial terms involving the most significant predictors. It’s worth investigating certain ratios between these predictors as well. To get great results, though, it’s also important to think about domain-specific combinations of predictors; for example, I tried to make 1) a “fussiness” variable which combined “no_of_special_requests” with “required_car_parking_space” and 2) a “cancellation rate” predictor. In the hotels dataset, though, this didn’t help too much.\nThe image below shows feature distributions for the most promising engineered features:\n\n\n\nEngineered Feature Distributions\n\n\nI think the best features are ones for which the distributions are different not only for the two target classes, but also different from other features already appearing in the dataset.\n\n\nRare Event Modeling\nOne of the competitions involved modeling the risk of credit card default for bank accounts. Another involved predicting a wine taster rating for different wines. In the former case, defaults were rare; in the latter case, there were very few wines that received a score of either 8 or 3 (maximum and minimum scores in the dataset, respectively). So I had to learn about methods to detect rare events. The problem with rare events is that a baseline model which predicts that the event doesn’t occur has a high accuracy. The rarity of the positive observations increases the variance of the predictors for the positive observations, so it becomes difficult to construct a model with a better accuracy than the baseline one.\nThe method I settled on was to ensemble: I divided the training data into about 100 data sets. Each data set had all of the positive observations, and a different subset of the negative observations. The result was that the classes were distributed more equally in the smaller datasets than in the original dataset. Then, I trained a classification (random forest) model on each of the 100 data sets separately, and took the mean prediction probability over the 100 models. This technique is known as “ensembling”: the idea is that each of the constituent models has a chance to learn something meaningful about what distinguishes some negative observations from the positive ones. If certain features keep appearing in the various models, then they will aggregate to an important feature in the ensemble. Another way of putting this is that we want to be close to the inflection point of the logistic function:\n\n\n\n\n\nIf we start at the distant asymptotes of the logistic function, then we will need very large coefficients in front of our predictors to produce a meaningful difference in probabilities. Then, a small amount of noise in that predictor can produce a very large change in probabilities.\nIn the end, the ensemble method will dramatically overrate the probabilities that it assigns to positive events, but usually we are more interested in accuracy or area under the ROC curve, so that the probability over-estimate can be compensated by a change in the threshold.\n\n\nConclusion\nWhen I was revisiting my infant mortality project, I noticed that most of what I discussed above would have been very applicable to that project as well. For example, distribution plots would have helped me to engineer features that may have improved my models. In any case, looking back on my Kaggle competitions has helped me to appreciate how far I’ve come in my journey since November."
  }
]