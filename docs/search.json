[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eugene’s Deep Learning Blog",
    "section": "",
    "text": "Principal Components Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\nConstructing a Neural-Net Digit Classifier (Almost) from Scratch\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\nThe Universal Approximation Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\nWelcome to my blog!\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi everybody! My name is Eugene! I’m a postdoc in pure mathematics at the University of Notre Dame. I received my PhD in mathematics from University of California, Berkeley in 2021. My research interests center around mathematical formulations of quantum field theory. As a side/pet project, I am learning about data science and machine learning. This blog is to document some of the things I’m learning. I am using the book Deep Learning for Coders and following the corresponding course from fast.ai."
  },
  {
    "objectID": "posts/first-ipynb-post/index.html",
    "href": "posts/first-ipynb-post/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces, so I may have to periodically rebuild. If it doesn’t work for you, raise an issue on the GitHub page, and I will rebuild.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay."
  },
  {
    "objectID": "posts/10-01-23-post-1/index.html",
    "href": "posts/10-01-23-post-1/index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "For my first entry, I will just plug my very simple bird-song classifier, which can be found here. It is hosted using the free platform Hugging Face Spaces. You may have to ask Hugging Face to rebuild the app, which may take some time.\nThis is a very simple classifier I built using fastai. It uses an 18-layer neural net (resnet18) to classify sounds as being the songs of one of three species of bird: American Robin, Northern Cardinal, or Blue Jay.\nUnder the hood, the classifier converts the audio file into an image/spectrogram (this accounts for most of the processing time of the app), then uses the neural net to classify the images.\nWhat I’m liking so far about the fastai course is that you build a project in lectures 1 and 2, and then go under the hood in subsequent lectures. I’m a mathematician, so I’m used to going from theoretical foundations to practice and not the other way around. That way certainly has its merits, but I’ve also noticed that in my own research, I’m much more able to digest theory if I have a sense of the kind of problem I want to solve and how the theory helps me solve that problem. I also think this practically-focused way to learn things is well-suited to my background: I am already finding that I can often fill in the backgrond theory based on the brief allusions made in the course. More on this in my next post…"
  },
  {
    "objectID": "posts/12-01-23-post-1/index.html",
    "href": "posts/12-01-23-post-1/index.html",
    "title": "The Universal Approximation Theorem",
    "section": "",
    "text": "I’ve read in the fast.ai book about the universal approximation theorem. It’s described vaguely, but since I am a mathematician by training, I’m going to try to do the following in this post: first, I’ll guess at the precise statement of the theorem. Then, I’ll look the precise statement up. And finally, I’ll try to extract some lessons from the exercise.\nSo, here’s the guess: Let \\(g:\\mathbb R \\to \\mathbb R\\) be the function\n\\[g(x) = \\max(x,0).\\]\nThis is the ReLU/rectified linear unit function. Given any other continuous function \\(f: \\mathbb R^n \\to \\mathbb R\\), any \\(\\epsilon>0\\), and any compact subset \\(S\\subset \\mathbb R^n\\), there exist constants \\(\\{c_{i}^j\\}_{i=1,j=1}^{i=n,j=m}\\), \\(\\{b_i\\}_{i=1}^n\\), \\(\\{d_j\\}_{j=1}^m\\), and \\(w\\) such that \\[ \\left| f(x^1,\\ldots, x^n) - w - \\sum_{j=1}^m d_j g\\left(b_i+ \\sum_{i=1}^n c_i^j x^i\\right)\\right|<\\epsilon,\\quad \\forall x\\in S.\\]\nTaking a look here, we see that this version of the theorem is called the “arbitrary-width” version of the theorem. The only thing which is different between the above statement and the reference in Wikipedia is that Wikipedia informs us that the theorem applies for any continuous function which is not polynomial in place of \\(g\\) (the ReLU function is not polynomial because it is not identically zero but has infinitely many zeroes). All the other differences are a matter of differences in notation but not content; the biggest such difference is that \\(f\\) is allowed on the Wikipedia page to have codomain \\(\\mathbb R^k\\) for some \\(k\\); but this follows from my case by the triangle inequality.\nOn the Wikipedia page, there are other versions of the theorem. The most interesting one to me is the one which allows one to fix \\(m\\) (the “width” of the network) to be bounded by \\(n+m+2\\) by allowing arbitrarily many layers in the network, i.e. by combining the various \\(d_j g\\) terms as the inputs to more copies of \\(g\\). This represents that tradeoff between depth and width that I’ve learned about. This works if \\(g\\) is any non-affine function. Apparently, and this is really cool to me, it’s possible to determine the minimum required depth for a fixed \\(f\\) and \\(\\epsilon\\).\nFinally, there is a version of the theorem that, by choosing a suitable candidate for \\(g\\), one can put a global bound on both the depth and width of the network! I wonder if this choice of \\(g\\) gives significant performance improvements in practice…"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html",
    "href": "posts/18-01-23-post-1/index.html",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "",
    "text": "In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.\nIt actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.\n\n\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\((N-1)\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}(w,x_i).\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) is something that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand it, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\mathbb{R}^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "href": "posts/18-01-23-post-1/index.html#aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?",
    "text": "Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?\nLet’s describe what happens in machine learning. We are given a collection of inputs and outputs \\(\\{x_i,y_i\\}\\) that are supposed to be related to each other in some way. For example, each \\(x_i\\) could be the data of an image (the RGB coordinates of each of its pixels) and \\(y_i\\) could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the \\(x_i\\) and \\(y_i\\) and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model \\(M\\), whose job is to take in an input \\(x\\) and and “spit out” an output \\(M(x)\\). Now, we better hope that \\(M(x_i)\\) is (in some way) as close to \\(y_i\\) as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that \\(M\\) itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter \\(w\\). So, more properly, we have a collection of predictions \\(M(w,x)\\), one for each value of the parameters \\(w\\) (we consider \\(x\\) to be fixed for now). I’ve learned that the name for the family of models is called the architecture of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.\nOk, so how do we describe which value of \\(w\\) is “best”? We need to measure “how far off” \\(M(w,x_i)\\) is from \\(y_i\\), and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions \\(M(w,x_i)\\). The problem is the following: if \\(w\\) and \\(x_i\\) are continuous (numerical) data types, and our output \\(y_i\\) is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—\\(M(w,x)\\) is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights \\(w\\) a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)\nSo, instead of just predicting the class that each \\(x_i\\) belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have \\(N\\) different classes (in other words that each \\(y_i\\) is whole number between 1 and \\(N\\)), and we provide \\(N\\) functions \\(P_1(w,x),\\ldots, P_j(w,x),\\ldots, P_N(w,x)\\) which represent the probabilities that the model corresponding to \\(w\\) assigns to a given input \\(x\\) producing each of the \\(N\\) possible outputs. Since these are probabilities, we need to have \\[ P_1(w,x)+P_2(w,x)+\\cdots+P_N(w,x)=1\\] and each \\(P_j\\) needs to be non-negative for all \\(w\\) and \\(x\\). (In math, we say that the functions \\(\\{P_j\\}\\) provides a function from the space \\(S\\) of parameters and input variables to the \\(N-1\\)-simplex \\(\\Delta^{N-1}\\)) Finally, we can simply set \\(M(w,x)= \\max_{j}P_j(w,x)\\). The beauty of this approach is that now we can more readily measure how good the model \\(M\\) is, and this is called the likelihood function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is \\(y_i\\) given \\(x_i\\), i.e., we form \\[L(w)= \\prod_{i} P_{y_i}.\\] \\(L(w)\\) (conceived of as a function of the parameters \\(w\\)) that we can seek to optimize, since if the \\(P\\)’s have reasonable behavior (e.g. smoothness or continuity), so too will \\(L(w)\\). In practice, we optimize \\(\\log(L(w))\\) instead. As far as I understand this, this is because since all the probabilities \\(P_{y_i}\\) are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.\nNow, usually, we have many smart ways of producing a collection of \\(N\\) functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map \\(\\RR^N \\to \\Delta^{N-1}\\) (it’s essentially the simplest such function). I won’t get into the details of that here.\nOk, so now we have a task: given this particular form of \\(L(w)\\), find the \\(w\\) which will minimize \\(-\\log(L(w))\\) (which we now call the loss function), or at least give us a resonable approximation to the maximum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights \\(w_0\\) and then replace \\(w_0\\) with \\(w_0 + \\eta \\nabla_{w}\\log(L(w_0))\\), where \\(\\eta\\) is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit \\(\\eta\\to 0\\) is this completely accurate as way to find the minima. So the smaller \\(\\eta\\) is, the more likely we will be to find the minimum of the loss function. But since \\(\\eta\\) is small, if we start with \\(w_0\\) far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak \\(\\eta\\) to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.\nFinally, I want to mention that in practice instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point \\((x_i,y_i)\\), the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called stochastic gradient descent, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss."
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/18-01-23-post-1/index.html#pixels-the-foundations-of-computer-vision",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02<00:00]\n    \n    \n\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('testing'),Path('training')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/1'),Path('training/2'),Path('training/5'),Path('training/9'),Path('training/7'),Path('training/6'),Path('training/0'),Path('training/8'),Path('training/4'),Path('training/3')]\n\n\n\nnums = [(path/'training'/str(i)).ls() for i in range(10)]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "href": "posts/18-01-23-post-1/index.html#first-try-pixel-similarity",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\nStep one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\nLet’s create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\nWe will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:\n\nnum_tensors = [[tensor(Image.open(o)) for o in nums[i]] for i in range(10)]\n\n\nlen(num_tensors), len(num_tensors[2])\n\n(10, 5958)\n\n\n\nstacks = [torch.stack(num_tensors[i]).float()/255 for i in range(10)]\nstacks[2].shape\n\ntorch.Size([5958, 28, 28])\n\n\n\nmeans = [stacks[i].float().mean(0) for i in range(10)]\n[show_image(o) for o in means]"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "href": "posts/18-01-23-post-1/index.html#computing-metrics-using-broadcasting",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\n#An ordinary Python list of pyTorch tensors, with the validation images for digit i in the ith index.\nvalids = [torch.stack([tensor(Image.open(o)) \n                            for o in (path/'testing'/str(i)).ls()]) for i in range(10)]\nvalids = [o.float()/255 for o in valids]\n\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\n\ndef mnist_distance_list(x):\n  dists = [mnist_distance(x,means[i]) for i in range(10)]\n  return torch.stack(dists)\n\ndef which_is_it(x):\n  return torch.argmin(mnist_distance_list(x),dim=0)\n\nLet’s test it on our example case:\n\na_3 = stacks[3][2007]\nmnist_distance_list(a_3), which_is_it(a_3), show_image(a_3)\n\n(tensor([0.1583, 0.1372, 0.1469, 0.1612, 0.1534, 0.1595, 0.1483, 0.1514, 0.1612, 0.1485]),\n tensor(1),\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7dc64648b0>)\n\n\n\n\n\n\nsmall_test = stacks[3][0:2]\nsmall_test.shape\nmnist_distance_list(small_test[0]), mnist_distance_list(small_test[1]),mnist_distance_list(small_test)\n\n(tensor([0.1826, 0.1183, 0.1458, 0.1037, 0.1429, 0.1319, 0.1613, 0.1408, 0.1254, 0.1331]),\n tensor([0.1965, 0.1569, 0.1953, 0.1421, 0.1825, 0.1581, 0.1766, 0.1784, 0.1759, 0.1732]),\n tensor([[0.1826, 0.1965],\n         [0.1183, 0.1569],\n         [0.1458, 0.1953],\n         [0.1037, 0.1421],\n         [0.1429, 0.1825],\n         [0.1319, 0.1581],\n         [0.1613, 0.1766],\n         [0.1408, 0.1784],\n         [0.1254, 0.1759],\n         [0.1331, 0.1732]]))\n\n\n\nwhich_is_it(small_test[0]),which_is_it(small_test[1]), which_is_it(small_test), which_is_it(small_test)==3\n\n(tensor(3), tensor(3), tensor([3, 3]), tensor([True, True]))\n\n\n\naccuracies = [(which_is_it(valids[i])==i).float().mean() for i in range(10)]\noverall_accuracy = tensor([accuracies[i]*len(valids[i]) for i in range(10)])\noverall_accuracy = overall_accuracy.sum()/10000.\naccuracies, overall_accuracy\n\n([tensor(0.8153),\n  tensor(0.9982),\n  tensor(0.4234),\n  tensor(0.6089),\n  tensor(0.6680),\n  tensor(0.3262),\n  tensor(0.7871),\n  tensor(0.7646),\n  tensor(0.4425),\n  tensor(0.7760)],\n tensor(0.6685))"
  },
  {
    "objectID": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "href": "posts/18-01-23-post-1/index.html#the-mnist-loss-function",
    "title": "Constructing a Neural-Net Digit Classifier (Almost) from Scratch",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\ntrain_x = torch.cat([stacks[i] for i in range(10)]).view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\ntrain_y = tensor([])\nfor i in range (10):\n  add = torch.stack([tensor([i]) for j in range(len(nums[i]))])\n  train_y=torch.cat((train_y,add),0)\ntrain_y= train_y.squeeze()\ntrain_x.shape, train_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000]))\n\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor(0.))\n\n\n\nvalid_x = torch.cat([valids[i] for i in range(10)]).view(-1, 28*28)\nvalid_y = tensor([])\nfor i in range(10):\n  add=torch.stack([tensor([i]) for j in range(len(valids[i]))])\n  valid_y =torch.cat((valid_y,add),0)\nvalid_y = valid_y.squeeze()\nvalid_dset = list(zip(valid_x,valid_y))\n\n\nvalid_x.shape, valid_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000]))\n\n\nNow we need an (initially random) weight for every pixel (this is the initialize step in our seven-step process):\n\ndef init_params(size, std=1.0): \n  ungrad = (torch.randn(size)*std).squeeze()\n  return ungrad.requires_grad_()\n\n: \n\n\n: \n\n\n\nweights = init_params((28*28,10))\nweights.shape\n\ntorch.Size([784, 10])\n\n\n\nbias = init_params((1,10))\nbias.shape\n\ntorch.Size([10])\n\n\n\n((train_x.float()@(weights)) + bias).shape\n\ntorch.Size([60000, 10])\n\n\n\ndef linear1(xb): return xb.float()@weights + bias\npreds_log_prob = linear1(train_x)\n\n\npreds_log_prob[:4]\n\ntensor([[-18.1373,  -6.1067,   2.9795,  -1.2941,  -2.0108,  -1.8045,  -5.9375, -10.6166,   6.8176,   9.3555],\n        [ -8.9188,   4.6967,  -3.0078,   2.0427,  -9.5952,   6.2474,  -1.6729,  -6.3322,  13.9906,  27.3094],\n        [-12.4062,  -0.5152,   6.4807,   4.4728, -13.1531,   2.5008,  -1.0917, -12.9701,   7.1787,  10.7713],\n        [ -9.2247,  11.5279,  -0.7430,   1.1205,  -5.2792, -11.2010, -11.0678, -11.6129,   5.9237,  15.7904]], grad_fn=<SliceBackward0>)\n\n\n\npreds= torch.argmax(preds_log_prob,1)\npreds = preds.reshape(60000)\npreds.shape, preds\n\n(torch.Size([60000]), tensor([9, 9, 9,  ..., 8, 9, 8]))\n\n\n\ncorrects = preds.float()==train_y.squeeze()\ncorrects\n\ntensor([False, False, False,  ..., False,  True, False])\n\n\n\ncorrects.float().mean().item()\n\n0.12043333053588867\n\n\n\ndef mnist_loss(predictions, targets):\n    targets = targets.long()\n    losses = F.cross_entropy(predictions,targets.squeeze())\n    return losses.mean()\n\n\nSGD and Mini-Batches\n\nweights = init_params((28*28,10))\nbias = init_params(10)\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nbatch = train_x[:4]\nbatch.shape, train_y[:4].shape\n\n(torch.Size([4, 784]), torch.Size([4, 1]))\n\n\n\npreds = linear1(batch)\npreds, train_y[:4]\n\n(tensor([[ 19.2725,  22.0151, -27.2578,  10.6764,  -4.2019,  22.7855,   6.2560,   4.3134,  -0.2266,  -5.1313],\n         [ 18.2444,   4.6494, -17.6840,  10.6016, -12.3112,  23.4429,   3.3657,   5.7126,   3.6596,   5.7853],\n         [ 25.9703,   9.6952, -19.0290,  10.1290,   4.2884,  21.3991,   5.5114,  -1.0090,   9.3477, -20.2521],\n         [  8.6966,   1.3858,  -6.3632,   1.8474,   2.0362,  23.2705, -12.0754,   3.2197,   4.8403,   2.4272]], grad_fn=<AddBackward0>),\n tensor([[0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(5.9254, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\nbias.grad\n\ntensor([-7.4619e-01,  7.7517e-02,  3.3744e-14,  1.6141e-06,  2.4515e-10,  6.6868e-01,  1.1896e-08,  7.0398e-09,  1.8070e-08,  5.5542e-09])\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-5.5499e-10),\n tensor([-6.0718e-02,  2.6098e-06,  8.3043e-06,  3.9334e-06,  4.4649e-02,  1.0095e-02,  1.9338e-03,  2.0388e-14,  1.0862e-08,  4.0262e-03]))\n\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\ndef batch_accuracy(xb, yb):\n    preds = torch.argmax(xb,dim=1).squeeze()\n    correct = preds.float() == yb.float().squeeze()\n    return correct.float().mean()\n\nWe can check it works:\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.2500)\n\n\nand then put the batches together:\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.1018\n\n\nThat’s our starting point. Let’s train for one epoch, and see if the accuracy improves:\n\nlr = .1\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.7798\n\n\nThen do a few more:\n\nfor i in range(30):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end='\\n')\n\n0.8844 0.8854 0.8856 0.8866 0.8869 0.8869 0.8861 0.8859 0.8862 0.8864 0.8864 0.8866 0.887 0.8873 0.8873 0.8872 0.8872 0.8874 0.8875 0.8876 0.8876 0.8878 0.8877 0.8877 0.8881 0.8881 0.888 0.888 0.8881 0.8881 \n\n\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,10))\nb2 = init_params(10)\n\n\ntrain_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\nprint(validate_epoch(simple_net), end=' ')\n\n\nfor i in range(150):\n  train_epoch(simple_net, lr=.1, params=(w1,b1,w2,b2))\n  if i%15==0:\n    print(validate_epoch(simple_net), end='\\n')"
  },
  {
    "objectID": "posts/02-02-23-post-1/index.html",
    "href": "posts/02-02-23-post-1/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "For this blog post, I want to talk about principal components analysis (PCA). This is something I learned about from the great book Introduction to Statistical Learning with R by James, Witten, Hastie, and Tibshirani. This is another one of those “delve deeper into the math theory” posts.\nSo the book presents two different equivalent formulations of the principal components problem. To study this, let \\(X\\) be our data set, represented as a \\(n\\times p\\) matrix. We assume that the columns of \\(X\\) have zero mean. We will use symbols like \\(\\phi_1\\) to represent a \\(p\\)-vector, and symbols like \\(z_1\\) to represent an \\(n\\)-vector. We will think of vectors always as column vectors and use the superscript \\(T\\) to denote the corresponding row vectors, e.g. \\(\\phi_1^T\\). The first formulation of the PCA problem is in the following optimization problem: we want to maximize \\(\\phi^T X^T X \\phi= ||X\\phi||^2\\) by varying \\(\\phi\\) among all unit \\(p\\)-vectors. The vector \\(z_1 = X\\phi_1\\) is the linear combination of our features with the highest variance (for this interpretation, it was important to set the columns of \\(X\\) to have zero mean). The matrix \\(X^T X\\) is symmetric and positive-semi-definite, so it can be diagonalized and all its eigenvalues are non-negative. Let us assume that \\(X^T X\\) has only positive eigenvalues, since a zero eigenvalue would mean that some linear combination of the columns of \\(X\\) is zero, and therefore that one of the features encoded in \\(X\\) is redundant. Let \\(y_1,\\ldots, y_p\\) be the eigenvectors of \\(X^TX\\), and \\(\\lambda_1,\\ldots, \\lambda_p\\) be the corresponding eigenvalues, arranged in increasing order. Then, if we write \\[\\phi = c_1y_1+\\cdots+c_py_p,\\] we have \\[||\\phi||^2 = \\sum_i c_i^2\\] and \\[||X\\phi||^2 = \\sum_i \\lambda_i c_i^2.\\] This is a constrained optimization problem (maximize \\(\\sum_i c_i^2\\lambda_i\\) subject to \\(\\sum_i c_i^2=1\\)) and it can be solved by Lagrange multipliers. The maximum value for the objective function is \\(\\lambda_p\\) and is obtained when \\(c_p=1\\) and the other \\(c\\)s are zero. Then, the first principal component loading vector \\(\\phi_1 =y_p\\) is the eigenvector of \\(X^TX\\) corresponding to the highest eigenvalue, and the corresponding principal component is \\(z_1 = X\\phi_1\\). Next, we seek \\(z_2 = X\\phi_2\\) such that \\(z_2^T z_2\\) is maximal among \\(z_2\\) such that \\(z_2^T z_1=0\\). Since \\(z_1 = Xy_p\\) and \\(z_2 = X\\phi_2\\), we have \\[z_2^T z_1 = \\phi_2^T X^TX y_p = \\lambda_p \\phi_2^T y_p=\\lambda_p \\phi_2^T\\phi_1.\\] So, because \\(\\lambda_p\\neq 0\\), \\(z_1\\) and \\(z_2\\) are orthogonal only if \\(\\phi_1\\) and \\(\\phi_2\\) are orthogonal. So, finding \\(\\phi_2\\) amounts to finding the next highest eigenvalue \\(\\lambda_{p-1}\\) and the corresponding eigenvector \\(y_{p-1}\\) (the orthogonal complement to \\(\\phi_1\\) is spanned by \\(y_1,\\ldots, y_{p-1}\\), so we simply repeat the argument that got \\(y_1\\) on this smaller space). And so on, till we’ve found the \\(m\\) largest eigenvalues of \\(X^TX\\) and have decided to stop. To recap: in PCA, we\n\nFind the linear combination \\(z_1=X\\phi_1\\) of the columns of \\(X\\) which has maximal variance. This corresponds to picking the eigenvector of \\(X^TX\\) with maximal eigenvalue.\nAmong the linear combinations \\(z_2 = X\\phi_2\\) of the columns of \\(X\\) wich \\(z_2\\) orthogonal to \\(z_1\\), we choose the one which has maximal variance. This amounts to picking out the second-largest eigenvalue of \\(X^TX\\).\nAnd so on… We pick out the \\(m\\) largest eigenvalues of \\(X^TX\\), their corresponding eigenvectors, and their corresponding principal components.\n\nThe book claims but doesn’t show that it this procedure is equivalent to considering the following optimization problem instead: minimize \\(\\mathrm{tr}((X-AB)^T(X-AB))\\) as a\\(A\\) ranges over the space of \\(n\\times m\\) matrices and \\(B\\) ranges over the space of \\(m \\times p\\) matrices. Let’s show this. First, let’s note that we can write \\(AB = \\sum_{j=1}^m z_i \\phi_i^T\\), where the \\(z_i\\) are \\(n\\) vectors and the \\(\\phi_i\\) are \\(p\\)-vectors (unrelated so far to the ones found from the first formulation of PCA). We may assume that the \\(\\phi_i\\) are linearly independent (for if we can write one \\(\\phi\\) as a linear combination of the others, we can manifest this as a redefinition of the \\(z\\)’s). For a similar reason, we may assume that the \\(\\phi_i\\) are orthonormal. Under these assumptions, we may write \\[\n\\begin{aligned}\n\\mathrm{tr}((X-AB)^T(X-AB))&= \\mathrm{tr}(X^TX)-2\\sum_{i} \\mathrm{tr}(X^T z_i \\phi_i^T)+\\sum_{i,j}\\mathrm{tr}(\\phi_i z_i^T z_j \\phi_j^T)\\\\\n&= \\mathrm{tr}(X^TX)- 2\\sum_{i} \\phi_i^T X^T z_i + \\sum_{i} z_i^T z_j,\n\\end{aligned}\n\\] where we have used that \\[\n\\mathrm{tr}(\\phi' \\phi^T) = ||\\phi||^2 \\phi \\cdot \\phi'\n\\] for any two \\(p\\)-vectors \\(\\phi\\), \\(\\phi'\\). So, we are trying to find \\(z_i\\) and \\(\\phi_i\\) to minimize the above trace, subject to the condition that \\(\\phi_i \\cdot \\phi_j = \\delta_{i,j}\\) (i.e. the \\(\\phi\\)’s are orthonormal). There is no constraint on the \\(z\\)’s, and the simple optimization problem for the \\(z\\)’s gives \\[z_i = X\\phi_i\\]. The optimization problem with respect to the \\(y_i\\) is a bit more subtle because the \\(\\phi\\)’s are constrained. But the Lagrange optimization problem tells us that \\[\nX^Tz_i =X^TX\\phi_i = \\sum_j \\beta_j \\phi_j,\n\\] where the \\(\\beta_i\\) are undetermined Lagrange multipliers. But this tells us that \\(X^TX\\) preserves the space spanned by the \\(\\phi_i\\). Let’s call this space \\(W\\). Taking all this into account, the objective function becomes \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX).\n\\] To compute the second trace in the above equation, we just need to know the eigenvalues of \\(X^TX\\) when restricted to \\(W\\). Let us suppose that the eigenvalues of \\(X^TX\\) on \\(W\\) are \\(\\lambda_{i_1},\\ldots, \\lambda_{i_m}\\). Let \\(\\lambda_{i_{m+1}},\\ldots, \\lambda_{i_{p}}\\) be the remaining eigenvalues. Then, \\[\n\\mathrm{tr}_{\\mathbb{R}^p}(X^TX)-\\mathrm{tr}_{W}(X^TX)=\\sum_{j=m+1}^p\\lambda_{i_j}.\n\\] So the objective will be minimized precisely by choosing the \\(m\\) largest eigenvalues of \\(X^TX\\), just as in the first formulation of PCA! It follows that to minimze \\[\\mathrm{tr}((X-AB)^T(X-AB)),\\] we choose the \\(m\\) largest eigenvalues of \\(X^TX\\) (which above we called \\(\\lambda_p, \\lambda_{p-1},\\ldots, \\lambda_{p-m+1}\\), with respective eigenvectors \\(y_p,\\ldots, y_{p-m+1}\\)), set \\(\\phi_i = y_{p-i+1}\\), \\(z_i = Xy_{p-i+1}\\), and \\(AB = \\sum_{i=1}^m z_i \\phi_i^T\\), as desired."
  }
]