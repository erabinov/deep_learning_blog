<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-01-18">

<title>Eugene’s Deep Learning Journey - Constructing a Neural-Net Digit Classifier (Almost) from Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Eugene’s Deep Learning Journey</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/erabinov"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Constructing a Neural-Net Digit Classifier (Almost) from Scratch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 18, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction-and-summary" class="level1">
<h1>Introduction and Summary</h1>
<p>In this post, I am basically adapting Chapter 4 of the fast.ai book to extend the 3/7 classifier into a digit classifier for all digits. Just as in the book, I’m going to use the MNIST data set. The biggest struggles in constructing this classifier were in deciding the loss function in the multi-class case and finding a good learning rate. What ate up the most time in this project, though, was working with pyTorch tensors. I would frequently have to change tensors of shape (1,10) to tensors of shape (10) and vice versa. This was a headache; perhaps it would behoove me to learn a bit more about pyTorch’s defaults and conversions. I’ve ordered a book to help me with that.</p>
<p>It actually turned out that reading Chapter 5 of the book gave me a lot of the tools I needed to finish this little project. Before I read the chapter, I was on the right track by computing softmax activations and the likelihoods for each individual observation, but instead of computing the overall likelihood for the loss function, I took the mean likelihood across all observations, which isn’t as natural a quantity to compute. Finally, I was missing the step of taking the log, which doesn’t change the mathematical structure of the optimization problem, but I think it has numerical consequences.</p>
<section id="aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="aside-what-is-the-cross-entropynegative-log-likelihood-what-is-the-learning-rate">Aside: What is the cross-entropy/negative-log-likelihood? What is the learning rate?</h2>
<p>Let’s describe what happens in machine learning. We are given a collection of inputs and outputs <span class="math inline">\(\{x_i,y_i\}\)</span> that are supposed to be related to each other in some way. For example, each <span class="math inline">\(x_i\)</span> could be the data of an image (the RGB coordinates of each of its pixels) and <span class="math inline">\(y_i\)</span> could be the digit that the image is supposed to represent. There is supposed to be an abstract relationship between the <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> and we have a particular noisy sample of such inputs and outputs. Our goal is to construct a model <span class="math inline">\(M\)</span>, whose job is to take in an input <span class="math inline">\(x\)</span> and and “spit out” an output <span class="math inline">\(M(x)\)</span>. Now, we better hope that <span class="math inline">\(M(x_i)\)</span> is (in some way) as close to <span class="math inline">\(y_i\)</span> as possible, since the model needs to be able to predict the actual data we fed into it (but also we want to prevent the model from overgeneralizing features of the particular data set we have fed it!). An important thing that I’ve neglected to mention is that <span class="math inline">\(M\)</span> itself is usually part of a family of models, each parametrized by a set of weights, for which I will use the single letter <span class="math inline">\(w\)</span>. So, more properly, we have a collection of predictions <span class="math inline">\(M(w,x)\)</span>, one for each value of the parameters <span class="math inline">\(w\)</span> (we consider <span class="math inline">\(x\)</span> to be fixed for now). I’ve learned that the name for the family of models is called the <em>architecture</em> of the model. A large fraction of machine learning consists of choosing an appropriate architecture for your problem: you want an architecture flexible enough to find the relationships between inputs and outputs, but not one that is so flexible as to find spurious relationships particular only to the data set you train your model on.</p>
<p>Ok, so how do we describe which value of <span class="math inline">\(w\)</span> is “best”? We need to measure “how far off” <span class="math inline">\(M(w,x_i)\)</span> is from <span class="math inline">\(y_i\)</span>, and aggregate all this information somehow. One way to do this is to just compute the average accuracy of the predictions <span class="math inline">\(M(w,x_i)\)</span>. The problem is the following: if <span class="math inline">\(w\)</span> and <span class="math inline">\(x_i\)</span> are continuous (numerical) data types, and our output <span class="math inline">\(y_i\)</span> is a discrete data type (which it is in classification problems like the one we’re considering), and— finally—<span class="math inline">\(M(w,x)\)</span> is continuous (or nearly so) with respect to its parameters, then it’s “generically” impossible to improve the accuracy of the predictions just by tweaking the weights <span class="math inline">\(w\)</span> a little bit. But this is pretty much the main way that we can have a computer optimize things: by slightly tweaking the parameters and seeing which small change of parameters improves our desired metric the most. (The name for this tweaking process is gradient descent.)</p>
<p>So, instead of just predicting the class that each <span class="math inline">\(x_i\)</span> belongs to, we can ask our model to also assign probabilities to those class predictions. So, suppose that we have <span class="math inline">\(N\)</span> different classes (in other words that each <span class="math inline">\(y_i\)</span> is whole number between 1 and <span class="math inline">\(N\)</span>), and we provide <span class="math inline">\(N\)</span> functions <span class="math inline">\(P_1(w,x),\ldots, P_j(w,x),\ldots, P_N(w,x)\)</span> which represent the probabilities that the model corresponding to <span class="math inline">\(w\)</span> assigns to a given input <span class="math inline">\(x\)</span> producing each of the <span class="math inline">\(N\)</span> possible outputs. Since these are probabilities, we need to have <span class="math display">\[ P_1(w,x)+P_2(w,x)+\cdots+P_N(w,x)=1\]</span> and each <span class="math inline">\(P_j\)</span> needs to be non-negative for all <span class="math inline">\(w\)</span> and <span class="math inline">\(x\)</span>. (In math, we say that the functions <span class="math inline">\(\{P_j\}\)</span> provides a function from the space <span class="math inline">\(S\)</span> of parameters and input variables to the <span class="math inline">\((N-1)\)</span>-simplex <span class="math inline">\(\Delta^{N-1}\)</span>) Finally, we can simply set <span class="math inline">\(M(w,x)= \max_{j}P_j(w,x)\)</span>. The beauty of this approach is that now we can more readily measure how good the model <span class="math inline">\(M\)</span> is, and this is called the <em>likelihood</em> function. What it does is tell us how likely our model says the observed data set is. Assuming that each data point is an independent probabilistic event, we simply multiply the likelihood associated in our model that the output is <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span>, i.e., we form <span class="math display">\[L(w)= \prod_{i} P_{y_i}(w,x_i).\]</span> <span class="math inline">\(L(w)\)</span> (conceived of as a function of the parameters <span class="math inline">\(w\)</span>) is something that we can seek to optimize, since if the <span class="math inline">\(P\)</span>’s have reasonable behavior (e.g.&nbsp;smoothness or continuity), so too will <span class="math inline">\(L(w)\)</span>. In practice, we optimize <span class="math inline">\(\log(L(w))\)</span> instead. As far as I understand it, this is because since all the probabilities <span class="math inline">\(P_{y_i}\)</span> are less than 1, and there may be thousands that we multiply together, the likelihood will be a very small number, and so it will be hard to detect (given finite precision) improvements in the likelihood.</p>
<p>Now, usually, we have many smart ways of producing a collection of <span class="math inline">\(N\)</span> functions of the parameters and inputs, but we still need to make sure that they are all non-negative and sum to 1. That’s the purpose of the softmax function, which is a map <span class="math inline">\(\mathbb{R}^N \to \Delta^{N-1}\)</span> (it’s essentially the simplest such function). I won’t get into the details of that here.</p>
<p>Ok, so now we have a task: given this particular form of <span class="math inline">\(L(w)\)</span>, find the <span class="math inline">\(w\)</span> which will minimize <span class="math inline">\(-\log(L(w))\)</span> (which we now call the <em>loss function</em>), or at least give us a resonable approximation to the minimum. This amounts to following a path in the parameter space whose tangent vector is the opposite vector of the gradient. The way this is typically done is to randomly choose some starting weights <span class="math inline">\(w_0\)</span> and then replace <span class="math inline">\(w_0\)</span> with <span class="math inline">\(w_0 + \eta \nabla_{w}\log(L(w_0))\)</span>, where <span class="math inline">\(\eta\)</span> is some small “step size” or “learning rate”. Then we iterate the process until we are reasonably convinced we’re close to a minimum. Only in the limit <span class="math inline">\(\eta\to 0\)</span> is this completely accurate as way to find the minima. So the smaller <span class="math inline">\(\eta\)</span> is, the more likely we will be to find the minimum of the loss function. But since <span class="math inline">\(\eta\)</span> is small, if we start with <span class="math inline">\(w_0\)</span> far from the actual minimum, our hair might grow very long while we wait for the iterative process of updating the parameters to bring about meaningful reductions of the loss. So in practice we have to tweak <span class="math inline">\(\eta\)</span> to give reasonable enough results subject to our time/resource constraints. I messed around a bit with learning rates and found that .1 was a sufficiently middle-ground learning rate.</p>
<p>Finally, I want to mention that in practice, instead of doing full gradient descent, we take advantage of the particular structure of the loss function to do something more computationally feasible. Because the likelihood is the product of contributions from each separate data point <span class="math inline">\((x_i,y_i)\)</span>, the loss function is a sum of such contributions. In each update to the parameters, we can replace the full loss function with the corresponding sum of contributions from a random subset of the full data set. This is called <em>stochastic gradient descent</em>, and I think it makes the gradient descent process more computationally feasible. And I think the idea is also that it allows us to quickly identify which parameters have the greatest effect on the loss without wasting the resources to compute the full loss.</p>
</section>
</section>
<section id="unpacking-the-data" class="level1">
<h1>Unpacking the Data</h1>
<p>We untar the MNIST data set from the fastai library, and name the resulting Path object <code>path</code>.</p>
<div class="cell" data-outputid="ee551574-ca2e-40c7-9808-3b82da1fb6b1" data-execution_count="59">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.MNIST)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that the directory consists of a “testing” and “training” folder.</p>
<div class="cell" data-outputid="8cf40ed7-0aff-4d47-bfb1-910861599f4f" data-execution_count="61">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>path.ls()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>(#2) [Path('training'),Path('testing')]</code></pre>
</div>
</div>
<p>Within the training folder, there is one folder for each digit.</p>
<div class="cell" data-outputid="dcf47a60-2c0b-4fd8-8d8f-44770c1d7f5a" data-execution_count="62">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>(path<span class="op">/</span><span class="st">'training'</span>).ls()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>(#10) [Path('training/9'),Path('training/0'),Path('training/7'),Path('training/6'),Path('training/1'),Path('training/8'),Path('training/4'),Path('training/3'),Path('training/2'),Path('training/5')]</code></pre>
</div>
</div>
<p>The list <code>nums</code> has ten entries. Each entry is the list of all the file names corresponding to the respective digit.</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>nums <span class="op">=</span> [(path<span class="op">/</span><span class="st">'training'</span><span class="op">/</span><span class="bu">str</span>(i)).ls() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="l1-distance-to-the-mean-digits" class="level1">
<h1>L1 Distance to the Mean Digits</h1>
<p>In this section, we compute the mean image for each digit, and then assess the accuracy of the model which assigns to each image the digit whose mean image is closest in L1 norm to the given image.</p>
<p>We form a two-dimensional array of Tensors; the first dimension corresponds to a digit, and the second dimension identifies a particular image.</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>num_tensors <span class="op">=</span> [[tensor(Image.<span class="bu">open</span>(o)) <span class="cf">for</span> o <span class="kw">in</span> nums[i]] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="56a1ae64-5c2a-4874-977f-f3899cec923b" data-execution_count="65">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(num_tensors), <span class="bu">len</span>(num_tensors[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>(10, 5958)</code></pre>
</div>
</div>
<p>Instead of having a list of tensors for each digit, we stack all the tensors for a given digit into a single Tensor.</p>
<div class="cell" data-outputid="9e966624-5a74-4aab-d90e-332498f3d402" data-execution_count="66">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>stacks <span class="op">=</span> [torch.stack(num_tensors[i]).<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>stacks[<span class="dv">2</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>torch.Size([5958, 28, 28])</code></pre>
</div>
</div>
<p>We compute and display the mean images. It’s kind of cool to see the fuzzy gray signifying different slants to vertical components of digits, say in 1, 4, and 9.</p>
<div class="cell" data-outputid="5c1c2fbc-b2ba-4558-cefd-64e94c131c68" data-execution_count="67">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [stacks[i].<span class="bu">float</span>().mean(<span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>[show_image(o) <span class="cf">for</span> o <span class="kw">in</span> means]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>[&lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;,
 &lt;AxesSubplot: &gt;]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-11.png" class="img-fluid"></p>
</div>
</div>
<p>Now we test the accuracy of the distance-to-mean model. First, we unpack the validation set data in the same way as we did for the training data.</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#An ordinary Python list of pyTorch tensors, with the validation images for digit i in the ith index.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>valids <span class="op">=</span> [torch.stack([tensor(Image.<span class="bu">open</span>(o)) </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> o <span class="kw">in</span> (path<span class="op">/</span><span class="st">'testing'</span><span class="op">/</span><span class="bu">str</span>(i)).ls()]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>valids <span class="op">=</span> [o.<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span> <span class="cf">for</span> o <span class="kw">in</span> valids]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This just measures the L1 distance between two tensors.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_distance(a,b): <span class="cf">return</span> (a<span class="op">-</span>b).<span class="bu">abs</span>().mean((<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The funciton <code>mnist_distance_list</code> computes the distance of its input from each of the ten mean digits. The function <code>which_is_it</code> returns the digit with the least distance to its input.</p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_distance_list(x):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  dists <span class="op">=</span> [mnist_distance(x,means[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> torch.stack(dists)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> which_is_it(x):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> torch.argmin(mnist_distance_list(x),dim<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test it on an example case:</p>
<div class="cell" data-outputid="ea90f834-8889-41dd-d67b-f1faf02d2d70" data-execution_count="71">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>a_3 <span class="op">=</span> stacks[<span class="dv">3</span>][<span class="dv">2007</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>mnist_distance_list(a_3), which_is_it(a_3), show_image(a_3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>(tensor([0.1919, 0.1139, 0.1667, 0.1233, 0.1551, 0.1351, 0.1632, 0.1476, 0.1493, 0.1453]),
 tensor(1),
 &lt;AxesSubplot: &gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The classifier incorrectly classifies this 3 as a 1, although 3 is the second guess of our classifier.</p>
<p>The following couple of cells are basically to test to make sure that <code>which_is_it</code> broadcasts correctly with respect to how our validation data are organized.</p>
<div class="cell" data-outputid="92287bda-13fc-40ba-cca1-a3896f5437d1" data-execution_count="72">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>small_test <span class="op">=</span> stacks[<span class="dv">3</span>][<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>small_test.shape</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>mnist_distance_list(small_test[<span class="dv">0</span>]), mnist_distance_list(small_test[<span class="dv">1</span>]),mnist_distance_list(small_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>(tensor([0.1783, 0.1153, 0.1520, 0.1144, 0.1579, 0.1366, 0.1683, 0.1565, 0.1416, 0.1568]),
 tensor([0.1692, 0.1246, 0.1621, 0.1234, 0.1649, 0.1344, 0.1612, 0.1602, 0.1557, 0.1616]),
 tensor([[0.1783, 0.1692],
         [0.1153, 0.1246],
         [0.1520, 0.1621],
         [0.1144, 0.1234],
         [0.1579, 0.1649],
         [0.1366, 0.1344],
         [0.1683, 0.1612],
         [0.1565, 0.1602],
         [0.1416, 0.1557],
         [0.1568, 0.1616]]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="693e566e-cb4e-4b2a-e1dd-0e1329b5085d" data-execution_count="73">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>which_is_it(small_test[<span class="dv">0</span>]),which_is_it(small_test[<span class="dv">1</span>]), which_is_it(small_test), which_is_it(small_test)<span class="op">==</span><span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>(tensor(3), tensor(3), tensor([3, 3]), tensor([True, True]))</code></pre>
</div>
</div>
<p>We see that the classifier correctly identifies both of the images as 3s, and <code>which_is_it</code>, when given a list of images, produces a rank 1 tensor whose entries are <code>which_is_it</code> applied elementwise.</p>
<p>Here, we compute the digit-by-digit accuracies, and then the overall accuracy, which is 67%, which will be our baseline for comparison.</p>
<div class="cell" data-outputid="4d3bca3d-28a4-4caf-ecc9-02102acb8c8b" data-execution_count="74">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> [(which_is_it(valids[i])<span class="op">==</span>i).<span class="bu">float</span>().mean() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>overall_accuracy <span class="op">=</span> tensor([accuracies[i]<span class="op">*</span><span class="bu">len</span>(valids[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)])</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>overall_accuracy <span class="op">=</span> overall_accuracy.<span class="bu">sum</span>()<span class="op">/</span><span class="fl">10000.</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>accuracies, overall_accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>([tensor(0.8153),
  tensor(0.9982),
  tensor(0.4234),
  tensor(0.6089),
  tensor(0.6680),
  tensor(0.3262),
  tensor(0.7871),
  tensor(0.7646),
  tensor(0.4425),
  tensor(0.7760)],
 tensor(0.6685))</code></pre>
</div>
</div>
</section>
<section id="the-linear-model" class="level1">
<h1>The Linear Model</h1>
<p>Here, we put all of the images into single vectors, instead of 28-by-28 arrays. This is for ease of manipulation of the data.</p>
<div class="cell" data-outputid="faedaec1-9d48-4916-fdf5-d25148afcdfb" data-execution_count="75">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> torch.cat([stacks[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>train_x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>torch.Size([60000, 784])</code></pre>
</div>
</div>
<p>We put together a list of classifications for the training data.</p>
<div class="cell" data-outputid="c8b98708-7585-4786-a127-3967622f212c" data-execution_count="76">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> tensor([])</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span> (<span class="dv">10</span>):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  add <span class="op">=</span> torch.stack([tensor([i]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nums[i]))])</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  train_y<span class="op">=</span>torch.cat((train_y,add),<span class="dv">0</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>train_y<span class="op">=</span> train_y.squeeze()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>train_x.shape, train_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>(torch.Size([60000, 784]), torch.Size([60000]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="27d3e5e2-8a89-4429-f607-1edd1c38b384" data-execution_count="77">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x,train_y))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>x,y <span class="op">=</span> dset[<span class="dv">0</span>]</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>x.shape,y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>(torch.Size([784]), tensor(0.))</code></pre>
</div>
</div>
<p>We do the same for the validation data:</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>valid_x <span class="op">=</span> torch.cat([valids[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>valid_y <span class="op">=</span> tensor([])</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  add<span class="op">=</span>torch.stack([tensor([i]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(valids[i]))])</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  valid_y <span class="op">=</span>torch.cat((valid_y,add),<span class="dv">0</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>valid_y <span class="op">=</span> valid_y.squeeze()</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(valid_x,valid_y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="32709116-8f3c-401b-e7a4-32a6a06bbefb" data-execution_count="79">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>valid_x.shape, valid_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>(torch.Size([10000, 784]), torch.Size([10000]))</code></pre>
</div>
</div>
<p>We define a function to randomly initialize parameters:</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">1.0</span>): </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  ungrad <span class="op">=</span> (torch.randn(size)<span class="op">*</span>std).squeeze()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> ungrad.requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="cae09f80-66b0-46f3-fcb4-e7b272cc0269" data-execution_count="81">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">10</span>))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>weights.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>torch.Size([784, 10])</code></pre>
</div>
</div>
<div class="cell" data-outputid="f9b40872-8289-4cb8-f06c-308194e962d2" data-execution_count="82">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> init_params((<span class="dv">1</span>,<span class="dv">10</span>))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>bias.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>torch.Size([10])</code></pre>
</div>
</div>
<p>A check to make sure we’re sizing the outputs of <code>init_parameters</code> correctly.</p>
<div class="cell" data-outputid="dd724af3-8caf-4f32-c009-102c77270546" data-execution_count="83">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>((train_x.<span class="bu">float</span>()<span class="op">@</span>(weights)) <span class="op">+</span> bias).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>torch.Size([60000, 10])</code></pre>
</div>
</div>
<p>The function <code>linear1</code> packages <code>weights</code> and <code>bias</code> together:</p>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear1(xb): <span class="cf">return</span> xb.<span class="bu">float</span>()<span class="op">@</span>weights <span class="op">+</span> bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We create a batch to make sure our functions work as expected.</p>
<div class="cell" data-outputid="71508125-be77-4808-ef58-58c093226c16" data-execution_count="86">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>preds_log_prob <span class="op">=</span> linear1(train_x)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>preds<span class="op">=</span> torch.argmax(preds_log_prob,<span class="dv">1</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> preds.reshape(<span class="dv">60000</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>preds.shape, preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>(torch.Size([60000]), tensor([0, 0, 0,  ..., 8, 8, 0]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="8e5a26d4-e2db-4379-fc1e-d0e8e0a9cdd6" data-execution_count="87">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>corrects <span class="op">=</span> preds.<span class="bu">float</span>()<span class="op">==</span>train_y.squeeze()</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>corrects</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>tensor([ True,  True,  True,  ..., False, False, False])</code></pre>
</div>
</div>
<p>We get a pretty bad accuracy, since our weights were just completely random.</p>
<div class="cell" data-outputid="5f28e688-81e6-4c72-cbc0-d3d4ca4362af" data-execution_count="88">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>corrects.<span class="bu">float</span>().mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>0.1610500067472458</code></pre>
</div>
</div>
<p>Here finally we introduce the loss function by which we have already discussed (the log of likelihood).</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_loss(predictions, targets):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.<span class="bu">long</span>()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> F.cross_entropy(predictions,targets.squeeze())</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sgd-and-mini-batches" class="level3">
<h3 class="anchored" data-anchor-id="sgd-and-mini-batches">SGD and Mini-Batches</h3>
<p>Re-initialize parameters:</p>
<div class="cell" data-execution_count="182">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">10</span>))</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> init_params(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Create a <code>DataLoader</code> from our test data. This is the least “from scratch” part of our undertaking.</p>
<div class="cell" data-outputid="93c207a5-4014-4361-e388-9f2f25a416fc" data-execution_count="91">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> first(dl)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>xb.shape,yb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>(torch.Size([256, 784]), torch.Size([256]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We make a small subset of the training data to make sure the functions are working the right way.</p>
<div class="cell" data-outputid="fd511772-f0c9-4f73-d67d-68d05c5c6031" data-execution_count="93">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> train_x[:<span class="dv">4</span>]</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>batch.shape, train_y[:<span class="dv">4</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>(torch.Size([4, 784]), torch.Size([4]))</code></pre>
</div>
</div>
<p>We can read off from <code>preds</code> that the model predicts digits (0,7,0,9) for the four images in batch. This will explain the .5 accuracy that we compute below.</p>
<div class="cell" data-outputid="9da2dfbb-b39f-4962-97fd-cff6a713876f" data-execution_count="94">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> linear1(batch)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>preds, train_y[:<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>(tensor([[ 15.1205, -17.4561,   1.0612,  -7.9669,  -7.6242,   3.3798, -23.8781, -13.1161,   7.8836,   4.6135],
         [  2.9981,  -8.6182, -11.9235, -10.0922,   0.3654,   1.5978,  -9.6981,  13.6491,   0.2252,  -2.6899],
         [ 18.6683, -15.5988,  -0.7190,  -9.8376,  -9.9725,   1.7350, -20.1133, -12.2211,  15.1338,   3.8342],
         [  8.4809,  -5.4655,   5.2217,   3.3916,  -4.6596,   5.9768, -20.8959, -12.0383,  -2.2892,  19.1940]], grad_fn=&lt;AddBackward0&gt;),
 tensor([0., 0., 0., 0.]))</code></pre>
</div>
</div>
<p>The following few cells contain almost no changes from the corresponding ones in the book.</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_grad(xb, yb, model):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mnist_loss(preds, yb)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, lr, params):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb,yb <span class="kw">in</span> dl:</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> params:</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">-=</span> p.grad<span class="op">*</span>lr</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>            p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_accuracy(xb, yb):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> torch.argmax(xb,dim<span class="op">=</span><span class="dv">1</span>).squeeze()</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> preds.<span class="bu">float</span>() <span class="op">==</span> yb.<span class="bu">float</span>().squeeze()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can check it works (recall that we computed a 50% accuracy earlier):</p>
<div class="cell" data-outputid="ee04bec7-d92d-4b6a-f2bd-40823bbbb724" data-execution_count="101">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>batch_accuracy(linear1(batch), train_y[:<span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>tensor(0.5000)</code></pre>
</div>
</div>
<p>and then put the batches together:</p>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate_epoch(model):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    accs <span class="op">=</span> [batch_accuracy(model(xb), yb) <span class="cf">for</span> xb,yb <span class="kw">in</span> valid_dl]</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(accs).mean().item(), <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="d54f700c-2548-4de8-8bae-fda143ab6ab1" data-execution_count="103">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>validate_epoch(linear1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre><code>0.189</code></pre>
</div>
</div>
<p>After some experimentation, I found that it was useful to train for a bit at a learning rate of .1 to give the model a chance to search the parameter space relatively widely, and then to train for many more epochs at .01.</p>
<div class="cell" data-outputid="0904bfc2-36dc-4eb4-efa4-34ea958762ef" data-execution_count="183">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> weights,bias</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>train_epoch(linear1, lr, params)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>validate_epoch(linear1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="183">
<pre><code>0.1512</code></pre>
</div>
</div>
<div class="cell" data-outputid="00e17cd5-1541-47aa-dc77-5fe540e07103" data-execution_count="184">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">120</span>):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    train_epoch(linear1, lr, params)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">15</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(validate_epoch(linear1), end<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.6648
0.716
0.7333
0.7397
0.744
0.7475
0.7491
0.7516</code></pre>
</div>
</div>
<div class="cell" data-execution_count="185">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">210</span>):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    train_epoch(linear1, lr<span class="op">=</span><span class="fl">.01</span>, params<span class="op">=</span>params)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">15</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(validate_epoch(linear1), end<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8886
0.8926
0.8951
0.8958
0.8965
0.8979
0.8981
0.8989
0.8995
0.8998
0.9003
0.9005
0.9009
0.9009</code></pre>
</div>
</div>
<p>Not bad: 90%</p>
</section>
</section>
<section id="a-simple-neural-net" class="level1">
<h1>A Simple Neural Net</h1>
<p>Now we turn to the simple neural-net model. It has a linear layer, followed by a RELU layer, followed by a linear layer. The model is almost identical to the original one, except that we need 10 ouputs from the final layer to give our ten probabilities for the ten digits. As for the linear model, it made sense to train for a bit with a higher learning rate to get “in the general ballpark”, and then to tamp down the learning rate for the fine-tuning. There is a substantial improvement in the accuracy when we switch to the lower learning rate, and then steady but small gains in each epoch. Notice also that we train for many more epochs for the neural net because there are many more parameters to tune.</p>
<div class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_net(xb): </span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1 <span class="op">+</span> b1</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.0</span>))</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>w2 <span class="op">+</span> b2</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="186">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">30</span>))</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">30</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">30</span>,<span class="dv">10</span>))</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="187">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">420</span>):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  train_epoch(simple_net, lr<span class="op">=</span><span class="fl">.1</span>, params<span class="op">=</span>(w1,b1,w2,b2))</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">15</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(validate_epoch(simple_net), end<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.3006
0.3268
0.3543
0.3865
0.4399
0.5045
0.5327
0.5451
0.5581
0.5689
0.5795
0.5942
0.6057
0.6154
0.6221
0.6309
0.6379
0.6435
0.6496
0.6565
0.6628
0.6702
0.6772
0.6831
0.6895
0.6943
0.6983
0.7035</code></pre>
</div>
</div>
<p>Let’s set it to train for 780 epochs, go have lunch, and then come back:</p>
<div class="cell" data-execution_count="188">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">780</span>):</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>  train_epoch(simple_net, lr<span class="op">=</span><span class="fl">.01</span>, params<span class="op">=</span>(w1,b1,w2,b2))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">15</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(validate_epoch(simple_net), end<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9082
0.9089
0.9093
0.9096
0.9099
0.9103
0.9103
0.9106
0.9114
0.912
0.9125
0.9124
0.9129
0.9132
0.9134
0.9141
0.9142
0.9145
0.9147
0.9147
0.9154
0.9156
0.9159
0.9161
0.9165
0.9169
0.9174
0.9177
0.9183
0.9186
0.9187
0.9193
0.9196
0.92
0.9201
0.9206
0.921
0.9216
0.9219
0.922
0.922
0.9224
0.9228
0.9226
0.9229
0.9249
0.9249
0.9254
0.9256
0.9254
0.9258
0.9263</code></pre>
</div>
</div>
<p>Our final accuracy with the simple neural-net is pretty good: 93% almost!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>