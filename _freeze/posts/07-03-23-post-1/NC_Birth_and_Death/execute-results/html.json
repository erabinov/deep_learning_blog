{
  "hash": "be3937909d6bce0e4e91f4706df5dc0f",
  "result": {
    "markdown": "---\ntitle: \"Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016\"\ndate: \"2023-03-07\"\nformat:\n  html:\n    html-math-method: mathjax\n---\n\n\n\n\nLoading the Data and Packages\n\nI'm using Google Maps data here.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(urbnmapr)\nlibrary(dplyr)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(mapdata)\nlibrary(glmnet)\n```\n:::\n\n\nSet up the base NC map:\n\n::: {.cell}\n\n```{.r .cell-code}\nditch_the_axes <- theme(\n  axis.text = element_blank(),\n  axis.line = element_blank(),\n  axis.ticks = element_blank(),\n  panel.border = element_blank(),\n  panel.grid = element_blank(),\n  axis.title = element_blank()\n  )\nstates<-map_data(\"state\")\nnc_df<-subset(states,region==\"north carolina\")\nnc_base <- ggplot(data = nc_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.3) + \n  geom_polygon(color = \"black\", fill = \"gray\")\ncounties<-map_data(\"county\")\nnc_county<-subset(counties,region==\"north carolina\")\n```\n:::\n\n\nWe read the data in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw.bd1<-read_csv(\"data/birth_data.csv\")\nraw.bd1$Year<-as.integer(raw.bd1$Year)\nraw.bd2<-read_csv(\"data/birth_data2.csv\")\nbr<-read_csv(\"data/Yr1116Birth.csv\")\ndr<-read_csv(\"data/Yr1116Death.csv\")\ncounty.conversion<-read_csv(\"data/County_Codes_Table_1.csv\")\ncounty.conversion<-county.conversion[,1:3]\nincomes<-read_csv(\"data/Income_by_Location.csv\")\nincomes$Geography<-str_sub(incomes$Geography,1,-12)\nincomes<-rename(incomes,subregion=`Geography`)\n#Resolve YOB vs. YEAR column names.\nbr<-rename(br, YEAR=YOB)\n#Resolve MRACER vs. RACE column names.\nbr<-rename(br, RACE=MRACER)\n```\n:::\n\n\n\nWe massage br so that we can compare the br and dr data, and so that we will be able to make a map later.\n\n::: {.cell}\n\n```{.r .cell-code}\n## The birth and death tables contain different codings for race, so I have to change that smh. I begin by defining a function \"collapse\" which translates the birth data race coding to dezath data race  coding. \ncollapse<-function(x){\n  if(x==1|x==2|x==3)\n    return(x)\n  else\n    return(4)\n}\nbr$RACE<-sapply(br$RACE,collapse)\ncounty.conversion$CORES<-as.double(county.conversion$CORES)\nbr<-left_join(br,county.conversion[,1:2])\nbr$COUNTY<-tolower(br$COUNTY)\nbr<-rename(br, subregion=COUNTY)\nsimple_incomes<-incomes[incomes$`ID Race`==0&incomes$`ID Year`==2016,c(5,7)]\nsimple_incomes$subregion<-tolower(simple_incomes$subregion)\nbr<-left_join(br, simple_incomes)\n```\n:::\n\nIn order to minimze some of the variance associated with the small county data, we form 17 clusters of counties, and we will take in-cluster means of all the data in br.\n\n::: {.cell}\n\n```{.r .cell-code}\ncountycounts<-count(br,CORES)\ncountyracecounts<-count(br,CORES,RACE)\nrace1births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==1,]$n)\n}\nrace2births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==2,]$n)\n}\nrace3births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==3,]$n)\n}\nrace4births<-rep(0,100)\nfor (i in 1:100) {\n  race1births[i]<- sum(countyracecounts[countyracecounts$CORES==i&countyracecounts$RACE==4,]$n)\n}\ncountycounts<-bind_cols(countycounts,race1births,race2births,race3births,race4births)\ncountycounts<-rename(countycounts,NUMBIRTH=n,NUM1=...3,NUM2=...4,NUM3=...5,NUM4=...6)\n\n##We try K-means clustering with about 20 clusters. We try to identify clusters with  We want population to have the largest effect, so we don't normalize the data. The result is probably that race plays a very small role in the size of the clusters.\nset.seed(1)\nkm.out<-kmeans(countycounts[,2:6],17,nstart=50)\nkm.out$cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1]  4  9 10 12 12 11  8 11  9 14 16  2 16 15 10 17 12  4 17 12 11 10 14 17  3\n [26]  6 12  9  4  9  2  1 17  1 15 16 10 10 17 12  6 17  4 17 14 12 14  7  4  9\n [51] 16 10  2 15 15  8  9 12 12 13 11  9 14 14 16 11  1  5 10  8 17 11  9 16 11\n [76]  3 17  4 14  3 15  2  8 15  9 15 12 12  7 16 17 13 11 11  9  4 15 14  9 11\n```\n:::\n\n```{.r .cell-code}\ncountycounts<-bind_cols(countycounts,CLUSTER=km.out$cluster)\n\nCOREStoCLUSTER<-function(x){\n  if(0<=x&x<=100)\n    return(km.out$cluster[x])\n  else\n    return(NA)\n}\nclustervals<-sapply(br$CORES,COREStoCLUSTER)\nbr<-bind_cols(br,CLUSTER=clustervals)\ndrclustervals<-sapply(dr$CORES,COREStoCLUSTER)\ndr<-bind_cols(dr,drclustervals)\ndr<-rename(dr,CLUSTER=...6)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbrdrcounts<-count(br,YEAR,CLUSTER,RACE)\nbrdrcounts<-rename(brdrcounts,NUMBIRTH=n)\n#I noticed, for example, that the original brdrcounts had no entry for (YEAR,CORES,RACE)=(2011,3,3). Based on a superficial check of a few of the missing values, this seems plausible, but I still want to set NUMBIRTH=0 for the missing combinations. Mostly RACE=3 rows are missing, which seems plausible since the Native American population of NC is around 1-2%. \nbrdrcounts<-complete(brdrcounts,YEAR,CLUSTER,RACE)\nbrdrcounts<-replace_na(brdrcounts,list(NUMBIRTH=0))\ndrcounts<-count(dr,YEAR,CLUSTER,RACE)\n#Same sort of completion for death counts. There are even more missing combos, since the death numbers overall in a low-population county can be quite small.\ndrcounts<-complete(drcounts,YEAR,CLUSTER,RACE)\ndrcounts<-replace_na(drcounts,list(n=0))\n\nbrdrcounts<-left_join(brdrcounts,drcounts)\nbrdrcounts<-rename(brdrcounts,NUMDEATH=n)\n\n##I add the group means for each statistic in br to the birth and death counts.\nbrdrcounts<-bind_cols(brdrcounts,CIGPN=rep(0,nrow(brdrcounts)),CIGFN=rep(0,nrow(brdrcounts)),CIGSN=rep(0,nrow(brdrcounts)),CIGLN=rep(0,nrow(brdrcounts)),BWTG=rep(0,nrow(brdrcounts)),GEST=rep(0,nrow(brdrcounts)),PLUR=rep(0,nrow(brdrcounts)), MAGE=rep(0,nrow(brdrcounts)),PARITY=rep(0,nrow(brdrcounts)),INCOME=rep(0,nrow(brdrcounts)))\n\nfor (i in 2011:2016) {\n  for(j in 1:17){\n   for(k in 1:4){\n     subset<-br[br$YEAR==i&br$CLUSTER==j&br$RACE==k,]\n     cigpn<-mean(subset$CIGPN)\n     cigfn<-mean(subset$CIGFN)\n     cigsn<-mean(subset$CIGSN)\n     cigln<-mean(subset$CIGLN)\n     bwtg<-mean(subset$BWTG)\n     gest<-mean(subset$GEST)\n     plur<-mean(subset$PLUR)\n     mage<-mean(subset$MAGE)\n     parity<-mean(subset$PARITY)\n     incomes<-mean(subset$`Household Income by Race`)\n     n<-which(brdrcounts$YEAR==i&brdrcounts$CLUSTER==j&brdrcounts$RACE==k)[[1]]\n     brdrcounts[n,6]<-cigpn\n     brdrcounts[n,7]<-cigfn\n     brdrcounts[n,8]<-cigsn\n     brdrcounts[n,9]<-cigln\n     brdrcounts[n,10]<-bwtg\n     brdrcounts[n,11]<-gest\n     brdrcounts[n,12]<-plur\n     brdrcounts[n,13]<-mage\n     brdrcounts[n,14]<-parity\n     brdrcounts[n,15]<-incomes\n     } \n  }\n}\nbrdrcounts<-replace_na(brdrcounts,list(CIGPN=0,CIGFN=0,CIGSN=0,CIGLN=0,BWTG=0,GEST=0,PLUR=0,MAGE=0,PARITY=0,INCOME=0))\nimrates<-brdrcounts$NUMDEATH/brdrcounts$NUMBIRTH\nbrdrcounts<-bind_cols(brdrcounts,IMR=imrates)\n## We chose not to replace NaN IMR with 0.\n```\n:::\n\n\nThese are the indices in brdrcounts of the CLUSTER/RACE/YEAR combos which have no births:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich(brdrcounts$NUMBIRTH==0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  27  95 163 231 299 367\n```\n:::\n:::\n\n\nI'll take that, as better than having many.\n\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#brdrcounts$YEAR<-as.factor(brdrcounts$YEAR)\nbrdrcounts$RACE<-as.factor(brdrcounts$RACE)\nbrdrcounts$CLUSTER<-as.factor(brdrcounts$CLUSTER)\ntrain.data<-brdrcounts[brdrcounts$YEAR!=2016,]\ntest.data<-brdrcounts[brdrcounts$YEAR==2016,]\nfoldsnums<-sample(1:10,nrow(train.data),replace=TRUE)\nfits<-list(length=10)\nfor(i in 1:10){\n  foldslogical<-foldsnums==i\n  fits[[i]]<- glm(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,subset=foldslogical,na.action = na.exclude)\n}\nlibrary(leaps)\nregfit<-regsubsets(IMR~.-NUMBIRTH-NUMDEATH,data=train.data,nvmax=35)\nval.errors<-rep(NA,30)\ntest.mat<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,data=test.data)\nfor (i in 1:30) {\n  coefi<-coef(regfit,id=i)\n  pred<-test.mat[,names(coefi)]%*% coefi\n  val.errors[i]<-mean((na.omit(test.data$IMR)-pred)^2)\n}\n##logreg.bycty<-glm(SURVIVED~CORES+YEAR+RACE,data=br,subset=train,family=\"binomial\")\n```\n:::\n\nThe above is a validation-set approach to variable selection. I find that the best model predicts infant-mortality rates taking into account only whether or not the group is African American:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.min(val.errors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\ncoef(regfit,which.min(val.errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       RACE2 \n0.005308092 0.009012201 \n```\n:::\n\n```{.r .cell-code}\nsubset.error<-val.errors[which.min(val.errors)]\nsubset.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002065257\n```\n:::\n:::\n\n\nLet's try lasso regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,train.data)[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncoef(cv.lasso,cv.lasso$lambda.min)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  1.732326e-01\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     8.439871e-04\nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11   -7.764165e-04\nCLUSTER12    1.146498e-03\nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        4.158737e-03\nRACE3        .           \nRACE4       -2.297542e-03\nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -4.663241e-06\nGEST        -3.926446e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n```\n:::\n\n```{.r .cell-code}\ncoef(cv.lasso,cv.lasso$lambda.1se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  6.808897e-02\nYEAR         .           \nCLUSTER2     .           \nCLUSTER3     .           \nCLUSTER4     .           \nCLUSTER5     .           \nCLUSTER6     .           \nCLUSTER7     .           \nCLUSTER8     .           \nCLUSTER9     .           \nCLUSTER10    .           \nCLUSTER11    .           \nCLUSTER12    .           \nCLUSTER13    .           \nCLUSTER14    .           \nCLUSTER15    .           \nCLUSTER16    .           \nCLUSTER17    .           \nRACE2        3.111985e-03\nRACE3        .           \nRACE4        .           \nCIGPN        .           \nCIGFN        .           \nCIGSN        .           \nCIGLN        .           \nBWTG        -7.565233e-07\nGEST        -1.529002e-03\nPLUR         .           \nMAGE         .           \nPARITY       .           \nINCOME       .           \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple.lasso.model <- glmnet(x, y, alpha = 1, family = \"gaussian\", lambda = cv.lasso$lambda.1se)\nbest.lasso.model <- glmnet(x,y,alpha=1,family=\"gaussian\",lambda=cv.lasso$lambda.min)\ntest.x<-model.matrix(IMR~.-NUMBIRTH-NUMDEATH,test.data)[,-1]\nsimple.preds<-predict(simple.lasso.model,test.x)\nbest.preds<-predict(best.lasso.model,test.x)\nsimple.lasso.error<-mean((simple.preds-na.omit(test.data$IMR))^2)\nsimple.lasso.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002038072\n```\n:::\n\n```{.r .cell-code}\nbest.lasso.error<-mean((best.preds-na.omit(test.data$IMR))^2)\nbest.lasso.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002161291\n```\n:::\n:::\n\nThe simple model, which uses only RACE2 (African-American), birth weight in grams, and gestation period, is better than the more complex one on the test data. Also, it is better than the best subset selected model.\n\n\nAn unpruned tree model does worse than all three:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\ntree.model<-tree(IMR~.-NUMBIRTH-NUMDEATH,train.data)\nsummary(tree.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\ntree(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = train.data)\nVariables actually used in tree construction:\n[1] \"BWTG\"    \"GEST\"    \"CLUSTER\" \"PLUR\"    \"PARITY\"  \"CIGFN\"   \"RACE\"   \n[8] \"MAGE\"   \nNumber of terminal nodes:  14 \nResidual mean deviance:  3.229e-05 = 0.01036 / 321 \nDistribution of residuals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.0242200 -0.0018800 -0.0005768  0.0000000  0.0016300  0.0265900 \n```\n:::\n\n```{.r .cell-code}\nplot(tree.model)\ntext(tree.model,pretty=0)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\ntree.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002433887\n```\n:::\n:::\n\n\nLet's do some tree pruning. The following shows that the minimum deviance is obtained via a tree with 5 nodes. The pruning improves the test MSE a bit, but it still doesn't beat the lasso.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprune.tree.model<-prune.tree(tree.model,best=5)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\nprune.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002200646\n```\n:::\n:::\n\nLet's try random forests. These don't provide a dramatic improvement over the single tree. Continue to confirm that RACE, GEST, and BWTG are biggest predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-NUMBIRTH-NUMDEATH,na.omit(train.data), importance=TRUE)\nrf.model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = IMR ~ . - NUMBIRTH - NUMDEATH, data = na.omit(train.data),      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 5.982042e-05\n                    % Var explained: 18.83\n```\n:::\n\n```{.r .cell-code}\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nrf.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002202987\n```\n:::\n\n```{.r .cell-code}\nvarImpPlot(rf.model)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHave to try boosting:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-NUMBIRTH-NUMDEATH,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            var   rel.inf\nCLUSTER CLUSTER 31.569196\nGEST       GEST 12.781752\nBWTG       BWTG 10.765729\nPLUR       PLUR  9.510822\nRACE       RACE  7.189561\nMAGE       MAGE  6.451966\nPARITY   PARITY  6.013129\nCIGPN     CIGPN  4.016546\nCIGFN     CIGFN  3.494723\nINCOME   INCOME  3.101411\nCIGSN     CIGSN  1.897428\nCIGLN     CIGLN  1.866473\nYEAR       YEAR  1.341264\n```\n:::\n\n```{.r .cell-code}\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)\nboost.error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0002398635\n```\n:::\n:::\n\n\nNow, we summarize our results. As a whole, the regularized linear methods worked the best. Of the trees, the best method was the pruned tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean.error<-mean(na.omit(test.data$IMR)-mean(na.omit(train.data$IMR)))^2\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\",\"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Method   Test.Error\n1 No Dependence on Predictors 1.541823e-06\n2    Best Subset Linear Model 2.065257e-04\n3                Simple Lasso 2.038072e-04\n4            Lowest MSE Lasso 2.161291e-04\n5                        Tree 2.433887e-04\n6                 Pruned Tree 2.200646e-04\n7              Random Forests 2.202987e-04\n8                    Boosting 2.398635e-04\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbr.dr.race.year<-count(br,RACE,YEAR)\nbr.dr.race.year<-rename(br.dr.race.year,numbirth=n)\nbr.dr.race.year<-left_join(br.dr.race.year, count(dr,RACE,YEAR))\nbr.dr.race.year<-rename(br.dr.race.year,numdeath=n)\ndata.plot<-ggplot() + \n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"White\"), data=br.dr.race.year[br.dr.race.year$RACE==1,], )+\n geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Black\"), data=br.dr.race.year[br.dr.race.year$RACE==2,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Indian\"), data=br.dr.race.year[br.dr.race.year$RACE==3,], )+\n  geom_line(aes(x=YEAR, y = numdeath/numbirth,colour = \"Other\"), data=br.dr.race.year[br.dr.race.year$RACE==4,], )+\nscale_color_manual(name = \"Race\", values = c(\"White\" = \"darkred\", \"Black\" = \"darkblue\",\"Indian\"=\"darkgreen\",\"Other\"=\"black\"))\ndata.plot\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nGoing to try to make a map of North Carolina, and to see the county FIPs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbr.dr.county<-count(br,subregion)\nbr.dr.county<-rename(br.dr.county,numbirth=n)\ndr.county<-count(dr,CORES)\ndr.county<-left_join(dr.county,county.conversion[,1:2])\ndr.county<-rename(dr.county,subregion=COUNTY)\ndr.county<-rename(dr.county,numdeath=n)\ndr.county$subregion<-tolower(dr.county$subregion)\nbr.dr.county<-left_join(br.dr.county,dr.county)\nimr<-br.dr.county$numdeath/br.dr.county$numbirth\nbr.dr.county<-bind_cols(br.dr.county,IMR=imr)\nnccodr<-inner_join(nc_county,br.dr.county,by=\"subregion\")\ndrmap <- nc_base + \n      geom_polygon(data = nccodr, aes(fill = IMR), color = \"white\") +\n      geom_polygon(color = \"black\", fill = NA) +\n      theme_bw() +\n      ditch_the_axes+\n      scale_fill_gradient(\n        low = \"#00FFFF\",\n        high = \"#000000\",\n        space = \"Lab\",\n        na.value = \"grey50\",\n        guide = \"colourbar\",\n        aesthetics = \"fill\"\n        )\ndrmap\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "NC_Birth_and_Death_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}