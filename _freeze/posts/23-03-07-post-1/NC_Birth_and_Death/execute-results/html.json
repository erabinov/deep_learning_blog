{
  "hash": "34bc538d19b7ad0c66737d46ebf5161d",
  "result": {
    "markdown": "---\ntitle: \"Analyzing Infant Mortality Rates by County, Race, and Income in North Carolina, 2011-2016\"\ndate: \"2023-03-07\"\nformat:\n  html:\n    html-math-method: mathjax\n---\n\n\n## Introduction \n\nIn November 2022, my friend gave me the datasets included in the \"data\" folder for this post as \"Yr1116Birth.csv\" and \"Yr1116Death.csv\" and told me to develop a model that predicts a county-by-county infant mortality rate based on various demographic features that were recorded in the data. The data lists births and deaths of infants in the years 2011-2016 in North Carolina by county of residence, race, and ethnicity of the mother. The Excel file `Dataset Descriptions.xls` lists all the information available in this dataset, and I presume that the information comes from the [NC State Center for Vital Statistics](https://schs.dph.ncdhhs.gov/data/vital.cfm).\nI've also supplemented the data with county-by-county income data which I found at [datausa](https://datausa.io/profile/geo/north-carolina), though this data is also readily available from various public agencies like the Census Bureau.\nI use the data to compute infant mortality rates by county, race, and year.\nI used the 2011-2015 data to train my models and the 2016 data for validation.\n\nThis post is a write-up of my analysis of this data.\nThis was an interesting task because it was a crash course in a few important data science skills: data wrangling, rare event modeling, and maps & data visualization.\n\n### Data wrangling\n\nMy data came from a number of different sources. There were separate birth and death data tables, the income data came from its own source, and the purely geographic data used to generate the map at the end of this post came from the `mapdata` package.\n\nCombining these sources presented some difficulty. The birth and death data had different numerical codings for the race of the baby/mother, so I had to make a custom function `collapse` which collapsed the many extra race codings in the birth data into the simpler \"other\" category in the death data.\nIn a similar vein, different sources encoded county information in different ways: North Carolina orders the counties from 1 to 100, but there is also a FIPS code which is more useful for national data, and of course the name of a county is a fine label for it.\nOne defect in my data was that one of my sources mis-spelled \"Tyrrell\", and it took me a while to detect this error.\n\n### Rare Event Modeling\n\nWhen my friend presented me with this data, he and I discussed the interesting fact that some counties recorded no infant deaths for certain races in certain years.\nI don't think that this was due to incomplete records or reporting anomalies: when I investigated these cases, I found that there were fewer than 100 births in the previous year in the same race and county.\nThe overall infant mortality rate was about .7% in these years, so the expected number of infant deaths when there are fewer than 100 births is less than 1.\n\nMy friend raised the possibility that I could model this problem as a classification problem: given a partiular infant birth, predict the probability that it would die in the first year of its life.\nI considered this possibility, but decided not to do the analysis in this way, since the birth data contained more information, like the infant's birth weight, that was not reflected in the death data, so I thought it might be hard to measure the effect of these additional variables on a given infant's likelihood of death.\nSo, instead I modeled the problem as a regression problem: predict a county's infant mortality rate in a given county by year and race, given the county's average values for the other predictors in the birth data (e.g., birth weight in grams, median income in the county, number of cigarettes smoked by the mother).\n\nNevertheless, the data still presented the challenges associated with classification problems in which the classes are very unbalanced in number.\nTo get a feel for why there is an issue, let's consider one of those counties where there were no infant deaths in a given year.\nBecause infant mortality rates are on the order of 1/1000, to detect significant changes in rates between counties, it makes sense to measure them on a log scale.\nThe counties where there were no infant deaths, we would record an infant mortality rate of 0, which would be (infinitely) many orders of magnitude smaller than the typical rate.\nTo solve this, I added .001 to the infant mortality rates before taking the logarithm.\nThis is sort of like label smoothing: I don't want the model to make too much of those points where there happened to be no infant deaths.\n\nOne other thing I tried to do was to aggregate the counties into clusters and compute only in-cluster infant mortality rates for the training data.\nThis was an attempt to reduce year-to-year variance due to small sample sizes.\nHowever, I found that my implementation of this idea didn't really improve the validation-set error.\nSo, in the end, I didn't put this into practice.\nBut, if you look at the code for this notebook, you'll find relics of that approach.\n\n\n### Visualization and Maps\n\nThis project was a good way for me to practice what I had learned about making data visualizations from *Introduction to Statistical Learning with R*, including feature importance charts. But it was also an opportunity for me to learn how to make a county-by-county heat map; this map appears in the last section of this \n\n\n## Loading Packages and Cleaning Data\n\n\nIn this section, I clean the data and wrangle it into a form amenable to analysis.\nI omit most of this process from the presentation version of the notebook, but the interested readers can examine the code, which is available on my Github page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe result of all these manipulations is a data frame `brdrcounts` which looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(brdrcounts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 15\n   YEAR CORES  RACE CIGPN CIGFN CIGSN CIGLN  BWTG  GEST  PLUR  MAGE PARITY\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1  2011     1     1 3.25  1.91  1.43  1.33  3318.  38.7  1.03  27.1   2.33\n2  2011     1     2 2.58  1.58  1.06  0.942 3089.  38.2  1.05  25.5   2.77\n3  2011     1     3 6.5   5     5     5     2758.  38.5  1     25     2.75\n4  2011     1     4 0.347 0.180 0.178 0.159 3287.  38.7  1.03  27.2   3.05\n5  2011     2     1 2.93  1.87  1.63  1.49  3252.  38.2  1.05  26.6   2.71\n6  2011     2     2 0     0     0     0     3433.  39.5  1     23.4   2.15\n# … with 3 more variables: INCOME <dbl>, IMR <dbl>, CLUSTER <dbl>\n```\n:::\n:::\n\nMost of the columns are explained in the file `Dataset Descriptions.xls`; \"IMR\" is the log of infant mortality rate, and \"Cluster\" is just a duplicate of \"CORES\" (it's an artifact from when I tried to apply clustering to the data).\n\n## Trying Different Models\n\nNow, we proceed to try different models on the test data. I think a bit of a warning is in order about concluding too much about variable importance, since we expect there to be significant collinearity between some of the predictors.\n\nThe first method we try is just a linear model; we perform subset selection by validation-set MSE.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.min(val.errors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10\n```\n:::\n\n```{.r .cell-code}\ncoef(regfit,which.min(val.errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)         RACE2         RACE3         CIGPN         CIGFN \n-1.983988e+00  4.961281e-01  1.442747e-01 -7.878452e-03 -5.804568e-03 \n        CIGSN          BWTG          GEST          PLUR        PARITY \n 1.456896e-02 -1.282548e-04 -4.569572e-02 -4.074168e-01  7.307301e-03 \n       INCOME \n-9.240510e-06 \n```\n:::\n\n```{.r .cell-code}\nsubset.error<-val.errors[which.min(val.errors)]\n```\n:::\n\n\nThe best model seems to associate a decline in infant mortality rate if the mother is American Indian or \"Other\" (not White, Black, or American Indian).\nIt's hard to understand the sign of the coefficients for \"CIGPN\" and \"CIGFN\".\nMy guess is that this has to do with the fact that I imputed a slightly lower-than-average infant mortality rate when the death count for a given county, race, and year is zero.\n\n\nLet's now try lasso regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx<-model.matrix(IMR~.-CORES,train.data[,1:14])[,-1]\ny<-na.omit(train.data$IMR)\nset.seed(1)\ncv.lasso<-cv.glmnet(x,y,alpha=1,family=\"gaussian\")\nplot(cv.lasso)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncoef(cv.lasso,cv.lasso$lambda.min)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -7.914631e+00\nYEAR         2.907554e-03\nRACE2        4.879737e-01\nRACE3        1.309074e-01\nRACE4       -7.877888e-03\nCIGPN       -6.122286e-03\nCIGFN        .           \nCIGSN        6.751858e-03\nCIGLN        .           \nBWTG        -1.183393e-04\nGEST        -4.529146e-02\nPLUR        -3.807254e-01\nMAGE        -1.634793e-04\nPARITY       6.375718e-03\nINCOME      -8.955468e-06\n```\n:::\n\n```{.r .cell-code}\ncoef(cv.lasso,cv.lasso$lambda.1se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) -4.8453328\nYEAR         .        \nRACE2        0.1535657\nRACE3        .        \nRACE4        .        \nCIGPN        .        \nCIGFN        .        \nCIGSN        .        \nCIGLN        .        \nBWTG         .        \nGEST         .        \nPLUR         .        \nMAGE         .        \nPARITY       .        \nINCOME       .        \n```\n:::\n:::\n\nThe 1se lambda value gives a model in which the only predictor is RACE2 (African American).\n\n\n\n\n\nNow we try ridge regression:\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.ridge<-cv.glmnet(x,y,alpha=0,family=\"gaussian\")\nplot(cv.ridge)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncoef(cv.ridge,cv.ridge$lambda.min)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -1.002797e+01\nYEAR         3.996398e-03\nRACE2        4.597315e-01\nRACE3        1.149176e-01\nRACE4       -2.115184e-02\nCIGPN       -5.598731e-03\nCIGFN       -1.539379e-03\nCIGSN        5.012017e-03\nCIGLN        2.696535e-03\nBWTG        -1.336627e-04\nGEST        -4.438239e-02\nPLUR        -3.947143e-01\nMAGE        -2.115157e-03\nPARITY       7.580164e-03\nINCOME      -8.632730e-06\n```\n:::\n\n```{.r .cell-code}\ncoef(cv.ridge,cv.ridge$lambda.1se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -5.599637e+00\nYEAR         8.684702e-04\nRACE2        8.397161e-02\nRACE3       -3.927352e-03\nRACE4       -3.221903e-02\nCIGPN       -6.046998e-05\nCIGFN       -5.124061e-05\nCIGSN       -9.542885e-06\nCIGLN       -3.908353e-05\nBWTG        -5.967245e-05\nGEST        -1.357928e-02\nPLUR        -1.939902e-02\nMAGE        -6.071366e-03\nPARITY       7.083761e-04\nINCOME      -1.630609e-06\n```\n:::\n:::\n\n\n\n\nLet's try to train a single tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\ntree.model<-tree(IMR~.-CORES,train.data)\nsummary(tree.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\ntree(formula = IMR ~ . - CORES, data = train.data)\nVariables actually used in tree construction:\n[1] \"RACE\"   \"INCOME\" \"PLUR\"   \"GEST\"   \"CIGLN\" \nNumber of terminal nodes:  6 \nResidual mean deviance:  0.3125 = 582.5 / 1864 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.33800 -0.15540 -0.08285  0.00000  0.14070  4.18600 \n```\n:::\n\n```{.r .cell-code}\nplot(tree.model)\ntext(tree.model,pretty=0)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntree.preds<-predict(tree.model,na.omit(test.data))\ntree.error<-mean((na.omit(tree.preds)-na.omit(test.data$IMR))^2)\n```\n:::\n\nThe most notable differences in IMR come from GEST and RACE2.\n\n\nLet's do some tree pruning. The following graph shows that the minimum deviance is obtained via a tree with 6 nodes; however, there doesn't seem to be much difference between a tree with 3 nodes and a tree with 6 nodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntree.cv<-cv.tree(tree.model)\nplot(tree.cv$size,tree.cv$dev,type=\"b\")\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprune.tree.model<-prune.tree(tree.model,best=3)\nplot(prune.tree.model)\ntext(prune.tree.model,pretty=0)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprune.tree.preds<-predict(prune.tree.model,na.omit(test.data))\nprune.error<-mean((na.omit(prune.tree.preds)-na.omit(test.data$IMR))^2)\n```\n:::\n\n\nAs we noted above, GEST and RACE2 are the major factors in this tree.\n\n\nLet's try random forests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(12)\nrf.model<-randomForest(IMR~.-CORES,na.omit(train.data[,1:14]), importance=TRUE)\nrf.preds<-predict(rf.model,newdata=na.omit(test.data))\nrf.error<-mean((rf.preds-na.omit(test.data$IMR))^2)\nvarImpPlot(rf.model)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\nAgain, \"RACE\", \"BWTG\", and the various \"CIG\" predictors appear near the top of the %IncMSE chart.\n\n\n\n\n\n\nLast model is boosting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\nset.seed(15)\nboost.model<-gbm(IMR~.-CORES,data=na.omit(train.data),distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nsummary(boost.model)\n```\n\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            var   rel.inf\nGEST       GEST 14.410847\nBWTG       BWTG 12.606412\nMAGE       MAGE 11.118088\nPARITY   PARITY 10.495755\nCLUSTER CLUSTER  8.163164\nINCOME   INCOME  7.517338\nCIGPN     CIGPN  6.803060\nPLUR       PLUR  6.779247\nCIGFN     CIGFN  5.553177\nCIGLN     CIGLN  4.856280\nCIGSN     CIGSN  4.735801\nRACE       RACE  4.725559\nYEAR       YEAR  2.235272\n```\n:::\n\n```{.r .cell-code}\nboost.preds<-predict(boost.model,newdata=na.omit(test.data))\nboost.error<-mean((boost.preds-na.omit(test.data$IMR))^2)\n```\n:::\n\n\n##Summary of Results\n\nNow, we summarize our results in a table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean.error<-mean((na.omit(test.data$IMR)-log(avgIMR+.007))^2)\ntest.error.data<-data.frame(Method=c(\"No Dependence on Predictors\",\"Best Subset Linear Model\",\"Simple Lasso\",\"Lowest MSE Lasso\", \"Simple Ridge\", \"Lowest MSE Ridge\", \"Tree\",\"Pruned Tree\",\"Random Forests\",\"Boosting\"),`Test Error`=c(mean.error,subset.error,simple.lasso.error,best.lasso.error,simple.ridge.error, best.ridge.error, tree.error,prune.error,rf.error,boost.error))\ntest.error.data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Method Test.Error\n1  No Dependence on Predictors  0.6817740\n2     Best Subset Linear Model  0.3456520\n3                 Simple Lasso  0.3640914\n4             Lowest MSE Lasso  0.3462914\n5                 Simple Ridge  0.3660809\n6             Lowest MSE Ridge  0.3466169\n7                         Tree  0.3436803\n8                  Pruned Tree  0.3643193\n9               Random Forests  0.3240173\n10                    Boosting  0.4121946\n```\n:::\n:::\n\nRandom forests seems to have done the best. This is consistent with its reputation as the best out-of-the-box method.\nBoosting is finnicky, and probably required some more hyperparameter tuning.\n\n##Graphs and Visualizations\nThis is a graph of infant mortality rates by race and year.\n\n::: {.cell}\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThis is a heat map of North Carolina by infant mortality rate:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](NC_Birth_and_Death_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n## Conlcusion\n\nIt was interesting to go back to this project a few months after I first did it, because I noticed a lot of places where what I have learned in the interim could have come in handy.\nFor example, I now have more robust EDA and feature engineering frameworks.\nKeep an eye out for a future blog post in which I discuss these issues in more depth in the context of my participation in recent Kaggle competitions.\n\n",
    "supporting": [
      "NC_Birth_and_Death_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}